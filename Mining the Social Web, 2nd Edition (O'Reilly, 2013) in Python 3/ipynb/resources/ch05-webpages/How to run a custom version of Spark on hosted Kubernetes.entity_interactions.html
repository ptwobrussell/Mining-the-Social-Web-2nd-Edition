<html>
    <head>
        <title>How to run a custom version of Spark on hosted Kubernetes Interactions</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    </head>
    <body><strong>Learn</strong> how <strong>Spark</strong> 2.3.0+ <strong>integrates</strong> with <strong>K8s</strong> <strong>clusters</strong> on <strong>Google Cloud</strong> and <strong>Azure.Do</strong> you want to try out a new <strong>version</strong> of Apache <strong>Spark</strong> without waiting around for the entire release process? Does running alpha-quality software sound like fun? <strong>Does</strong> setting up a <strong>test cluster sound</strong> like work? This is the blog post for you, my friend! We will help you deploy code that hasn't even been reviewed yet (if that is the adventure you seek). If you<strong>’</strong>re a little cautious, reading this might sound like a bad <strong>idea</strong>, and often it is, but it can be a great <strong>way</strong> to ensure that a <strong>PR</strong> really fixes your <strong>bug</strong>, or the new proposed <strong>Spark</strong> <strong>release doesn</strong><strong>’</strong><strong>t break anything</strong> you depend on (and if it does, you can raise the alarm). This <strong>post</strong> will help you try out new (2.3.0+) and custom <strong>versions</strong> of <strong>Spark</strong> on <strong>Google/Azure</strong> with Kubernetes. Just don't run this in <strong>production</strong> without a <strong>backup</strong> and a very fancy <strong>support contract</strong> for when things go sideways. <strong>Note</strong>: This is a cross-vendor <strong>post</strong> (<strong>Azure</strong>'s <strong>Spark</strong> on <strong>AKS</strong> and <strong>Google Cloud</strong>'s Custom <strong>Spark</strong> on <strong>GKE</strong>), each of which have their own vendor-specific <strong>post</strong>s if that’s more your thing. <s<strong>t</strong>rong>Warning</s<strong>t</strong>rong>: i<strong>t</strong>’s impor<strong>t</strong>an<strong>t</strong> <strong>t</strong>o make sure your <s<strong>t</strong>rong><strong>t</strong>es<strong>t</strong>s</s<strong>t</strong>rong> don’<strong>t</strong> des<strong>t</strong>roy your real da<strong>t</strong>a, so consider using a sub-accoun<strong>t</strong> wi<strong>t</strong>h lesser permissions. Setting up your <strong><strong>version</strong></strong> of <strong>Spark</strong> to run
If there is an off-the-shelf <strong><strong>version</strong></strong> of <strong>Spark</strong> you want to run, you can go ahead and download it. If you want to try out a specific <strong>patch</strong>, you can checkout the <strong>pull request</strong> to your local <strong>machine</strong> with git <strong>fetch origin pull/<strong>ID</strong>/head</strong>:<strong>BRANCHNAME</strong>, where <strong>ID</strong> is the <strong>PR</strong> <strong>number</strong>, and then follow the <strong>directions</strong> to build <strong>Spark</strong> (remember to include the -P <strong>components</strong> you want/need, including your <strong>cluster manager</strong> of choice). Now that we’ve got <strong>Spark</strong> built, we will build a <strong>container image</strong> and upload it to the <strong>registry</strong> of your <strong>choice</strong>, like shipping a <strong>PXE</strong> <strong>boot image</strong> in the early 90s (bear with me, I miss the 90s). Depending on which <strong>registry</strong> you want to use, you’ll <strong>need</strong> to point both the <strong>build tool</strong> and <strong>spark-submit</strong> in the correct location. We can do this with an <strong>environment variable—for</strong> <strong>Docker Hub</strong>, this is the <strong>name</strong> of the <strong>registry</strong>; for <strong>Azure Container Registry</strong> (<strong><strong>ACR</strong></strong>), this <strong>value</strong> is the <strong><strong>ACR</strong></strong> login server <strong>name</strong>; and for <strong>Google Container Registry</strong>, this is gcr.io/$PROJECTNAME. <strong>export</strong> <strong>REGISTRY=value</strong>
For <strong>Google</strong> <strong>cloud</strong> <strong>users</strong> who want to use the <strong>Google</strong>-provided <strong><strong>Docker</strong></strong> <strong>registry</strong>, you will need to set up <strong><strong>Docker</strong></strong> to run through g<strong>cloud</strong>. In the <strong>bash shell</strong>, you can do this with an <strong>alias</strong>:
<strong>shopt</strong> <strong>-s</strong> expand_<strong>alias</strong>es && <strong>alias</strong> <strong>docker=</strong>"<strong>gcloud docker</strong> --"
For <strong>Azure</strong> <strong>users</strong> who want to use <strong>Azure</strong> Container Registry (<strong><strong>ACR</strong></strong>), you will need to grant <strong>Azure</strong> Container Service (<strong>AKS</strong>) <strong>cluster</strong> read <strong>access</strong> to the <strong><strong>ACR</strong></strong> resource. For non-Google <s<strong>t</strong>rong>users</s<strong>t</strong>rong>, you don’<strong>t</strong> need <strong>t</strong>o wrap <strong>t</strong>he <strong>Docker</strong> <strong>command</strong>, and jus<strong>t</strong> skip <strong>t</strong>ha<strong>t</strong> s<strong>t</strong>ep and keep going:

expor<strong>t</strong> <strong>DOCKER_REPO=</strong>$<strong>REGISTRY/spark</strong>
expor<strong>t</strong> <strong><strong>SPARK_VERSION</strong></strong>=`gi<strong>t</strong> rev-parse <strong>HEAD`</strong>
./bin/docker-image-<strong>t</strong>ool.sh <strong><strong>-r</strong></strong> $DOCKER_REPO -<strong>t</strong> $<strong><strong>SPARK_VERSION</strong></strong> build
./bin/docker-image-<strong>t</strong>ool.sh <strong><strong>-r</strong></strong> $DOCKER_REPO -<strong>t</strong> $<strong><strong>SPARK_VERSION</strong></strong> <strong>push</strong>

Building your <strong><strong>Spark</strong></strong> projec<strong>t</strong> for deploymen<strong>t</strong> (or, op<strong>t</strong>ionally, s<strong>t</strong>ar<strong>t</strong>ing a new one)
<strong><strong>Spark</strong></strong> on <strong>K8s</strong> does no<strong>t</strong> au<strong>t</strong>oma<strong>t</strong>ically handle <strong>push</strong>ing <strong><strong>JARs</strong></strong> <strong>t</strong>o a dis<strong>t</strong>ribu<strong>t</strong>ed file sys<strong>t</strong>em, so we will need <strong>t</strong>o upload wha<strong>t</strong>ever <strong><strong>JARs</strong></strong> our projec<strong>t</strong> requires <strong>t</strong>o work. One of the easiest <strong>ways</strong> to do this is to turn our <strong>Spark</strong> <strong>project</strong> into an <strong>assembly</strong> JAR. If you’re starting a new <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong> and you have <strong>sbt</strong> installed, you can use the <strong<strong>></strong>Spark</strong<strong>></strong> <strong>template</strong> <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong>:
<strong>sbt</strong> new <strong<strong>></strong>holdenk/sparkProjectTemplate.g8</strong<strong>></strong>
If you have an existing <strong><strong>SBT</strong></strong>-based <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong>, you can add the <strong>sbt</strong>-<strong>assembly</strong> <strong<strong>></strong>plugin</strong<strong>></strong>:

touch <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong>/<strong>assembly</strong>.<strong>sbt</strong>
echo <strong<strong>></strong>'addSbtPlugin</strong<strong>></strong>("com.eed3si9n" <strong<strong>></strong><strong<strong>></strong>%</strong<strong>></strong></strong<strong>></strong> "<strong>sbt</strong>-<strong>assembly</strong>" <strong<strong>></strong><strong<strong>></strong>%</strong<strong>></strong></strong<strong>></strong> "0.14.6")' <strong>></strong><strong>></strong> <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong>/<strong>assembly</strong>.<strong>sbt</strong>

With <strong><strong>SBT</strong></strong>, once you have the <strong><strong>SBT</strong></strong> <strong>assembly</strong> <strong<strong>></strong>plugin</strong<strong>></strong> (either through creating a <strong<strong>></strong><strong<strong>></strong><strong>project</strong></strong<strong>></strong></strong<strong>></strong> with it included in the <strong>template</strong> or adding it to an existing one), you can produce an <strong>assembly</strong> <strong><strong>JAR</strong></strong> by running:
<strong>sbt</strong> <strong>assembly</strong>
The resulting <strong><strong>JAR</strong></strong> not only will have your <strong>source code</strong>, but all of the requirements as well. <strong>Note</strong> that this <strong>JAR</strong> may have multiple <strong>entry</strong> <strong>points</strong>, so later on, we’re going to need to tell <strong>Spark</strong> <strong>submit</strong> about the <strong>entry</strong> point we want it to use. For the <strong>world standard wordcount example</strong>, we might use:
<strong>export</strong> <strong>CLASS_NAME=org.apache.spark.examples.JavaWordCount</strong>
If you have a <strong>maven</strong> or other <strong>project</strong>, there are a few different <strong>options</strong> for <strong>building assembly</strong> JARs. Sometimes, these may be referred to as <strong>“</strong>fat <strong>jars</strong>” in the documentation. If starting a new <strong>project</strong> sounds like too much <strong>work</strong> and you really just want to double check that your <strong><strong>Spark</strong></strong> on <strong>K8s</strong> <strong>deployment</strong> <strong>work</strong>s, you can use the <strong>example</strong> <strong>JAR</strong> that <strong><strong>Spark</strong></strong> ships with (e.g., <strong>example</strong>s/target/spark-<strong>example</strong>s). Uploading your <strong><strong>JARs</strong></strong>
One of the <strong>differences</strong> between <strong><strong>Spark</strong></strong> on <strong>K8s</strong> and <strong><strong>Spark</strong></strong> in the other <strong>cluster</strong> <strong>managers</strong> is that there is no automatic <strong>tool</strong> to distribute our <strong><strong>JARs</strong></strong> (or other <strong>job</strong> dependencies). To make sure your <strong>containers</strong> have <strong>access</strong> to your <strong>JAR</strong>, the fastest option is normally to upload it. <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>Regardless</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> of <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>pla<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>form</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>, we need <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>o specify which <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>, <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>con<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ainer / bucke<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>, and <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>he <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>arge<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>:

<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>expor<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>FOLDER_NAME</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>=mybucke<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>expor<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> SRC<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>=<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>arge<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>/scala-2.11/...
<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>expor<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> MY<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>=myjar

Wi<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>h <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<strong>t</strong>rong>Azure</s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>:

<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>RESOURCE_GROUP</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>=sparkdemo
<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>STORAGE_ACCT</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>=sparkdemo$<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>RANDOM</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
az <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>group crea<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>e</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>RESOURCE_GROUP</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>loca<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ion eas<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>us</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
az <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage accoun<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> crea<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>e</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --resource-group $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>RESOURCE_GROUP</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>STORAGE_ACCT</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --sku <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>S<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>andard_LRS</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>expor<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>AZURE_STORAGE_CONNECTION_STRING=`az</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage accoun<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> show-connec<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ion-s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ring --resource-group $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>RESOURCE_GROUP</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>STORAGE_ACCT</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> -o <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>sv`
az s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage con<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ainer crea<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>e --name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>FOLDER_NAME</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
az <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage con<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ainer se<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>-permission</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>FOLDER_NAME</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --public-<s<strong>t</strong>rong><strong>access</strong></s<strong>t</strong>rong> blob
az s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage blob upload --con<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ainer-name $<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>FOLDER_NAME</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --file $SRC<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> --name $MY<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>

Wi<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>h <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>Google Cloud</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>:
<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>gs</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>u<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>il</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> cp $SRC<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>gs</s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>://$<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>BUCKETNAME/$MY<s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>
For now <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>hough, we don<s<strong>t</strong>rong>’</s<strong>t</strong>rong><s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong> have <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>he <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong>s ins<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>alled <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>o <s<strong>t</strong>rong><strong>access</strong></s<strong>t</strong>rong> <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>he <s<strong>t</strong>rong>GCS</s<strong>t</strong>rong> or <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><s<strong>t</strong>rong>Azure</s<strong>t</strong>rong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> blob s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>orage, and <s<strong>t</strong>rong>Spark</s<strong>t</strong>rong> on <s<strong>t</strong>rong>K8s</s<strong>t</strong>rong> <s<strong>t</strong>rong>doesn</s<strong>t</strong>rong><s<strong>t</strong>rong>’</s<strong>t</strong>rong><s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong> curren<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>ly suppor<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong> <strong>spark-packages</strong>, which we could use <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>o <s<strong>t</strong>rong><strong>access</strong></s<strong>t</strong>rong> <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>hose, so we need <s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>o make our <s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong><strong>JAR</strong></s<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>rong> <s<strong>t</strong>rong><strong>access</strong></s<strong>t</strong>rong>ible over h<s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong><s<strong>t</strong>rong><strong>t</strong></s<strong>t</strong>rong>p. With <strong>Azure</strong>:
<strong>JAR_URL=</strong>$(az <strong>storage blob</strong> url --container-name $<strong>FOLDER_NAME</strong> --name $<strong><strong>MYJAR |</strong></strong> <strong>tr -d</strong> '"')
With <strong>Google Cloud</strong>:
<strong><strong>export</strong></strong> <strong><strong>PROJECTNAME</strong>=boos-demo-<strong>projects</strong>-are-rad</strong>
gcloud iam <strong>service-accounts</strong> <strong>create <strong><strong><strong>signer</strong></strong></strong></strong> --display-name "<strong><strong><strong>signer</strong></strong></strong>"
gcloud <strong>projects</strong> add-iam-policy-binding $<strong>PROJECTNAME</strong> --<strong>member serviceAccount</strong>:<strong><strong><strong>signer</strong></strong></strong>@$<strong>PROJECTNAME</strong>.iam.<strong>gs</strong>erviceaccount.com --<strong>role roles/storage.objectViewer</strong>
gcloud iam <strong>service-accounts</strong> keys create     <strong>~/key.json</strong>     --iam-account <strong><strong><strong>signer</strong></strong></strong>@$<strong>PROJECTNAME</strong>.iam.<strong>gs</strong>erviceaccount.com
<strong><strong>export</strong></strong> <strong>JAR_URL=</strong>`<strong>gs</strong>util signurl -m GET <strong>~/key.json</strong> <strong>gs</strong>://$<strong>JARBUCKETNAME/</strong>$<strong><strong>MYJAR |</strong></strong> cut  -f 4 | <strong>tail -n</strong> 1`

Starting your <strong>cluster</strong>
Now you are ready to kick off your <strong>super-fancy</strong> <strong>K8s Spark</strong> <strong>cluster</strong>. For <strong>Azure</strong>:

<strong>az</strong> <strong>group <strong>create</strong></strong> --name <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --<strong>location eastus</strong>
<strong>az</strong> aks <strong>create</strong> --resource-group <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --name <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --node-vm-size <strong>Standard_D3_v2</strong>
<strong>az</strong> aks <strong>get-credentials</strong> --resource-group <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --name <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong>
<strong><strong>kubectl</strong></strong> <strong><strong>proxy</strong></strong> &

For <strong>Google</strong> <strong>cloud</strong>:

g<strong>cloud</strong> <strong><strong>container</strong></strong> <strong><strong><strong>cluster</strong></strong>s</strong> <strong>create</strong>  <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --zone us-east1-b --project $<strong>PROJECTNAME</strong>
g<strong>cloud</strong> <strong><strong>container</strong></strong> <strong><strong><strong>cluster</strong></strong>s</strong> <strong>get-credentials</strong> <strong><strong><strong><strong><strong>my<strong><strong><strong><strong>Spark</strong></strong></strong></strong>Cluster</strong></strong></strong></strong></strong> --zone us-east1-b --project $<strong>PROJECTNAME</strong>
<strong><strong>kubectl</strong></strong> <strong><strong>proxy</strong></strong> &

On <strong>Google</strong> Cloud, before we kick off our <strong><strong><strong><strong>Spark</strong></strong></strong></strong> <strong><strong><strong>job</strong></strong></strong>, we need to make a <strong>service account</strong> for <strong><strong><strong><strong>Spark</strong></strong></strong></strong> that will have <strong>permission</strong> to edit the <strong><strong>cluster</strong></strong>:

<strong><strong>kubectl</strong></strong> <strong>create</strong> serviceaccount <strong>spark</strong>
<strong><strong>kubectl</strong></strong> <strong>create</strong> <strong><strong>cluster</strong></strong>rolebinding <strong>spark</strong>-role --<strong><strong>cluster</strong></strong>role=edit --<strong>serviceaccount=default</strong>:<strong>spark</strong> --namespace=default

Running your <strong><strong><strong><strong>Spark</strong></strong></strong></strong> <strong><strong><strong>job</strong></strong></strong>
And now we can finally run our <strong><strong><strong><strong>Spark</strong></strong></strong></strong> <strong><strong><strong>job</strong></strong></strong>:

./bin/<strong>spark</strong>-submit --<strong>master k8s</strong>:<strong>//http</strong>://127.0.0.1:8001  <strong><strong><strong><strong>\</strong></strong></strong></strong>
  --deploy-mode <strong><strong>cluster</strong></strong> --conf <strong><strong><strong><strong>\</strong></strong></strong></strong>
 <strong>spark</strong>.kubernetes.<strong><strong>container</strong></strong>.image=$DOCKER_REPO/<strong>spark</strong>:$SPARK_VERSION <strong><strong><strong><strong>\</strong></strong></strong></strong>
--conf <strong>spark</strong>.executor.instances=1 <strong><strong><strong><strong>\</strong></strong></strong></strong>
--<strong>class</strong> $CLASS_NAME <strong><strong><strong><strong>\</strong></strong></strong></strong>
--conf <strong>spark</strong>.kubernetes.authenticate.driver.serviceAccountName=<strong>spark</strong> <strong><strong><strong><strong>\</strong></strong></strong></strong>
--<strong>name wordcount</strong> <strong><strong><strong><strong>\</strong></strong></strong></strong>
$JAR_URL <strong><strong><strong><strong>\</strong></strong></strong></strong>
<strong>inputpath</strong>

And we can verify the <strong>output</strong> with:
<strong><strong>kubectl</strong></strong> <strong>logs</strong> [podname-from-<strong>spark</strong>-submit]
Handling <strong>dependencies</strong> in <strong><strong><strong><strong>Spark</strong></strong></strong></strong> K8s (and accessing your <strong>data/code</strong> without making it public):
What if we want to directly read our <strong>JARs</strong> from the <strong>storage engine</strong> without using https? Or if we have <s<strong>t</strong>rong>dependencies</s<strong>t</strong>rong> <strong>t</strong>ha<strong>t</strong> we don’<strong>t</strong> wan<strong>t</strong> <strong>t</strong>o package in our <strong>assembly</strong> JARs? In <strong>t</strong>ha<strong>t</strong> <s<strong>t</strong>rong>case</s<strong>t</strong>rong>, can <strong>t</strong>he necessary <s<strong>t</strong>rong>dependencies</s<strong>t</strong>rong> <strong>t</strong>o our <s<strong>t</strong>rong><strong><strong>docker</strong></strong> file</s<strong>t</strong>rong> as follows:

<s<strong>t</strong>rong>mkdir</s<strong>t</strong>rong> <s<strong>t</strong>rong>/<strong>t</strong>mp/build</s<strong>t</strong>rong> && echo <s<strong>t</strong>rong>“</s<strong>t</strong>rong><s<strong>t</strong>rong>FROM</s<strong>t</strong>rong> $<s<strong>t</strong>rong><strong><strong>DOCKER_REPO/spark</strong></strong></s<strong>t</strong>rong>:$<s<strong>t</strong>rong>SPARK_VERSION</s<strong>t</strong>rong>

# <s<strong>t</strong>rong>Manually</s<strong>t</strong>rong> upda<strong>t</strong>e <s<strong>t</strong>rong>Guava</s<strong>t</strong>rong> dele<strong>t</strong>ing <strong>t</strong>he old <s<strong>t</strong>rong>JAR</s<strong>t</strong>rong> <strong>t</strong>o ensure we don’<strong>t</strong> have class pa<strong>t</strong>h conflic<strong>t</strong>s
<strong>RUN</strong> rm <strong><strong>\</strong></strong>$<strong><strong>SPARK_HOME/jars</strong></strong>/guava-14.0.1.jar
<strong><strong><strong>ADD</strong></strong></strong> h<strong>t</strong><strong>t</strong>p://cen<strong>t</strong>ral.maven.org/maven2/com/google/guava/guava/23.0/guava-23.0.jar <strong><strong>\</strong></strong>$<strong><strong>SPARK_HOME/jars</strong></strong>
# <strong><strong>Add</strong></strong> <strong>t</strong>he <strong><strong>GCS</strong></strong> connec<strong>t</strong>ors
<strong><strong><strong>ADD</strong></strong></strong> h<strong>t</strong><strong>t</strong>ps://s<strong>t</strong>orage.googleapis.com/hadoop-lib/gcs/gcs-connec<strong>t</strong>or-la<strong>t</strong>es<strong>t</strong>-hadoop2.jar <strong><strong>\</strong></strong>$<strong><strong>SPARK_HOME/jars</strong></strong>
# <strong><strong>Add</strong></strong> <strong>t</strong>he <strong>Azure</strong> Hadoop/S<strong>t</strong>orage <s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>s
<strong><strong><strong>ADD</strong></strong></strong> h<strong>t</strong><strong>t</strong>p://cen<strong>t</strong>ral.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.0/hadoop-azure-2.7.0.jar
<strong><strong><strong>ADD</strong></strong></strong> h<strong>t</strong><strong>t</strong>p://cen<strong>t</strong>ral.maven.org/maven2/com/microsof<strong>t</strong>/azure/azure-s<strong>t</strong>orage/7.0.0/azure-s<strong>t</strong>orage-7.0.0.jar

ENTRYPOINT [ '/op<strong>t</strong>/en<strong>t</strong>rypoin<strong>t</strong>.sh' <strong>]</strong>” > <s<strong>t</strong>rong>/<strong>t</strong>mp/build</s<strong>t</strong>rong>/<strong><strong>docker</strong></strong>file
<strong><strong>docker</strong></strong> build -<strong>t</strong> $<s<strong>t</strong>rong><strong><strong>DOCKER_REPO/spark</strong></strong></s<strong>t</strong>rong>:$<s<strong>t</strong>rong>SPARK_VERSION</s<strong>t</strong>rong>-wi<strong>t</strong>h-deps -f <s<strong>t</strong>rong>/<strong>t</strong>mp/build</s<strong>t</strong>rong>/<strong><strong>docker</strong></strong>file <s<strong>t</strong>rong>/<strong>t</strong>mp/build</s<strong>t</strong>rong>

Push <strong>t</strong>o our regis<strong>t</strong>ry:

<strong><strong>docker</strong></strong> push $<s<strong>t</strong>rong><strong><strong>DOCKER_REPO/spark</strong></strong></s<strong>t</strong>rong>:$<s<strong>t</strong>rong>SPARK_VERSION</s<strong>t</strong>rong>-wi<strong>t</strong>h-deps

For <strong>Azure</strong> <strong><strong>folks</strong></strong> wan<strong>t</strong>ing <strong>t</strong>o launch using <strong>Azure</strong> S<strong>t</strong>orage ra<strong>t</strong>her <strong>t</strong>han h<strong>t</strong><strong>t</strong>ps:

expor<strong>t</strong> <s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>_URL=wasbs://$FOLDER_NAME@$STORAGE_ACCT.blob.core.windows.ne<strong>t</strong>/$MY<s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>

For <strong>Google</strong> <strong><strong>folks</strong></strong> wan<strong>t</strong>ing <strong>t</strong>o launch using <strong><strong>GCS</strong></strong> ra<strong>t</strong>her <strong>t</strong>han h<strong>t</strong><strong>t</strong>ps:
expor<strong>t</strong> <s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>_URL=gs://$<s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>BUCKETNAME/$MY<s<strong>t</strong>rong>JAR</s<strong>t</strong>rong>
And <strong>t</strong>hen run <strong>t</strong>he same spark-submi<strong>t</strong> as shown previously. Wrapping up
Notably, each <strong>vendor</strong> has a more detailed <strong>guide</strong> to running <strong>Spark</strong> <strong>jobs</strong> on hosted <strong>K8s</strong> focused on their own <strong>platforms</strong> (<strong>e.g.</strong>, Azure’s <strong>guide</strong>, Google’s <strong>guide</strong>, etc. ), but hopefully this cross-vendor <strong>version</strong> <strong>shows</strong> you the relative <strong>portability</strong> between the different hosted <strong>K8s</strong> <strong>engines</strong> and our respective <strong>APIs</strong> with Spark. If you’re interested in helping <strong>join</strong> in <strong>Spark</strong> <strong><strong>code</strong></strong> <strong><strong>reviews</strong></strong>, you can see the contributing <strong>guide</strong> and also watch Karau’s past streamed <strong><strong>code</strong></strong> <strong><strong>reviews</strong></strong> on <strong><strong>YouTube</strong></strong> (and <strong>subscribe</strong> to her <strong><strong>YouTube</strong></strong> or <strong>Twitch</strong> <strong>channels</strong> for new livestreams). You can also follow the <strong>authors</strong> on their respective <strong>Twitter</strong> <strong>accounts</strong>: <strong>Alena Hall</strong> and Holden Karau. <strong>Continue</strong> reading <strong>How</strong> to run a custom <strong>version</strong> of <strong>Spark</strong> on hosted Kubernetes.</body>
</html>