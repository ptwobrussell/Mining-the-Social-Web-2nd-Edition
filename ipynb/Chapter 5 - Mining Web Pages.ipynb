{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mining the Social Web, 2nd Edition\n",
    "\n",
    "##Chapter 5: . Mining Web Pages: Using Natural Language Processing to Understand Human Language, Summarize Blog Posts, and More\n",
    "\n",
    "This IPython Notebook provides an interactive way to follow along with and explore the numbered examples from [_Mining the Social Web (2nd Edition)_](http://bit.ly/135dHfs). The intent behind this notebook is to reinforce the concepts from the sample code in a fun, convenient, and effective way. This notebook assumes that you are reading along with the book and have the context of the discussion as you work through these exercises.\n",
    "\n",
    "In the somewhat unlikely event that you've somehow stumbled across this notebook outside of its context on GitHub, [you can find the full source code repository here](http://bit.ly/16kGNyb).\n",
    "\n",
    "## Copyright and Licensing\n",
    "\n",
    "You are free to use or adapt this notebook for any purpose you'd like. However, please respect the [Simplified BSD License](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/LICENSE.txt) that governs its use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you find yourself wanting to copy output files from this notebook back to your host environment, see the bottom of this notebook for one possible way to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Using boilerpipe to extract the text from a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen\n",
      "The Louvre of the Industrial Age\n",
      "The Henry Ford is one of the world's great museums, and the world it chronicles is our own.\n",
      "by Tim O'Reilly | @timoreilly | +Tim O'Reilly | July 30, 2010\n",
      "This morning I had the chance to get a tour of The Henry Ford Museum in Dearborn, MI, along with Dale Dougherty, creator of Make: and Makerfaire, and Marc Greuther, the chief curator of the museum.  I had expected a museum dedicated to the auto industry, but it’s so much more than that.  As I wrote in my first stunned tweet, “it’s the Louvre of the Industrial Age.”\n",
      "When we first entered, Marc took us to what he said may be his favorite artifact in the museum, a block of concrete that contains Luther Burbank’s shovel, and Thomas Edison’s signature and footprints.  Luther Burbank was, of course, the great agricultural inventor who created such treasures as the nectarine and the Santa Rosa plum. Ford was a farm boy who became an industrialist; Thomas Edison was his friend and mentor. The museum, opened in 1929, was Ford’s personal homage to the transformation of the world that he was so much a part of.  This museum chronicles that transformation.\n",
      "The machines are astonishing – steam engines and coal-fired electric generators as big as houses, the first lathes capable of making other precision lathes (the makerbot of the 19th century), a ribbon glass machine that is one of five that in the 1970s made virtually all of the incandescent lightbulbs in the world, combine harvesters, railroad locomotives, cars, airplanes, even motels, gas stations, an early McDonalds’ restaurant and other epiphenomena of the automobile era.\n",
      "Under Marc’s eye, we also saw the transformation of the machines from purely functional objects to things of beauty.  We saw the advances in engineering — the materials, the workmanship, the design, over a hundred years of innovation.  Visiting The Henry Ford, as they call it, is a truly humbling experience.  I would never in a hundred years have thought of making a visit to Detroit just to visit this museum, but knowing what I know now, I will tell you confidently that it is as worth your while as a visit to Paris just to see the Louvre, to Rome for the Vatican Museum, to Florence for the Uffizi Gallery, to St. Petersburg for the Hermitage, or to Berlin for the Pergamon Museum.  This is truly one of the world’s great museums, and the world that it chronicles is our own.\n",
      "I am truly humbled that the Museum has partnered with us to hold Makerfaire Detroit on their grounds.  If you are anywhere in reach of Detroit this weekend, I heartily recommend that you plan to spend both days there.  You can easily spend a day at Makerfaire, and you could easily spend a day at The Henry Ford.\n",
      "P.S. Here are some of my photos from my visit .  (More to come soon. Can’t upload many as I’m currently on a plane.)\n",
      "Related:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from boilerpipe.extract import Extractor\n",
    "\n",
    "URL='http://radar.oreilly.com/2010/07/louvre-industrial-age-henry-ford.html'\n",
    "\n",
    "extractor = Extractor(extractor='ArticleExtractor', url=URL)\n",
    "\n",
    "print(extractor.getText())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Using feedparser to extract the text (and other fields) from an RSS or Atom feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning to our senses\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YToqMoCVWTM/returning-to-our-senses\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/5704563713_dd5a8c0de1_o_crop-81a1c2986a030549ca065f0a8ff8a7b9.jpg'/></p><p><em>An introduction to how human senses can be incorporated into design principles.</em></p>\r\n",
      "\r\n",
      "\r\n",
      "<h2>If a Tree Falls in the Forest…</h2>\r\n",
      "<p><a data-type=\"indexterm\" data-primary=\"fast thinking (System\" id=\"id-nxC1SLFrFM\"></a><a data-type=\"indexterm\" data-primary=\"Google (company)\" data-secondary=\"Google Earth Engine\" id=\"id-NnC7FnF2FQ\"></a><a data-type=\"indexterm\" data-primary=\"rainforest conservation\" id=\"id-8nCVInF8Fa\"></a><a data-type=\"indexterm\" data-primary=\"satellite imaging\" id=\"id-mwC9tyFBFb\"></a><a data-type=\"indexterm\" data-primary=\"senses and sensing\" data-secondary=\"rainforest conservation and\" id=\"id-dkCDhyFqFD\"></a><a data-type=\"indexterm\" data-primary=\"System\" id=\"id-BnCyTjFoFr\"></a>BRAZIL BEGAN USING SATELLITE imaging to monitor deforestation during the 1980s. This was the first large-scale, coordinated response to loggers and ranchers who had been illegally clearing the rainforests, and it worked, for a time. To avoid being spotted, loggers and ranchers began working more discreetly in smaller areas that were harder to detect (see <a data-type=\"xref\" href=\"#view_powered_by_google_earth_engine_showi\">Figure 1-1</a>).<a data-type=\"noteref\" id=\"fn1-1-marker\" href=\"#fn1-1\">1</a> This required a new approach to monitoring the forests.</p>\r\n",
      "<figure id=\"view_powered_by_google_earth_engine_showi\">\r\n",
      "<img src=\"https://d3ansictanv2wj.cloudfront.net/fig1-1-077c3ce119e30611d097151aa20b51f8.png\" alt=\"A view powered by Google Earth Engine showing global deforestation\">\r\n",
      "<figcaption>Figure 1-1. A view powered by Google Earth Engine showing global deforestation</figcaption>\r\n",
      "</figure>\r\n",
      "<p><a data-type=\"indexterm\" data-primary=\"Imazon (NGO)\" id=\"id-8nCmSQt8Fa\"></a><a data-type=\"indexterm\" data-primary=\"Rainforest Connection (NGO)\" id=\"id-mwCPFQtBFb\"></a><a data-type=\"indexterm\" data-primary=\"slow thinking (System\" id=\"id-dkCaIGtqFD\"></a><a data-type=\"indexterm\" data-primary=\"smartphones\" data-secondary=\"rainforest conservation and\" id=\"id-BnC0tJtoFr\"></a><a data-type=\"indexterm\" data-primary=\"System\" id=\"id-MnCnhgt9F8\"></a>Google Earth Engine and the Brazilian NGO, Imazon, worked together to create more powerful environmental monitoring capabilities. Their collaboration identified and mapped a much wider range of deforestation in much greater detail. With new analysis techniques for satellite imagery, they were able to classify forest topologies within the rainforest. This improved the accuracy of regional assessments, both for their contribution to the surrounding ecosystem and their vulnerability to human damage. They monitored the emergence of unofficial roads that marked new human activity. They modeled the environmental risks posed by agriculture, logging, and ranching to protected areas of the Amazon. They were also able to project future scenarios to help plan a more effective management strategy that balanced human land use with forest preservation.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/returning-to-our-senses'>Returning to our senses.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/YToqMoCVWTM\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Techniques for designing to reduce risk\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Me83-jJY2r8/techniques-for-designing-to-reduce-risk\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/danger-sign_crop-d678c1ed1b53bac6764b90eecd052006.jpg'/></p><p><em>We need to plan for what we can't see and design for a wide range of user scenarios to avoid bad outcomes.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/techniques-for-designing-to-reduce-risk'>Techniques for designing to reduce risk.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/Me83-jJY2r8\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Wrapping an RxJS observable stream into an Angular service\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HXbFtolEng4/wrapping-an-rxjs-observable-stream-into-an-angular-service\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/matrix-3109378_1920_crop-3792a6d5e1c9dff4eeab397741d94a9c.jpg'/></p><p><em>How to create an injectable service that emits a stream of values, and a UI component that subscribes to this stream, displaying its values in real time.</em></p><p>Angular’s dependency injection mechanism allows us to cleanly separate business logic (services) from UI (components). What if our app generates a stream of values and we want to implement it as an injectable service? Here I’ll show you how to create an injectable service that emits a stream of values, and a UI component that subscribes to this stream, displaying its values in real time.</p>\n",
      "\n",
      "<p>In <a href=\"https://yakovfain.com/2017/09/01/rxjs-essentials-part-3-using-observable-create/\">one of my RxJS blog posts</a>, I showed how you would use the method <code>Observable.create()</code> providing an observer as an argument. Let’s create a service with a method that will take an observer as an argument and emit the current time every second.</p>\n",
      "\n",
      "<pre>\n",
      "import {Observable} from 'rxjs/Observable';\n",
      " \n",
      "export class ObservableService{\n",
      " \n",
      "  createObservableService(): Observable&lt;Date&gt;{  // 1\n",
      " \n",
      "      return new Observable(  // 2\n",
      "          observer =&gt; {   // 3\n",
      "              setInterval(() =&gt;\n",
      "                  observer.next(new Date())  // 4\n",
      "              , 1000);\n",
      "          }\n",
      "      );\n",
      "  }\n",
      "}\n",
      "</pre>\n",
      "\n",
      "<p>1. Return an observable stream of dates</p>\n",
      "\n",
      "<p>2. Create an observable</p>\n",
      "\n",
      "<p>3. Provide an observer</p>\n",
      "\n",
      "<p>4. Emit the new date every second</p>\n",
      "\n",
      "<p>In this service, an instance of the RxJS Observable object is created, assuming that the subscriber will provide an Observer that knows what to do with the emitted data. Whenever the observable invokes the method <code>next(new Date())</code> on the observer, the subscriber will receive the current date and time. The data stream never throws an error and never completes.</p>\n",
      "\n",
      "<p>If you inject the <code>ObservableService</code> into the <code>AppComponent</code>, it invokes the method <code>createObservableService()</code> and subscribes to its stream of values, creating an observer that knows what to do with the data. The observer just assigns the received time to the variable currentTime which renders the time on UI.</p>\n",
      "\n",
      "<pre>\n",
      "import 'rxjs/add/operator/map';\n",
      "import {Component} from \"@angular/core\";\n",
      "import {ObservableService} from \"./observable.service\";\n",
      " \n",
      "@Component({\n",
      "  selector: 'app-root',\n",
      "  providers: [ ObservableService ],\n",
      "  template: `&lt;h1&gt;Custom observable service&lt;/h1&gt;\n",
      "       Current time: mediumTime  // 1\n",
      "  `})\n",
      "export class AppComponent {\n",
      " \n",
      "  currentTime: Date;\n",
      " \n",
      "  constructor(private observableService: ObservableService) { // 2\n",
      " \n",
      "    this.observableService.createObservableService()  // 3\n",
      "      .subscribe( data =&gt; this.currentTime = data );  // 4\n",
      "  }\n",
      "}\n",
      "</pre>\n",
      "\n",
      "<p>1. Display the time using the date pipe</p>\n",
      "\n",
      "<p>2. Inject the service that wraps the observable</p>\n",
      "\n",
      "<p>3. Create the observable and start emitting dates</p>\n",
      "\n",
      "<p>4. Subscribe to the stream of dates</p>\n",
      "\n",
      "<p>This app doesn’t use any servers, and you can see it in action in the great <a href=\"https://stackblitz.com/edit/angular-jqhemk\">Stackblitz online IDE</a>.</p>\n",
      "\n",
      "<p>In the browser’s window, the current time will be updated every second. You use <code>DatePipe</code> here with the format <code>'mediumTime'</code>, which displays only hours, minutes, and seconds (all date formats are described in the <a href=\"http://mng.bz/78lD\">DatePipe documentation</a>).</p>\n",
      "\n",
      "<p>This simple example demonstrates a basic technique for wrapping any application logic in an observable stream and subscribing to it. In this case, we use <code>setInterval()</code>, but you could replace it with any application-specific code that generates one or more values and sends them as a stream.</p>\n",
      "\n",
      "<p>Don’t forget about error handling and completing the stream if need be. The following code snippet shows a sample observable that sends one element to the observer, may throw an error, and tells the observer that the streaming is complete:</p>\n",
      "\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\">\n",
      "return new Observable(\n",
      "    observer =&gt; {\n",
      "      try {\n",
      "        observer.next('Hello from observable');\n",
      " \n",
      "        //throw (\"Got an error\");\n",
      " \n",
      "      } catch(err) {\n",
      "         observer.error(err);\n",
      "      } finally{\n",
      "         observer.complete();\n",
      "      }\n",
      "    }\n",
      ");\n",
      "</pre>\n",
      "\n",
      "<p>If you uncomment the line with a throw, <code>observer.error()</code> is invoked, which results in the invocation of the error handler on the subscriber if there is one.</p>\n",
      "\n",
      "<p>The data producer for the observable stream was generating date/time, but it could be any app code that generates some useful values—e.g., a WebSocket server generating stock quotes, auction bids, actions of online game players, etc. During workshops, I show a sample online auction app that has a Node server emulating users’ bids on products. That server uses a WebSocket connection to push new bids for products to all users that are interested in receiving them.</p>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/wrapping-an-rxjs-observable-stream-into-an-angular-service'>Wrapping an RxJS observable stream into an Angular service.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/HXbFtolEng4\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 27 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/frUwN7wejng/four-short-links-27-april-2018\n",
      "<p><em>Automating Commerce, Faster Training, MacOS Monitoring, and Formal Methods</em></p><ol>\n",
      "\t<li>\n",
      "<a href=\"https://loosethreads.com/thearchive/2017/05/30/mickey-drexler-and-the-death-of-a-supply-driven-world/\">Death of A Supply-Driven World</a> -- <i>The company employs under a dozen engineers that built a technology stack that integrates the brand’s planning, design, marketing and commerce systems into an all-knowing brain. If a shopper adds a product to the cart and then removes it, the brand knows and feeds this info back into its demand planning system. If a shopper returns one size and keeps another, this informs how the brand will reorder that product, if it does at all. All of this happens automatically, and while there are still humans making some decisions, the brand has no merchandising team. Most of its buying and planning is entirely automated.</i>\n",
      "</li>\n",
      "\t<li>\n",
      "<a href=\"https://eng.uber.com/accelerated-neuroevolution/\">Accelerated Neuro-Evolution</a> -- <a href=\"https://github.com/uber-common/deep-neuroevolution/tree/master/gpu_implementation\">open source code</a> that <i>maximizes the use of CPUs and GPUs in parallel. It runs deep neural networks on the GPU, the domains (e.g. video games or physics simulators) on the CPU, and executes multiple evaluations in parallel in a batch, allowing all available hardware to be utilized efficiently. [...] [I]t also contains custom TensorFlow operations, which significantly improve training speed.</i>\n",
      "</li>\n",
      "\t<li>\n",
      "<a href=\"https://blogs.dropbox.com/tech/2018/04/4696/\">MacOS Monitoring the Open Source Way</a> -- interesting read about how Dropbox security team monitor the employee laptops to catch malware, using <a href=\"https://osquery.io/\">osquery</a> for snapshots, <a href=\"https://github.com/google/santa\">Santa</a> for real-time process events, and <a href=\"http://www.openbsm.org/\">OpenBSM/Audit</a> for real-time syscall monitoring.</li>\n",
      "\t<li>\n",
      "<a href=\"https://www.hillelwayne.com/post/theorem-prover-showdown/\">The Great Theorem-Prover Showdown</a> -- he chose three imperative programs with variable assignment, and challenged theorem prover Twitter to formally prove the code's correctness, with interesting results. My favourite sentence in the write-up is <i>If the only result of this challenge is that Leftpad becomes the theorem prover’s “hello world”, I’ll be pretty happy.</i>\n",
      "</li>\n",
      "\t<li>\n",
      "<strong>Note: The email edition of Four Short Links will be discontinued on Monday, April 30</strong>. New editions of Four Short Links will still be published every weekday at <a href=\"https://www.oreilly.com/feed/four-short-links\">oreilly.com/4sl</a> and through the <a href=\"http://feeds.feedburner.com/FourShortLinks\">Four Short Links feed</a>. Please send questions about this change to onlinecap@oreilly.com.</li>\n",
      "</ol>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-27-april-2018'>Four short links: 27 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/frUwN7wejng\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "How to customize an Istio service mesh\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BZ75FEreSAU/how-to-customize-an-istio-service-mesh\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/louvre-2189967_crop-98f00246a2776bae3338fdcb88b2badf.jpg'/></p><p><em>Choose an Istio sidecar for reliability, observability, and security.</em></p><p>Even though service meshes provide value outside of the use of microservices and containers, it's in these environments that many teams first consider using a service mesh. The sheer volume of services that must be managed on an individual, distributed basis with microservices (versus centrally for a monolith) creates challenges for ensuring reliability, observability, and security of these services.</p>\r\n",
      " \r\n",
      "<p>Adoption of a container orchestrator addresses a layer of infrastructure needs, but leaves some application or service-level needs unmet. Rather than attempting to overcome distributed systems concerns by writing infrastructure logic into application code, some teams choose to manage these challenges with a service mesh. A service mesh can help by ensuring the responsibility of service management is centralized, avoiding redundant instrumentation, and making observability ubiquitous and uniform across services.</p>\r\n",
      "\r\n",
      "\r\n",
      "<h2>Choosing a service mesh</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Factors such as your teams’ operational and technology expertise, existing observability, and access control tooling will influence the service mesh components, adapters, and deployment model you choose. Among others, Istio is a popularly adopted, open source service mesh. Some choose Istio (or any service mesh) for the automatic and immediate visibility it provides into top-line service metrics. In fact, many become hooked on service meshes for the observability they provide alone.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>As a microservices platform, Istio is extensible through the way in which it offers choice of adapters and sidecars. Istio envelops and integrates with other open source projects to deliver a full-service mesh, which both bolsters its set of capabilities and offers a choice of which specific projects are included and deployed. Whether through Mixer adapters for observability or through swapping sidecars, Istio allows you to choose which components to include in your deployment.</p>\r\n",
      "\r\n",
      "<h2>Customizing an Istio service mesh</h2>\r\n",
      "\r\n",
      "<p>There are multiple deployment models you can use to lay down a service mesh. One of the most popular options is to deploy your service proxies as sidecars. Sidecarring your service proxy offers benefits like fine-grained policy enforcement and intra-cluster service-to-service encryption. This deployment model is the model of choice for Istio. Other Istio deployment choices include:</p>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "<strong>Mixer adapters:</strong> typically used for integrating with access control, telemetry, quota enforcement, and billing systems.</li>\r\n",
      "\t<li>\r\n",
      "<strong>Service proxies:</strong> abstract the network, translating requests between a client and service.</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<p>Though Envoy is the default service proxy sidecar, you may choose another service proxy for your sidecar. While there are multiple service proxies in the ecosystem, outside of Envoy, only two have currently demonstrated integration with Istio: Linkerd and NGINX. The arrival of choice in service proxies for Istio has generated a lot of excitement. <a href=\"https://linkerd.io/getting-started/istio/\">Linkerd’s integration</a> was created early in Istio’s 0.1.6 release. Similarly, the <a href=\"https://github.com/nginmesh/nginmesh\">nginMesh</a> project has drawn much interest in the use of NGINX as Istio’s service proxy, as many organizations have broad and deep operational expertise built around this battle-tested proxy.</p>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-3VpSk\">\r\n",
      "<p><em>Learn more about how to deploy your sidecar (Istio proxy) of choice in the free webcast <a href=\"http://www.oreilly.com/pub/e/3926?intcmp=il-data-webcast-lp-webcast_new_site_how-to-customize-an-istio-service-mesh_end_body_link_cta\">\"Istio—The extensible service mesh.\"</a></em></p>\r\n",
      "</aside>\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between O'Reilly and NGINX. <a href=\"http://www.oreilly.com/about/editorial_independence.html\">See our statement of editorial independence</a>.</em></p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/how-to-customize-an-istio-service-mesh'>How to customize an Istio service mesh.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/BZ75FEreSAU\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Teaching and implementing data science and AI in the enterprise\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/n8cBdBRINVA/teaching-and-implementing-data-science-and-ai-in-the-enterprise\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/5901727441_41dc4aa310_o_crop-b6f1cca3b997af8dbd252788b682ab7d.jpg'/></p><p><em>The O’Reilly Data Show Podcast: Jerry Overton on organizing data teams, agile experimentation, and the importance of ethics in data science.</em></p><p>In this episode of the <a href=\"https://www.oreilly.com/ideas/topics/oreilly-data-show-podcast\">Data Show</a>, I spoke with <a href=\"https://www.linkedin.com/in/jerryaoverton/\">Jerry Overton</a>, senior principal and distinguished technologist at <a href=\"http://www.dxc.technology/\">DXC Technology</a>. I wanted the perspective of someone who works across industries and with a variety of companies. I specifically wanted to explore the current state of data science and AI within companies and public sector agencies. As much as we talk about use cases, technologies, and algorithms, there are also important issues that practitioners like Overton need to address, including privacy, security, and ethics. Overton has long been involved in teaching and mentoring new data scientists, so we also discussed some tips and best practices he shares with new members of his team.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/teaching-and-implementing-data-science-and-ai-in-the-enterprise'>Teaching and implementing data science and AI in the enterprise.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/n8cBdBRINVA\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Building tools for the AI applications of tomorrow\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/e3rGotvDf9A/building-tools-for-the-ai-applications-of-tomorrow\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/no-one-3086366_1920_crop-c237deebc3dd8a4e0e7eac59f20dee42.jpg'/></p><p><em>We’re currently laying the foundation for future generations of AI applications, but we aren’t there yet.</em></p><p>For the last few years, AI has been almost synonymous with deep learning (DL). We’ve seen AlphaGo touted as an example of deep learning. We’ve seen deep learning used for <a href=\"http://aiweirdness.com/post/160776374467/new-paint-colors-invented-by-neural-network\">naming paint colors</a> (not very successfully), <a href=\"https://www.smithsonianmag.com/smart-news/new-rembrandt-created-347-years-after-the-dutch-masters-death-180958664/\">imitating Rembrandt</a> and other great painters, and <a href=\"https://machinelearningmastery.com/inspirational-applications-deep-learning/\">many other applications</a>. Deep learning has been successful in part because, as François Chollet tweeted, “<a href=\"https://twitter.com/fchollet/status/987331839839752193\">you can achieve a surprising amount using only a small set of very basic techniques</a>.” In other words, you can accomplish things with deep learning that don’t require you to become an AI expert. Deep learning’s apparent simplicity--the small number of basic techniques you need to know--makes it much easier to “democratize” AI, to build a core of AI developers that don’t have Ph.D.s in applied math or computer science.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>But having said that, there’s a deep problem with deep learning. As Ali Rahimi <a href=\"http://www.argmin.net/2018/01/25/optics/\">has argued</a>, we can often get deep learning to work, but we <a href=\"https://www.youtube.com/watch?v=Qi1Yry33TQE&amp;feature=youtu.be&amp;t=13m32s\">aren’t close to understanding how, when, or why</a> it works: “we’re equipping [new AI developers] with little more than folklore and pre-trained deep nets, then asking them to innovate. We can barely agree on the phenomena that we should be explaining away.” Deep learning’s successes are suggestive, but if we can’t figure out why it works, its value as a tool is limited. We can build an army of deep learning developers, but that won’t help much if all we can tell them is, “Here are some tools. Try random stuff. Good luck.”</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>However, nothing is as simple as it seems. The best applications we’ve seen to date have been hybrid systems. AlphaGo wasn’t a pure deep learning engine; it <a href=\"https://blog.google/topics/machine-learning/alphago-machine-learning-game-go/\">incorporated</a> Monte Carlo Tree Search, and at least two deep neural networks. At O’Reilly’s New York AI Conference in 2017, <a href=\"https://www.oreilly.com/ideas/building-machines-that-learn-and-think-like-people\">Josh Tenenbaum</a> and <a href=\"https://www.oreilly.com/ideas/machines-as-thought-partners\">David Ferrucci</a> sketched out systems they are working on, systems that combine deep learning with other ideas and methods. Tenenbaum is working with one-shot learning, <a href=\"https://arxiv.org/abs/1604.00289\">imitating the human ability</a> to learn based on a single experience, and Ferrucci is working on building cognitive models that enable machines to understand human language in a meaningful way, not just pattern matching. DeepStack’s <a href=\"https://www.deepstack.ai/\">poker playing system</a> combines neural networks with <a href=\"https://www.quora.com/What-is-an-intuitive-explanation-of-counterfactual-regret-minimization\">counterfactual regret minimization</a> and heuristic search.</p>\r\n",
      "\r\n",
      "<h2>Adding structure to improve models</h2>\r\n",
      "\r\n",
      "<p>The fundamental idea behind deep learning is very simple: deep learning systems are neural networks with several hidden layers. Each neuron is very simple: it takes a number of inputs from previous layers, combines them according to a set of weights, and produces an output that’s passed to the next layer. The network doesn’t really care whether it’s processing images, text, or telemetry. That simplicity, though, is a hint that we’re missing out on a lot of structure that’s inherent in data. Images and texts aren’t the same; they’re structured differently. <a href=\"http://www.earningmyturns.org/2017/06/a-computational-linguistic-farce-in.html\">Languages</a> have a lot of internal structure. As the computational linguist Chris Manning <a href=\"https://www.youtube.com/watch?v=fKk9KhGRBdI&amp;t=10m48s\">says</a>:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<blockquote><p>I think the current era where everyone touts this mantra of fast GPUs, massive data, and these great deep learning algorithms has ... sent computational linguistics off-track. Because it is the case that if you have huge computation and massive amounts of data, you can do a lot ... with a simple learning device. But those learners are extremely bad learners. Human beings are extremely good learners. What we want to do is build AI devices that are also extremely good learners. ... The way to achieve those learners is to put much more innate structures.</p></blockquote>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If we’re going to make AI applications that understand language as well as humans do, we will have to take advantage of the structures that are in language. From that standpoint, deep learning has been a fruitful dead end: it’s a shortcut that has prevented us from asking the really important questions about how knowledge is structured. Gary Marcus makes an <a href=\"https://arxiv.org/abs/1801.05667\">argument</a> that’s even more radical:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<blockquote><p>There is a whole world of possible innate mechanisms that AI researchers might profitably consider; simply presuming by default it is desirable to include little or no innate machinery seems, at best, close-minded. And, at worst, an unthinking commitment to relearning everything from scratch may be downright foolish, effectively putting each individual AI system in the position of having to recapitulate a large portion of a billion years of evolution.</p></blockquote>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Deep learning began with a model that was, at least in principle, based on the human brain: the interconnection of neurons, and the ancient notion that human brains start out as a <a href=\"https://en.wikipedia.org/wiki/Tabula_rasa\">blank slate</a>. Marcus is arguing that humans are born with innate abilities which are still very poorly understood--for example, the ability to learn language, or the ability to form abstractions. For AI to progress beyond deep learning, he suggests that researchers must learn how to model these innate abilities.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There are other paths forward. Ben Recht has written <a href=\"http://www.argmin.net/2018/04/19/outsider-rl/\">a series of posts</a> sketching out how one might approach problems that fall under reinforcement learning. He is also concerned with the possibility that deep learning, as practiced today, promises more than it can deliver:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<blockquote><p>If you read Hacker News, you’d think that deep reinforcement learning can be used to solve any problem. ... I personally get suspicious when audacious claims like this are thrown about in press releases, and I get even more suspicious when <a href=\"https://arxiv.org/abs/1709.06560\">other researchers call into question their reproducibility</a>.</p></blockquote>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Recht argues for taking a comprehensive view, and reviews the possibility for augmenting reinforcement learning with techniques from optimal control and dynamical systems. This allows RL models to <a href=\"http://www.argmin.net/2018/04/19/pid/\">benefit from research</a> results and techniques used in many real-world applications. He <a href=\"http://www.argmin.net/2018/03/13/pg-saga/\">notes</a>:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<blockquote><p>By throwing away models and knowledge, it is never clear if we can learn enough from a few instances and random seeds to generalize.</p></blockquote>\r\n",
      "\r\n",
      "<h2>AI is more than machine learning</h2>\r\n",
      "\r\n",
      "<p>As Michael Jordan pointed out in <a href=\"https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7\">a recent post</a>, what is called AI is often machine learning (ML). As someone who organizes AI conferences, I can attest to this: many of the proposals we receive are for standard machine learning applications. The confusion was inevitable: when calling a research project “artificial intelligence” was hardly respectable, we used the term “machine learning.” ML became a shorthand for “the parts of AI that work.” These parts, up to and including deep learning, were basically large-scale data analysis. Now that the tides of buzz have shifted, and everyone wants AI, machine learning applications are AI again.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>But a <a href=\"https://www.oreilly.com/ideas/the-state-of-ai-adoption\">full-fledged AI application, such as an autonomous vehicle</a>, requires much more than data analysis. It will require progress in many areas that go well beyond pattern recognition. To build an autonomous vehicle and other true AI applications, we will need significant advances in sensors and other hardware; we will need to learn how to build software for “edge devices,” which includes understanding how to partition problems between the edge devices and some kind of “cloud”; we will need to develop infrastructure for simulation and distributed computation; and we will need to understand how to craft the user experience for truly intelligent devices.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Jordan highlights the need for further research in two important areas:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><strong>Intelligence augmentation (IA):</strong> Tools that are designed to augment human intelligence and capabilities. These include search engines (which remember things we can’t), automated translation, and even aids for artists and musicians. These tools might involve high-level reasoning and thought, though current implementations don’t.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><strong>Intelligent infrastructure (II):</strong> Jordan defines II as “a web of computation, data, and physical entities exist that make human environments more supportive, interesting and safe.” This would include networks to share medical data safely, systems to make transportation safer (including smart cars and smart roads), and many other applications. Intelligent infrastructure is about managing flows of data in ways that support human life.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>What’s most important about Jordan’s argument, though, is that we won’t get either IA or II if we focus solely on deep learning. They are inherently multidisciplinary. Deep learning will inevitably be part of the solution, but just as inevitably, it won’t be the whole solution. It may even be a very small part.</p>\r\n",
      "\r\n",
      "<h2>Closing thoughts</h2>\r\n",
      "\r\n",
      "<p>Researchers from many institutions are building tools for creating the AI applications of the future. While there is still a lot of work to be done on deep learning, researchers are looking well beyond DL to build the next generation of AI systems. UC Berkeley's RISE Lab has <a href=\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-159.pdf\">sketched out a research agenda</a> that involves systems, architectures, and security.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Ameet Talwalkar’s <a href=\"https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning\">recent post</a> lists a number of research directions that should benefit industrial machine learning platforms. Industrial machine learning will have to meet system requirements, such as memory limitations, power budgets, and hard real time; they must be easy to deploy and to update, particularly since data models tend to grow stale over time; and they must be safe. Humans must understand how applications make decisions, along with the likely consequences of those decisions. These applications must take ethics into account.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>These are all requirements for Jordan’s intelligent infrastructure. Over the past few years, we’ve seen many examples of machine learning put to questionable purposes, ranging from <a href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\">setting bail and determining prison sentences</a> to <a href=\"https://www.wired.com/story/why-zuckerberg-15-year-apology-tour-hasnt-fixed-facebook/\">targeted advertising, emotional manipulation, and the spreading of misinformation</a>, that point us to a different set of needs. The research agenda for AI needs to take into account fairness and bias, transparency, privacy and user control over data, and <a href=\"https://www.oreilly.com/ideas/the-importance-of-transparency-and-user-control-in-machine-learning\">the models built from that data</a>. These issues encompass everything from ethics to design: getting informed consent, and explaining what that consent means, is not a trivial design problem. We’re only starting to understand how these disciplines connect to research in artificial intelligence. Fortunately, we’re seeing increasing interest within the data community in connecting ethics to practice. Events like the <a href=\"https://www.bloomberg.com/company/d4gx/\">Data For Good Exchange</a> (D4GX), the <a href=\"https://fatconference.org/\">Conference on Fairness, Accountability, and Transparency</a> (FAT*), and others are devoted to data ethics.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Talwalkar notes that air travel didn’t become commonplace until nearly 50 years after the Wright Brothers. While they were the first to achieve flight, many more developments were needed to make flying safe, inexpensive, and convenient. We’re at a similar stage in the history of AI. We’ve made progress in a few basic areas, and what we ultimately build will no doubt be amazing. We’re currently laying the foundation for future generations of AI applications, but we aren’t there yet.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><strong>Related content</strong>:</p>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li><a href=\"https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning\">“Toward the Jet Age of machine learning”</a></li>\r\n",
      "\t<li><a href=\"https://www.oreilly.com/ideas/open-endedness-the-last-grand-challenge-youve-never-heard-of\">“Open-endedness: The last grand challenge you’ve never heard of”</a></li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.oreilly.com/ideas/language-understanding-remains-one-of-ais-grand-challenges\">\"Language understanding remains one of AI’s grand challenges\"</a>: David Ferrucci on the evolution of AI systems for language understanding</li>\r\n",
      "\t<li><a href=\"https://www.oreilly.com/ideas/the-machine-learning-paradox\">“The machine learning paradox”</a></li>\r\n",
      "\t<li><a href=\"https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers\">“We need to build machine learning tools to augment machine learning engineers”</a></li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.oreilly.com/ideas/building-and-deploying-large-scale-machine-learning-pipelines\">\"Building and deploying large-scale machine learning pipelines\"</a>: Ben Recht on why we need primitives, pipeline synthesis tools, and most importantly, error analysis and verification.</li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.oreilly.com/ideas/how-to-train-and-deploy-deep-learning-at-scale\">\"How to train and deploy deep learning at scale\"</a>: Ameet Talwalkar on large-scale machine learning</li>\r\n",
      "</ul>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/building-tools-for-the-ai-applications-of-tomorrow'>Building tools for the AI applications of tomorrow.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/e3rGotvDf9A\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 26 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/TwyE7jrx8V8/four-short-links-26-april-2018\n",
      "<p><em>DNA for Data, Project Names, VGA SDR, and Image Magic</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://spectrum.ieee.org/semiconductors/devices/exabytes-in-a-test-tube-the-case-for-dna-data-storage\">Exabytes in a Test Tube: The Case for DNA Data Storage</a> -- still in its infancy, but researchers are drawn by high storage density (up to 1E12 GB/gram), unpowered, and durable in \"ideal\" conditions. There are even people working on random-access tech.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/HotelsDotCom/waggle-dance\">Waggle Dance</a> -- <i>Hive federation service. Enables disparate tables to be concurrently accessed across multiple Hive deployments.</i> (<a href=\"https://hive.apache.org/\">Hive</a> is an Apache data warehouse project.) This easily wins today's award for Best Project Name. (<a href=\"https://github.com/HotelsDotCom/circus-train\">Circus Train</a> is a good name, but not as {fingerkiss} as Waggle Dance.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://osmocom.org/projects/osmo-fl2k/wiki/Osmo-fl2k\">VGA as SDR</a> -- this is wild. <i>osmo-fl2k allows you to use USB 3.0 to VGA adapters based on the Fresco Logic FL2000 chip, which are available for around $5, as general purpose DACs and SDR transmitter generating a continuous stream of samples by avoiding the HSYNC and VSYNC blanking intervals.</i> Can <i>transmit low-power FM, DAB, DVB-T, GSM, UMTS, and GPS signals.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1804.07723\">Image Inpainting for Irregular Holes Using Partial Convolutions</a> -- the <a href=\"http://bit.ly/2KfoPEv\">video</a> is solid gold wow. (via <a href=\"https://news.developer.nvidia.com/new-ai-imaging-technique-reconstructs-photos-with-realistic-results/\">NVIDIA developer news</a>)</li>\r\n",
      "<li>\r\n",
      "<strong>Note: The email edition of Four Short Links will be discontinued on Monday, April 30</strong>. New editions of Four Short Links will still be published every weekday at <a href=\"https://www.oreilly.com/feed/four-short-links\">oreilly.com/4sl</a> and through the <a href=\"http://feeds.feedburner.com/FourShortLinks\">Four Short Links feed</a>. Please send questions about this change to onlinecap@oreilly.com.</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-26-april-2018'>Four short links: 26 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/TwyE7jrx8V8\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 25 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gsxJX98XdjA/four-short-links-25-april-2018\n",
      "<p><em>Music Biz, Amazon DNS Hijack, Embedded Platform, and Tech Change</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://musicindustryblog.wordpress.com/2018/04/19/global-recorded-music-revenues-grew-by-1-4-billion-in-2017/\">Music Industry's \"Fantastic 2017\"</a> -- <i>That $1.4 billion of growth puts the global total just below 2008 levels ($17.7 billion), meaning that the decline wrought through much of the last 10 years has been expunged. The recorded music business is locked firmly in growth mode, following nearly $1 billion growth in 2016.</i> <a href=\"https://boingboing.net/2018/04/24/which-side-are-you-on-3.html\">Cory Doctorow</a> makes the point that while the \"music industry\" is booming, artist incomes aren't growing at the same rate. Or, indeed, at all.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\">Amazon's DNS Hijacked For Two Hours</a> -- in service of raiding a cryptocurrency website.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://nerves-project.org/\">Nerves</a> -- <i>Pack your whole application into as little as 12MB and have it start in seconds by booting a lean cross-compiled Linux directly to the battle-hardened Erlang VM. Let Nerves take care of the network, discovery, I/O, firmware updates, and more. Focus on what matters, and have fun writing robust and maintainable software.</i> Nifty approach to a very real problem.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.dgsiegel.net/files/refs/Postman%20-%20Five%20Things%20We%20Need%20to%20Know%20about%20Technological%20Change.pdf\">Five Things We Need to Know About Technological Change</a> (Neil Postman) -- this is incredibly prescient and good. <i>Technological change is not additive; it is ecological.[...] A new medium does not add something; it changes everything. In the year 1500, after the printing press was invented, you did not have old Europe plus the printing press. You had a different Europe. After television, America was not America plus television. Television gave a new coloration to every political campaign, to every home, to every school, to every church, to every industry, and so on. That is why we must be cautious about technological innovation. The consequences of technological change are always vast, often unpredictable, and largely irreversible.</i> See also <a href=\"http://bit.ly/2HZHQu3\">a related talk by Postman</a>. (via <a href=\"https://twitter.com/dgsiegel\">Daniel G. Siegel</a>)</li>\r\n",
      "<li>\r\n",
      "<strong>Note: The email edition of Four Short Links will be discontinued on Monday, April 30</strong>. New editions of Four Short Links will still be published every weekday at <a href=\"https://www.oreilly.com/feed/four-short-links\">oreilly.com/4sl</a> and through the <a href=\"http://feeds.feedburner.com/FourShortLinks\">Four Short Links feed</a>. Please send questions about this change to onlinecap@oreilly.com.</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-25-april-2018'>Four short links: 25 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/gsxJX98XdjA\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Toward the Jet Age of machine learning\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9CQRHiYLMr8/toward-the-jet-age-of-machine-learning\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/2558061651_27eec20312_o_crop-1f1b13334578ddd71b9841f3d250867d.jpg'/></p><p><em>Solving the challenges of efficiency, automation, and safety will require cooperation between researchers and engineers spanning both academia and industry.</em></p><p>Machine learning today resembles the dawn of aviation. In 1903, dramatic flights by the Wright brothers ushered in the Pioneer Age of aviation, and within a decade, there was widespread belief that powered flight would revolutionize transportation and society more generally. Machine learning (ML) today is also rapidly advancing. We have recently witnessed remarkable breakthroughs on important problems including image recognition, speech translation, and natural language processing, and major technology companies are investing billions of dollars to transform themselves into ML-centric organizations. There is a growing conviction that ML holds the key to some of society’s most pressing problems.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-31Vik\"><img alt=\"A new engineering discipline brought aviation into the Jet Age\" width=\"65%\" src=\"https://d3ansictanv2wj.cloudfront.net/talwalkar_figure1-fe8eb482f6c59f9910fdee1626f48eba.png\"><figcaption><span class=\"label\">Figure 1. </span>The Wright brothers’ first powered airplane traveled 120 feet during its initial 12 second flight on December 17, 1903, at Kitty Hawk. Credit <a href=\"https://www.pancakeandfranks.com/\">Stacy Pancake</a>.</figcaption></figure>\r\n",
      "\r\n",
      "<p>However, this excitement should also be met with caution. For all the enthusiasm that the Wright brothers generated, nearly half a century would pass before widespread commercial aviation finally became a reality. During the Pioneer Age, aviation was largely restricted to private, sport, and military use. Getting to the Jet Age required a series of fundamental innovations in aeronautical engineering—monoplane wings, aluminum designs, turbine engines, stress testing, jumbo jets, etc.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-5vyir\"><img alt=\"commercial Jet Age in the 1950s\" width=\"75%\" src=\"https://d3ansictanv2wj.cloudfront.net/talwalkar_figure2-8a9d02d8fdd388835e3be7265728f2c4.png\"><figcaption><span class=\"label\">Figure 2. </span>Decades of advances in aeronautical engineering led to the Jet Age in the 1950s, which fundamentally changed societal behavior and enabled us to tackle new challenges—e.g., space exploration. Credit: Stacy Pancake.</figcaption></figure>\r\n",
      "\r\n",
      "<p>Simply put, we needed to invent aeronautical engineering before we could transform the aviation industry. Similarly, we need to invent a new kind of engineering to build ML applications. Data-driven software development is radically different from conventional software development, as it targets complex applications domains (e.g., vision, speech, language) and focuses on learned behaviors instead of rule-based operations (e.g., training deep neural networks on massive data sets versus hand-coded if-then-else statements). Currently, very few organizations have the expertise to do this kind of engineering, and we are just scratching the surface of the potential for ML-powered technology. We describe three key challenges of this new development paradigm below.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-6nGie\"><img alt=\"The turbine engine\" width=\"40%\" src=\"https://d3ansictanv2wj.cloudfront.net/talwalkar_figure3-aeb37da231f8248fb595f650abb4aa49.png\"><figcaption><span class=\"label\">Figure 3. </span>The turbine engine, developed over several decades, resulted in planes that were dramatically faster and more efficient, enabling travel around the world in less than a day. Credit: Stacy Pancake.</figcaption></figure>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Challenge 1: Efficiency</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p id=\"F1\">Modern ML applications typically involve complex models and massive data sets, requiring significant computational and storage resources. For instance, engineers at Google Brain needed more than 250,000 GPU hours to train a neural translation model for a <em>single</em> pair of languages (English and German), which costs about $200,000 on Google Compute Engine.<a href=\"#_ftn1\"><sup>[1]</sup></a> In response, a wide range of specialized hardware solutions are being developed (e.g., GPUs, TPUs, massively parallel CPUs, FPGAs) to improve the speed, energy efficiency, and cost of ML-powered applications.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>However, effectively leveraging heterogenous hardware will require us to fundamentally redesign ML software itself. In particular, systems-aware algorithms and software are needed (i) to efficiently train models on massively parallel and heterogeneous hardware, and (ii) to satisfy service level agreements (SLAs) related to latency, power consumption, and memory footprint constraints for production deployments. Advances in hardware must be closely coupled with algorithmic and software innovation in order to develop and deploy ML-based applications in a timely and economical fashion.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-5dEi7\"><img alt=\"Automation in modern commercial aviation\" width=\"40%\" src=\"https://d3ansictanv2wj.cloudfront.net/talwalkar_figure4-41a732f201d6c773bb89e5267673a740.png\"><figcaption><span class=\"label\">Figure 4. </span>Automation is widespread in modern commercial aviation, including plane manufacturing / testing, air traffic control, and even operating planes. Credit: Stacy Pancake.</figcaption></figure>\r\n",
      "\r\n",
      "\r\n",
      "<h2>Challenge 2: Automation</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>In addition to being computationally intensive, ML-powered applications are incredibly labor-intensive for ML engineers to train, debug, and deploy. First, even selecting the appropriate computational platform is challenging, given the rapidly changing hardware landscape and diverse set of available cloud-based offerings. Second, the quality of an ML model is highly sensitive to hyperparameters; tuning these hyperparameters is crucial for accuracy but is often labor-intensive and expensive in computational cost. Third, utilizing parallel hardware at training time is highly non-trivial. Naively boosting computational power often does not result in meaningful speedups, and fair and effective sharing of cluster resources among users can be challenging.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p id=\"F2\">To make things worse, developing ML applications is not a one-shot process: data changes over time, and therefore models and systems must adapt. Diagnosing and updating stale models is challenging, and exacerbated by the surprising difficulty (and sometimes impossibility<a href=\"#_ftn2\"><sup>[2]</sup></a>) of reproducing the behavior of ML applications. These issues are due to many factors, including (i) the statistical or \"fuzzy\" nature of these applications; (ii) the complexity of ML applications (e.g., pipeline jungles<a href=\"#_ftn3\"><sup>[3]</sup></a>); and (iii) ad-hoc development processes in which both code and data evolve over time with inadequate (and sometimes non-existent) controls. Given the shortage and cost of ML talent and the increased demands for ML technology, there is a pressing need to automate and simplify these development and deployment processes.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-3EaiZ\"><img alt=\"aviation safety\" width=\"75%\" src=\"https://d3ansictanv2wj.cloudfront.net/talwalkar_figure5-49fc42b4960df59f7af6155c63cdcc5a.png\"><figcaption><span class=\"label\">Figure 5. </span>The widespread adoption of commercial aviation hinged on dramatic advances in aviation safety, including advances in plane design and testing, as well as the creation of international and domestic regulatory bodies—e.g., the ICAO and FAA. Credit: Stacy Pancake.</figcaption></figure>\r\n",
      "\r\n",
      "<h2>Challenge 3: Safety</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>As ML applications become more ubiquitous and increasingly influence societal interactions (e.g., curating news, determining credit worthiness, influencing criminal sentencing, navigating vehicles autonomously), the safety risks associated with the misuse or misunderstanding of this technology are magnified. It is, thus, critical to understand and audit the behavior of ML applications: do we understand how models are making their decisions? What is the confidence / uncertainty associated with individual decisions? Do these predictions pose immediate threats to an individual or to society? What are the broader ethical ramifications of a given ML application? What information is being used to make decisions? Is individual privacy adequately being preserved?</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Unfortunately, ML applications do not provide us with straightforward answers to these questions. They are inherently data-driven and not based on simple rules, and we have a fundamental lack of understanding as to why leading ML approaches (e.g., deep learning models) even work in the first place. In addition to advancing our basic scientific understanding, it is paramount that we develop robust ML-centric engineering processes to mitigate potential safety risks. These new processes must address the complexity and uncertainty inherent to ML applications.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>The interdisciplinary path forward</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>These challenges—efficiency, automation, and safety—won’t be solved overnight. It is clear that they touch a broad set of disciplines, and consequently devising effective solutions will require cooperation between researchers and engineers spanning both academia and industry.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p id=\"F4\">From an academic perspective, we are already witnessing encouraging signs of interdisciplinary progress, as these core challenges have spurred the development of new research communities. Two notable examples are: (i) the SysML<a href=\"#_ftn4\"><sup>[4]</sup></a> research community that works at the intersection of systems and ML to design system-aware algorithms and identify best practices for learning systems; and (ii) the FatML<a href=\"#_ftn5\"><sup>[5]</sup></a> research community that brings together a diverse set of social and quantitative researchers and practitioners concerned with fairness, accountability, and transparency in ML.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>However, we ultimately want to move beyond academic research, and leverage cutting-edge theoretical advances in order to design and build increasingly robust and sophisticated engineering systems. To do so will require coordination between researchers working on more abstract and theoretical problems and engineers who understand industrial processes and real-world deployment requirements. While we have a long way to go before we arrive at the Jet Age of ML, continued collaborative efforts will truly enable ML to take flight.</p>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-5qBSE\">\r\n",
      "\r\n",
      "<div id=\"_ftn1\">\r\n",
      "<p><sup>[1]</sup> Britz et al. Massive Exploration of Neural Machine Translation Architectures, Conference on Empirical Methods in Natural Language Processing, 2017. <a href=\"#F1\">(Return)</a></p>\r\n",
      "</div>\r\n",
      "\r\n",
      "<div id=\"_ftn2\">\r\n",
      "<p><sup>[2]</sup> For instance, various TensorFlow operations on GPUs or multi-threaded CPUs are known to be non-deterministic; see <a href=\"https://github.com/tensorflow/tensorflow/issues/3103\">https://github.com/tensorflow/tensorflow/issues/3103</a> for more details. <a href=\"#F2\">(Return)</a></p>\r\n",
      "</div>\r\n",
      "\r\n",
      "<div id=\"_ftn3\">\r\n",
      "<p><sup>[3]</sup> Sculley et al. Hidden Technical Debt in Machine Learning Systems, Neural Information Processing Systems, 2015. <a href=\"#F2\">(Return)</a></p>\r\n",
      "</div>\r\n",
      "\r\n",
      "<div id=\"_ftn4\">\r\n",
      "<p><sup>[4]</sup> <a href=\"http://www.sysml.cc\">http://www.sysml.cc</a> <a href=\"#F4\">(Return)</a></p>\r\n",
      "</div>\r\n",
      "\r\n",
      "<div id=\"_ftn5\">\r\n",
      "<p><sup>[5]</sup> <a href=\"https://fatconference.org\">https://fatconference.org</a> <a href=\"#F4\">(Return)</a></p>\r\n",
      "</div>\r\n",
      "</aside>\r\n",
      "\r\n",
      "<p><strong>Related content:</strong></p>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.oreilly.com/ideas/how-to-train-and-deploy-deep-learning-at-scale\">\"How to train and deploy deep learning at scale\"</a>: Ameet Talwalkar on large-scale machine learning.</li>\r\n",
      "</ul>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning'>Toward the Jet Age of machine learning.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/9CQRHiYLMr8\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 24 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lSu48BGq_L4/four-short-links-24-april-2018\n",
      "<p><em>IoT, Migrations, Prisoner's Dilemma, and Security</em></p><ol>\n",
      "\t<li>\n",
      "<a href=\"https://iot-inspector.princeton.edu/\">IoT Inspector</a> -- The Princeton University research team is digging into the traffic that IoT devices do, to identify malicious or otherwise dodgy behaviour. They want to know what IoT devices you have so they can test them. They'll release their packet capture and analysis tool as open source. (via <a href=\"https://boingboing.net/2018/04/23/promiscuous-mode.html\">BoingBoing</a>)</li>\n",
      "\t<li>\n",
      "<a href=\"https://lethain.com/migrations/\">Migrations</a> (Will Larson) -- very good explanation of how to manage migrations which are <i>usually the only available avenue to make meaningful progress on technical debt.</i> (via <a href=\"https://simonwillison.net/2018/Apr/23/will-larson/\">Simon Willison</a>)</li>\n",
      "\t<li>\n",
      "<a href=\"http://www.insidehighered.com/news/2013/02/12/students-boycott-final-challenge-professors-grading-policy-and-get\">Beating the Prisoner's Dilemma</a> -- In 2013 <i>as the semester ended in December, students in Fröhlich’s \"Intermediate Programming,\" \"Computer System Fundamentals,\" and \"Introduction to Programming for Scientists and Engineers\" classes decided to test the limits of the policy, and collectively planned to boycott the final. Because they all did, a zero was the highest score in each of the three classes, which, by the rules of Fröhlich’s curve, meant every student received an A.</i> How did they manage to avoid defection? (If just one student sat the test, that person would get an A and everyone else fail) <i>The students waited outside the rooms to make sure that others honored the boycott, and were poised to go in if someone [broke the pact]. No one did, though.</i> Prisoner's Dilemma only works if the prisoners can't communicate. (via <a href=\"http://freakonomics.com/2013/02/20/how-to-game-a-grading-curve/\">Freakonomics</a> and <a href=\"https://twitter.com/secparam/status/988161858665418753\">Ian Miers</a>)</li>\n",
      "\t<li>\n",
      "<a href=\"http://insct.syr.edu/wp-content/uploads/2015/05/Schell_Achilles_Heel.pdf\">Computer Security: The Achilles' Heel of the Air Force?</a> -- incredibly prescient 1979 article on the important problems of security. The stories of repeatedly improving early systems like GCOS and MULTICS are super-interesting and rich with parallels for today. <i>A contract cannot provide security. Basically, the same GCOS system was selected for a major command and control system. Advocates assured the users that it would be made multilevel secure because security was required by the contract. An extensive tiger team evaluation found there were many deep and complex security flaws that defied practical repair—the computer was finally deemed not only insecure but insecurable.</i>\n",
      "</li>\n",
      "\t<li>\n",
      "<strong>Note: The email edition of Four Short Links will be discontinued on Monday, April 30</strong>. New editions of Four Short Links will still be published every weekday at <a href=\"https://www.oreilly.com/feed/four-short-links\">oreilly.com/4sl</a> and through the <a href=\"http://feeds.feedburner.com/FourShortLinks\">Four Short Links feed</a>. Please send questions about this change to onlinecap@oreilly.com.</li>\n",
      "</ol>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-24-april-2018'>Four short links: 24 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/lSu48BGq_L4\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "The Intertwingularity is near: When humans transcend print media\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/aVwQuI6UP9M/the-intertwingularity-is-near-when-humans-transcend-print-media\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/fractal-1761271_1920_crop-6b9aeb8d719d94f5ea1225134d1fb21e.jpg'/></p><p><em>Both reproducible science and open source are necessary for collaboration at scale&mdash;the nexus for that intermingling is Jupyter.</em></p><p><em>(Apologies to <a href=\"http://www.singularity.com/\">Ray Kurzweil</a> for the title puns)</em></p>\r\n",
      "\r\n",
      "<p>Recent one-day events showcased the Jupyter community in Boston and Atlanta, <a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\">with another Jupyter Pop-up event coming on May 15 in Washington, D.C.</a> At the same time, <a href=\"https://jupyter.org/\">Project Jupyter</a> has been in the news. We’re finding overlap between the themes explored at these community events and recent articles written about Jupyter. That overlap, in turn, illustrates the kinds of dialog that we’re looking forward to at <a href=\"http://jupytercon.com/?intcmp=il-data-confreg-lp-jpny18_new_site_the-intertwingularity-is-near_body_text_link\">JupyterCon</a> this August.</p>\r\n",
      "\r\n",
      "<p>In the news, notably there was the James Somers article, <a href=\"https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/\">“The Scientific Paper Is Obsolete”</a>, in <em>The Atlantic</em>, and a subsequent piece, <a href=\"https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/\">“Jupyter, Mathematica, and the Future of the Research Paper”</a>, by Paul Romer, former chief economist at the World Bank. Both articles compare and contrast between Wolfram Research’s <a href=\"https://www.wolfram.com/mathematica/\">Mathematica</a> and Project Jupyter. On the surface these two approaches both implement notebooks, with excellent examples coming from both communities. However, Paul Romer nailed the contrast between them with a one-liner: <em>“The tie-breaker is social, not technical. The more I learn about the open source community, the more I trust its members.”</em></p>\r\n",
      "\r\n",
      "<p>Under the surface, the parallels end. Mathematica, which came first, is a popular commercial software product. Jupyter is an open standard for a suite of network protocols that support remote execution environments—plus a spectrum of open source software projects that build extensible environments atop, such as <a href=\"https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906\">JupyterLab</a>, <a href=\"https://jupyterhub.readthedocs.io/en/latest/\">JupyterHub</a>, <a href=\"http://mybinder.readthedocs.io/\">Binder</a>, etc. Organizations leverage Jupyter as a foundation for shared data infrastructure at scale. Organizational challenges emerge along with those implementations at scale: collaboration, discovery, security, compliance, privacy, ethics, provenance, etc. Through this open, community-centered approach, we get open standards, open source implementations, and open discussions about best practices for shared concerns.</p>\r\n",
      "\r\n",
      "<p>For common threads between the two, James Somers’ distillation is subtle: “Software is a dynamic medium; paper isn’t.” It’s been 27 years since the public debut of the World Wide Web, though we’re still barely scratching the surface of what that invention made possible. Frankly, an overwhelming amount of “digital paper” persists on the web. While the promise of WWW implies dynamic, interactive media shared across global infrastructure, questions linger about how best to make it happen. Some of those questions have also been in the news recently.</p>\r\n",
      "\r\n",
      "<p>Rolling the clock back a few decades further, one gem on my bookshelf is <a href=\"https://en.wikipedia.org/wiki/Computer_Lib/Dream_Machines\"><em>Computer Lib/Dream Machines</em></a>, by Ted Nelson, first published in 1974. Nelson explored hypertext, which he’d been working to implement since 1963—though, arguably, that notion traces back to Vannevar Bush and Jorge Luis Borges in the 1940s. To capture the essence of hypertext, <em>Computer Lib</em> also introduced the concept of \"intertwingularity\": complex interrelations within human knowledge. Nelson’s vision had documents representing the world’s knowledge, documents which could interact and intermingle. Borges prefigured a poetic glimpse of this in his 1941 short story, <a href=\"http://www.literatura.us/borges/jardin.html\"><em>El jardín de senderos que se bifurcan</em></a>: the legend of Ts’ui Pên constructing an infinite labyrinth, in which all would lose their way, along with a WWII espionage drama unfolding around that legend.</p>\r\n",
      "\r\n",
      "<p>Out of the many neologisms and one-liners that have attempted to describe Jupyter, <em>intertwingularity</em> nails it. One may “perform science” by authoring a research paper in a journal. That’s science with a lowercase “s,” on paper or something approximating it—merey navigating a single corner of Ts’ui Pên’s labyrinth. Ted Nelson’s vision, however, had documents interacting, intermingling. The practice of reproducible science, which is rapidly unfolding around Jupyter, also relies on documents interacting and intermingling. That opens the door to software as a dynamic medium, \"Science\" with an uppercase “S.” Not merely a library of “digital paper,” but an entirely new way of collaborating, extending our understanding. Potentially as a map through the entire labyrinth.</p>\r\n",
      "\r\n",
      "<p>Reproducible science via Jupyter finds immediate applications in many places. Certainly there are the “hard sciences”: at JupyterCon, we’ll have session talks ranging across astrophysics, quantum chemistry, genomics, geospatial analysis, climatology, and scientific computing in general. During the <a href=\"https://atl-jugheads.github.io/jupyter-day-atlanta-ii/\">Jupyter Day Atlanta</a> event, one excellent example was <a href=\"https://twitter.com/scianalytics/status/980167093596323840\">“Classification and Characterization of Metal Powder in Additive Manufacturing using Convolutional Neural Networks,”</a> by Anna Smith from CMU.</p>\r\n",
      "\r\n",
      "<p>Beyond research, reproducible science is vital for any organization that depends on analysis—and that forms Jupyter’s direct link to data science. During the <a href=\"https://conferences.oreilly.com/jupyter/popup-ma/public/content/slides\">Jupyter Pop-up Boston</a> event, Dave Stuart presented <a href=\"https://www.youtube.com/watch?v=glrBe2zwihc&amp;list=PL055Epbe6d5Zc0ZUWqaeW7haYzuLzmcAq&amp;index=11\">“Citizen Data Science campaign,”</a> about an open source project called <a href=\"https://github.com/nbgallery/\">nbgallery</a>, which thousands of DoD analysts use to discover and share Jupyter notebooks. While some teams have computational needs in common, they may not be allowed to share data. Similar data privacy concerns are encountered in finance, health care, social media, etc. The DoD project provides a fascinating approach to discovery (search, recommendations) for interactive content in highly regulated enterprise environments.</p>\r\n",
      "\r\n",
      "<p>In Atlanta, two industry use cases addressed similar needs: Peter Parente from Valassis Digital with <a href=\"http://bit.ly/give-atl\">“Give a Little Bit of Your Notebooks to Me”</a>—also about sharing and discovering notebook content across an enterprise organization—and John Patanian from General Electric with “Achieving Reproducible and Deployable Data Science Workflows,” about using <a href=\"https://github.com/patanijo/iris_ml_model\">templates</a> for reproducible workflows.</p>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-6rMSW\">\r\n",
      "<p><em>Dave Stuart and Peter Parente will both be presenting at the</em> <a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\"><em>Jupyter Pop-up D.C.</em></a><em> event.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Similar efforts are changing the classroom. In Boston, we had Allen Downey, Taylor Martin, and Doug Blank join the <a href=\"https://www.youtube.com/watch?v=rHHZFy2wIfA&amp;list=PL055Epbe6d5Zc0ZUWqaeW7haYzuLzmcAq&amp;index=7\">“Jupyter in Education” panel</a>. In particular, reproducible science via Jupyter notebooks helps instructors manage the <a href=\"https://en.wikipedia.org/wiki/Instructional_scaffolding\">scaffolding</a> needed to make course materials more engaging, more immediately hands-on, to give learners confidence and direct experience. Ryan Cooper from UConn presented <a href=\"https://www.youtube.com/watch?v=SZPTpx0VbIs&amp;list=PL055Epbe6d5Zc0ZUWqaeW7haYzuLzmcAq&amp;index=9\">“Flipping the classroom with Jupyter and GitHub”</a> as a case study for this. In Atlanta, Carol Willing guided us through several excellent examples in <a href=\"https://speakerdeck.com/willingc/steam-workshops-with-binder-and-jupyterhub\">“STEAM Workshops with Binder and JupyterHub.”</a></p>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-6xaSq\">\r\n",
      "<p><em><a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\">At the D.C. event</a>, we’ll have Lorena Barba from GWU presenting “Flipped Learning with Jupyter,” and Laura Norén from NYU Center for Data Science presenting “Data Science in U.S. and Canadian Higher Education.”</em></p>\r\n",
      "</aside>\r\n",
      "<p>At a higher level of abstraction, reproducible science has an impact on computer science. In Boston, David Koop and Colin Brown from UMassD presented <a href=\"https://www.youtube.com/watch?v=rJcpzSVWfj0&amp;list=PL055Epbe6d5Zc0ZUWqaeW7haYzuLzmcAq&amp;index=10\">“Supporting Reproducibility in Jupyter through Dataflows.”</a> Also, see a related project called <a href=\"https://multithreaded.stitchfix.com/blog/2017/07/26/nodebook/\">Nodebook</a> at Stitch Fix by Kevin Zielnicki. By default, cells in a Jupyter Notebook run from top to bottom—although, a person needs to “Run All” to be sure that results are correct. The Dataflows and Nodebook projects track inputs and outputs for each cell so that notebooks can be guaranteed to “rerun” successfully. The <a href=\"https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/68448\">UMassD project</a> also allows for rearranging cell order: for example, while you may need a long list of Python imports to initialize a notebook, why not move that cell to the end, so that the initial part of a notebook can jump directly into core code? On the one hand, that supports better scaffolding. On the other hand, these projects represent Jupyter Notebooks as dependency graphs, with pre- and post-conditions for each cell. That’s only a few steps away from Petri nets and other automata used for formal analysis of computer programs, concurrency, business process, reliability engineering, security audits, etc. An imaginable next step could be to leverage machine learning to start generating unit tests—and for <a href=\"https://airbnb.design/sketching-interfaces/\">code gen</a> in general.</p>\r\n",
      "\r\n",
      "<p>Here’s an intertwingled idea that weaves together most of the above. Generations of modern science have brought us to a point where reproducible science becomes a priority. Collaboration at a global scale can’t proceed further without it. Meanwhile, open source software, since roughly 1998, has similarly evolved to support collaboration at a global scale, leading to standard practices such as versioning (e.g., git), testing, documentation, pull requests, etc. Most of those practices support reusability. Adding some DevOps, continuous integration/continuous deployment is the software analogy for reproducible science.</p>\r\n",
      "\r\n",
      "<p>We’re at a point where those two cultures, science and open source, have much to learn from each other. Science must learn to reuse and improve common software tools, while software must embrace reproducible science. Both are necessary for collaboration at scale. The nexus for that intermingling is Jupyter, where (and when) humans move beyond using digital mimics of print media to take better advantage of what software and collaboration promise in the long term.</p>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-5jzSX\">\r\n",
      "<p><em><a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\">At the D.C. event</a>, Tony Fast will present “A Notebook is a Hypothesis: Blending the paradigms of modern science and open source software.”</em></p>\r\n",
      "</aside>\r\n",
      "<p>Join us at <a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\">Jupyter Pop-up D.C.</a> on Tuesday, May 15, 2018, at the GWU Marvin Center, from 9:00 a.m. to 5:00 p.m. We’ll have a mix of talks from government, industry, and education about Jupyter, along with a lot of opportunities for networking. It’s a great preview for what’s to come at <a href=\"https://conferences.oreilly.com/jupyter/jup-ny?intcmp=il-data-confreg-lp-jpny18_new_site_the-intertwingularity-is-near_body_text_cta\">JupyterCon</a>, August 21-24, 2018, in New York City.</p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/the-intertwingularity-is-near-when-humans-transcend-print-media'>The Intertwingularity is near: When humans transcend print media.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/aVwQuI6UP9M\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 23 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/M5YUFmfQwEM/four-short-links-23-april-2018\n",
      "<p><em>Metrics and Incentives, Facebook as Fire Starter, Meeting Mastery, and Weird Chart Types</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.telegraph.co.uk/science/2016/06/03/one-in-three-heart-surgeons-refuse-difficult-operations-to-avoid/\">Heart Surgeons Avoid Difficult Operations to Avoid Poor Performance Rankings</a> -- <i>Just under one-third of the 115 specialists who responded said they had recommended a different treatment path to avoid adding another death to their score. And 84% said they were aware of other surgeons doing the same.</i>  Reminds me of MySociety's hard-learned lessons with their MP scorecard, whereby MPs would ask pointless questions in Parliament just to get their numbers up.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.nytimes.com/2018/04/21/world/asia/facebook-sri-lanka-riots.html\">When Countries are Tinderboxes, and Facebook is a Match</a> (NYT) -- <i>where institutions are weak or undeveloped, Facebook’s newsfeed can inadvertently amplify dangerous tendencies. Designed to maximize user time on site, it promotes whatever wins the most attention. Posts that tap into negative, primal emotions like anger or fear, studies have found, produce the highest engagement, and so proliferate.</i> Plenty of horrifying examples of lynchings and riots triggered by Facebook posts.</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://interconnected.org/home/2018/04/20/reflections-4\">Reflections</a> (Matt Webb) -- <i>Much of any founder's time will be spent meeting advisors and investors. There's a knack to running the room and getting what you want out of it, while maintaining a feeling of collaboration and conversation. Meetings aren't just time you spend in a room together. Meetings are an atomic unit of work. They should have purpose and outcomes, although these don't necessarily need to be stated. There are a lot of small ways to make sure attendees don't drift or feel lost.</i> Really fascinating notes about how he coaches his founders through the incubator program.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://xeno.graphics/\">Xeno.graphics</a> -- <i>weird but (sometimes) useful charts</i>.</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2546?intcmp=il-data-confreg-lp-steu18_new_site_4SL_cta\">Check out the \"Strata Business Summit\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-23-april-2018'>Four short links: 23 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/M5YUFmfQwEM\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Traits you’ll find in good managers\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zAeTD0PQ-TQ/traits-youll-find-in-good-managers\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/14020760665_b1171bd329_o_crop-962fe667bd0e0554df6121f141f0e63d.jpg'/></p><p><em>Work with your manager to get what you need, when you need it.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/traits-youll-find-in-good-managers'>Traits you’ll find in good managers.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/zAeTD0PQ-TQ\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 20 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3CzPcc_aOe4/four-short-links-20-april-2018\n",
      "<p><em>Functional Programming, High-Dimensional Data, Games and Datavis, and Container Management</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"http://www.cs.cmu.edu/~popl-interviews/peytonjones.html\">Interview with Simon Peyton-Jones</a> -- <i>I had always assumed that the more bleeding-edge changes to the type system, things like type-level functions, generalized algebraic data types (GADTs), higher rank polymorphism, and existential data types, would be picked up and used enthusiastically by Ph.D. students in search of a topic, but not really used much in industry. But in fact, it turns out that people in companies are using some of these still-not-terribly-stable extensions. I think it's because people in companies are writing software that they want to still be able to maintain and modify in five years time.</i> SPJ is the creator of Haskell, and one of the leading thinkers in functional programming.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1701.08290\">HyperTools</a> -- <i>A Python toolbox for visualizing and manipulating high-dimensional data</i>. <a href=\"https://github.com/ContextLab/hypertools\">Open source</a>. High-dimensional = \"a lot of columns in each row\".</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/@Elijah_Meeks/what-video-games-have-to-teach-us-about-data-visualization-87c25ff7c62f\">What Videogames Have to Teach Us About Data Visualization</a> -- super-interesting exploration of space, storytelling, structure, and annotations.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/Netflix/titus\">Titus</a> -- Netflix open-sourced their <a href=\"https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436\">container management platform</a>. There aren't many companies with the scale problems of Amazon, Netflix, Google, etc., so it's always interesting to see what comes out of them.</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      " <p><em><a href=\"https://conferences.oreilly.com/oscon/oscon-or/public/schedule/topic/2590?intcmp=il-prog-confreg-lp-osor18_20180221_new_site_4sl_cta\">Check out the Evolutionary Architecture sessions at OSCON</a>, July 16-19, 2018, in Portland. Hurry—best price ends April 20.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-20-april-2018'>Four short links: 20 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/3CzPcc_aOe4\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Thinking beyond bots: How AI can drive social impact\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BXie2iEFXRE/thinking-beyond-bots-how-ai-can-drive-social-impact\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/light-3087405_1920_crop-d7858bb6e841e051bcf496b9131b7a0d.jpg'/></p><p><em>A few ways to think differently and integrate innovation and AI into your company's altruistic pursuits.</em></p><p>What do artificial intelligence (AI), invention, and social good have in common? While on the surface they serve very different purposes, at their core, they all require you to do one thing in order to be successful at them: think differently.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Take the act of inventing—in order to develop a great patent, trade secret, or other intellectual property, you need to think outside of the box. Similarly, at the heart of AI is the act of unlocking new capabilities, whether that’s making virtual personal assistants like Alexa more useful, or creating a chatbot that provides a personalized experience to customers. And because of the constantly changing economic and social landscapes, coming up with impactful social good initiatives requires you to constantly approach things through a new lens.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Individually, these fields have seen notable advancements over the past year, including new technologies that are bringing improvements to AI and large companies that are prioritizing giving back. But even more exciting is that we’re seeing more and more business leaders and nonprofits combining AI, innovation, and social good to reach communities in innovative ways, at a scale we’ve never before seen.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There’s no better time than now to explore how your organization approaches your social good efforts. Here are a few ways you can think differently and integrate innovation and AI into your company’s altruistic pursuits.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Approach social good through the mind of an inventor</h2>\r\n",
      "\r\n",
      "<p>As a master inventor at IBM, I’m part of the team responsible for helping the company become the <a href=\"https://www.bloomberg.com/news/articles/2018-01-09/ibm-breaks-patent-record-in-25th-straight-year-as-number-one\">leading recipient of U.S. patents for the last quarter century</a>. While developing patents and intellectual properties might not be what you’re setting out to do as part of your humanitarian efforts, the way we approach our jobs as inventors is something that can be applied across all aspects of giving back. Consider the United Nations’ <a href=\"https://sustainabledevelopment.un.org/?menu=1300\">17 Sustainable Development Goals</a>, which aim to eradicate things like poverty, hunger, disease, and more. These are game-changing initiatives that definitely require new ideas. What’s more, the United Nations estimates that we’re $5 trillion short on resources needed to accomplish these goals. How do we bridge this gap? Well, we need to start thinking differently.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Foundationally, coming up with a great invention is identifying a problem that needs to be solved and coming up with an out-of-the-box idea that’s smart, has the biggest impact, and the lowest risk. To do this, we look around us to see which relevant technologies we can use that are already at our disposal so we don’t have to completely reinvent the wheel if we don’t have to. We also identify which parts of the solution need a completely new idea to be created from scratch. Additionally, we look at the issue we’re trying to solve and the current landscape as a whole so we can predict any issues or future problems that may arise, and we try to address them ahead of time in our invention.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The same approach should be applied to social good—identify the problem you want to solve, the tools that already exist that can help you solve this dilemma, and the resources that need to be created or brought in from outside properties in order to execute your plan. At the heart of social good, similar to most inventions, are the people you’re trying to help. You need to make sure you’re maximizing the reach of your project while also minimizing any risks that may unintentionally create additional problems for the people you’re trying to help. To do this, you need to be creative in your approach.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>As an example, this is exactly the approach <a href=\"http://www.investedapp.com/\">InvestEd</a> is taking (full disclosure: I am an advisor for InvestEd). They started off by realizing they could commercialize and create social good at the same time by enabling financial education and facilitating microloans for small businesses in emerging markets. Helping these small businesses grow added more value to the small, local communities. And to make their product even better, InvestEd is adding AI capabilities to widen their offerings and provide a more innovative user experience.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>AI: Unlocking new capabilities</h2>\r\n",
      "\r\n",
      "<p>To grab the value and create disruptive AI technology for social good ideas, we have to think beyond the typical automation activities of a machine. Take <a href=\"http://ecc.ibm.com/case-study/us-%20en/ECCF-EDC03025USEN\">Guiding Eyes</a>, for example, which is using AI to discover the secrets behind successful guide dogs. By taking advantage of natural language processing (NLP) on structured and unstructured data, the system they’re using is trained to find correlations to successful dogs among genetic, health, temperament, and environmental factors—and the technology continues to learn and get better. By using AI, Guiding Eyes has seen a 10% increase in guide dog graduation rates, helping the organization meet the growing demand for guide dogs.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There are many other examples of AI being used for the betterment of society. For example, <a href=\"http://teamcore.usc.edu/people/Paws/index.html\">PAWS</a> is an organization that uses machine learning to predict where poachers may strike, or Dr. Eric Elster, who worked with the Walter Reed National Military Medical Center to apply <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4588374/\">machine learning techniques to improve the treatment of U.S. service members injured in combat</a>.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Best practices for getting started</h2>\r\n",
      "\r\n",
      "<p>These are a just a few ideas for how AI can be used for social good— there are still plenty of opportunities out there. The challenge is how to get started, so here are three best practices I’d like to share to help people who want to embark on this journey.</p>\r\n",
      "\r\n",
      "<ol>\r\n",
      "\t<li>First, build your understanding of what AI is and is not through great online learning, such as <a href=\"https://www.udacity.com/course/intro-to-artificial-intelligence--cs271\">Intro to Artificial Intelligence</a>, led by Peter Norvig and Sebastian Thrun, on Udacity.</li>\r\n",
      "\t<li>Second, think differently. AI is a different computing model. Instead of thinking about use cases and scenarios, really focus on the problem you want to solve. Think more “ideal scenario” on how to best develop a solution, and then see if a machine can be trained to do this work. Let’s consider personalized education, particularly reading comprehension (which has shown to have a tremendous impact on a child’s long-term educational performance across all subjects). With a traditional use case approach, we would probably try to develop a general framework that would help in a handful of scenarios. Now, <a href=\"http://www.united2read.org/learningovations/\">Learning Ovations</a> has thought about the more ideal scenario. They have realized there are too many possible scenarios to program or for a general framework to even cover. Instead, they’re training AI to assess each child’s performance (across traditional metrics and some new ones) as a tool for educators and parents. In addition, they’re creating an AI-powered recommendation engine based on each individual school’s curriculum to provide another tool for educators to create a customized reading program for each student. Thus, Learning Ovations thought differently on how to personalized education.</li>\r\n",
      "\t<li>Third, set aside preconceived notions. There are things that people are better than machines at doing, but there are things machines are better than people at doing—some of which may be surprising. For example, people seem to be more honest in sharing health or financial information with a machine than a person because they don’t worry about being judged. This typically means the machine gets more accurate data to provide recommendations. Thus, recognizing that a machine might be as capable in some areas could unlock whole new capabilities.</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>When it comes to AI, invention, and social good, the possibilities are endless. Technology will only continue to become more advanced, creating new opportunities to fix societal problems related to health, sustainability, conservation, accessibility, and much more. If you’re thinking of jumping into AI for good, just remember the most important rule: think differently.</p>\r\n",
      "\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/thinking-beyond-bots-how-ai-can-drive-social-impact'>Thinking beyond bots: How AI can drive social impact.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/BXie2iEFXRE\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "5 best practices for delivering design critiques\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jbPvrGd1a_M/5-best-practices-for-delivering-design-critiques\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/movie-297278_crop-7db0d00e5237d0cecc459b0898d552e2.jpg'/></p><p><em>Real critique helps teams strengthen their designs, products, and services.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/5-best-practices-for-delivering-design-critiques'>5 best practices for delivering design critiques.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/jbPvrGd1a_M\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "How to run a custom version of Spark on hosted Kubernetes\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/86Q45ikq5BE/how-to-run-a-custom-version-of-spark-on-hosted-kubernetes\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/paul-carmona-382565-unsplash_crop-cdea047960b21ae10b9f08f40b4778e4.jpg'/></p><p><em>Learn how Spark 2.3.0+ integrates with K8s clusters on Google Cloud and Azure.</em></p><p>Do you want to try out a new version of Apache Spark without waiting around for the entire release process? Does running alpha-quality software sound like fun? Does setting up a test cluster sound like work? This is the blog post for you, my friend! We will help you deploy code that hasn't even been reviewed yet (if that is the adventure you seek). If you’re a little cautious, reading this might sound like a bad idea, and often it is, but it can be a great way to ensure that a PR really fixes your bug, or the new proposed Spark release doesn’t break anything you depend on (and if it does, you can raise the alarm). This post will help you try out new (2.3.0+) and custom versions of Spark on Google/Azure with Kubernetes. Just don't run this in production without a backup and a very fancy support contract for when things go sideways.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Note: This is a cross-vendor post (<a href=\"https://docs.microsoft.com/en-us/azure/aks/spark-job\">Azure's Spark on AKS</a> and <a href=\"https://cloud.google.com/blog/big-data/2018/03/testing-future-apache-spark-releases-and-changes-on-google-kubernetes-engine-gke-and-cloud-dataproc\">Google Cloud's Custom Spark on GKE</a>), each of which have their own vendor-specific posts if that’s more your thing.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Warning: it’s important to make sure your tests don’t destroy your real data, so consider using a sub-account with lesser permissions.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Setting up your version of Spark to run</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If there is an off-the-shelf version of Spark you want to run, you can <a href=\"http://spark.apache.org/downloads.html\">go ahead and download it</a>. If you want to try out a specific patch, you can checkout the pull request to your local machine with <code>git fetch origin pull/ID/head:BRANCHNAME</code>, where ID is the PR number, and <a href=\"http://spark.apache.org/docs/latest/building-spark.html\">then follow the directions to build Spark</a> (remember to include the -P components you want/need, including your cluster manager of choice).</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Now that we’ve got Spark built, we will build a container image and upload it to the registry of your choice, like shipping a PXE boot image in the early 90s (bear with me, I miss the 90s).</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Depending on which registry you want to use, you’ll need to point both the build tool and spark-submit in the correct location. We can do this with an environment variable—for Docker Hub, this is the name of the registry; for Azure Container Registry (ACR), this value is the ACR login server name; and for Google Container Registry, this is gcr.io/$PROJECTNAME.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nb\">export </code><code class=\"nv\">REGISTRY</code><code class=\"o\">=</code>value</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For Google cloud users who want to use the Google-provided Docker registry, you will need to set up Docker to run through gcloud. In the bash shell, you can do this with an alias:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nb\">shopt</code> -s expand_aliases <code class=\"o\">&amp;&amp;</code> <code class=\"nb\">alias </code><code class=\"nv\">docker</code><code class=\"o\">=</code><code class=\"s2\">\"gcloud docker --\"</code></pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For Azure users who want to use Azure Container Registry (ACR), you will need to <a href=\"https://docs.microsoft.com/en-us/azure/container-registry/container-registry-auth-aks\">grant</a> Azure Container Service (AKS) cluster read access to the ACR resource.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For non-Google users, you don’t need to wrap the Docker command, and just skip that step and keep going:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">DOCKER_REPO</code><code class=\"o\">=</code><code class=\"nv\">$REGISTRY</code>/spark\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">SPARK_VERSION</code><code class=\"o\">=</code><code class=\"sb\">`</code>git rev-parse HEAD<code class=\"sb\">`</code>\r\n",
      "./bin/docker-image-tool.sh -r <code class=\"nv\">$DOCKER_REPO</code> -t <code class=\"nv\">$SPARK_VERSION</code> build\r\n",
      "./bin/docker-image-tool.sh -r <code class=\"nv\">$DOCKER_REPO</code> -t <code class=\"nv\">$SPARK_VERSION</code> push\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "<h2>Building your Spark project for deployment (or, optionally, starting a new one)</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Spark on K8s does not automatically handle pushing JARs to a distributed file system, so we will need to upload whatever JARs our project requires to work. One of the easiest ways to do this is to turn our Spark project into an assembly JAR.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If you’re starting a new project and you have <a href=\"https://www.scala-sbt.org/\">sbt</a> installed, you can use <a href=\"https://github.com/holdenk/sparkProjectTemplate.g8\">the Spark template project</a>:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">sbt new holdenk/sparkProjectTemplate.g8</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If you have an existing SBT-based project, you can add the sbt-assembly plugin:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "touch project/assembly.sbt\r\n",
      "<code class=\"nb\">echo</code> <code class=\"s1\">'addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")'</code> &gt;&gt; project/assembly.sbt\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With SBT, once you have the SBT assembly plugin (either through creating a project with it included in the template or adding it to an existing one), you can produce an assembly JAR by running:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">sbt assembly</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The resulting JAR not only will have your source code, but all of the requirements as well. Note that this JAR may have multiple entry points, so later on, we’re going to need to tell Spark submit about the entry point we want it to use. For the world standard wordcount example, we might use:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nb\">export </code><code class=\"nv\">CLASS_NAME</code><code class=\"o\">=</code>org.apache.spark.examples.JavaWordCount</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If you have a maven or other project, there are a few different options for building assembly JARs. Sometimes, these may be referred to as “fat jars” in the documentation.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If starting a new project sounds like too much work and you really just want to double check that your Spark on K8s deployment works, you can use the example JAR that Spark ships with (e.g., examples/target/spark-examples).</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Uploading your JARs</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>One of the differences between Spark on K8s and Spark in the other cluster managers is that there is no automatic tool to distribute our JARs (or other job dependencies). To make sure your containers have access to your JAR, the fastest option is normally to upload it.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Regardless of platform, we need to specify which JAR, container / bucket, and the target:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">FOLDER_NAME</code><code class=\"o\">=</code>mybucket\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">SRCJAR</code><code class=\"o\">=</code>target/scala-2.11/...\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">MYJAR</code><code class=\"o\">=</code>myjar\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With Azure:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "<code class=\"nv\">RESOURCE_GROUP</code><code class=\"o\">=</code>sparkdemo\r\n",
      "<code class=\"nv\">STORAGE_ACCT</code><code class=\"o\">=</code>sparkdemo<code class=\"nv\">$RANDOM</code>\r\n",
      "az group create --name <code class=\"nv\">$RESOURCE_GROUP</code> --location eastus\r\n",
      "az storage account create --resource-group <code class=\"nv\">$RESOURCE_GROUP</code> --name <code class=\"nv\">$STORAGE_ACCT</code> --sku Standard_LRS\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">AZURE_STORAGE_CONNECTION_STRING</code><code class=\"o\">=</code><code class=\"sb\">`</code>az storage account show-connection-string --resource-group <code class=\"nv\">$RESOURCE_GROUP</code> --name <code class=\"nv\">$STORAGE_ACCT</code> -o tsv<code class=\"sb\">`</code>\r\n",
      "az storage container create --name <code class=\"nv\">$FOLDER_NAME</code>\r\n",
      "az storage container <code class=\"nb\">set</code>-permission --name <code class=\"nv\">$FOLDER_NAME</code> --public-access blob\r\n",
      "az storage blob upload --container-name <code class=\"nv\">$FOLDER_NAME</code> --file <code class=\"nv\">$SRCJAR</code> --name <code class=\"nv\">$MYJAR</code>\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With Google Cloud:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">gsutil cp <code class=\"nv\">$SRCJAR</code> gs://<code class=\"nv\">$JARBUCKETNAME</code>/<code class=\"nv\">$MYJAR</code></pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For now though, we don’t have the JARs installed to access the GCS or Azure blob storage, and Spark on K8s doesn’t currently support spark-packages, which we could use to access those, so we need to make our JAR accessible over http.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With Azure:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nv\">JAR_URL</code><code class=\"o\">=</code><code class=\"k\">$(</code>az storage blob url --container-name <code class=\"nv\">$FOLDER_NAME</code> --name <code class=\"nv\">$MYJAR</code> <code class=\"p\">|</code> tr -d <code class=\"s1\">'\"'</code><code class=\"k\">)</code></pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With Google Cloud:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nb\">export </code><code class=\"nv\">PROJECTNAME</code><code class=\"o\">=</code>boos-demo-projects-are-rad\r\n",
      "gcloud iam service-accounts create signer --display-name <code class=\"s2\">\"signer\"</code>\r\n",
      "gcloud projects add-iam-policy-binding <code class=\"nv\">$PROJECTNAME</code> --member serviceAccount:signer@<code class=\"nv\">$PROJECTNAME</code>.iam.gserviceaccount.com --role roles/storage.objectViewer\r\n",
      "gcloud iam service-accounts keys create     ~/key.json     --iam-account signer@<code class=\"nv\">$PROJECTNAME</code>.iam.gserviceaccount.com\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">JAR_URL</code><code class=\"o\">=</code><code class=\"sb\">`</code>gsutil signurl -m GET ~/key.json gs://<code class=\"nv\">$JARBUCKETNAME</code>/<code class=\"nv\">$MYJAR</code> <code class=\"p\">|</code> cut  -f <code class=\"m\">4</code> <code class=\"p\">|</code> tail -n 1<code class=\"sb\">`</code>\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Starting your cluster</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Now you are ready to kick off your super-fancy K8s Spark cluster.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For Azure:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "az group create --name mySparkCluster --location eastus\r\n",
      "az aks create --resource-group mySparkCluster --name mySparkCluster --node-vm-size Standard_D3_v2\r\n",
      "az aks get-credentials --resource-group mySparkCluster --name mySparkCluster\r\n",
      "kubectl proxy <code class=\"p\">&amp;</code>\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "<p>For Google cloud:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "gcloud container clusters create  mySparkCluster --zone us-east1-b --project <code class=\"nv\">$PROJECTNAME</code>\r\n",
      "gcloud container clusters get-credentials mySparkCluster --zone us-east1-b --project <code class=\"nv\">$PROJECTNAME</code>\r\n",
      "kubectl proxy <code class=\"p\">&amp;</code>\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "<p>On Google Cloud, before we kick off our Spark job, we need to make a service account for Spark that will have permission to edit the cluster:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "kubectl create serviceaccount spark\r\n",
      "kubectl create clusterrolebinding spark-role --clusterrole<code class=\"o\">=</code>edit --serviceaccount<code class=\"o\">=</code>default:spark --namespace<code class=\"o\">=</code>default\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "<h2>Running your Spark job</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>And now we can finally run our Spark job:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "./bin/spark-submit --master k8s://http://127.0.0.1:8001  <code class=\"se\">\\</code>\r\n",
      "  --deploy-mode cluster --conf <code class=\"se\">\\</code>\r\n",
      " spark.kubernetes.container.image<code class=\"o\">=</code><code class=\"nv\">$DOCKER_REPO</code>/spark:<code class=\"nv\">$SPARK_VERSION</code> <code class=\"se\">\\</code>\r\n",
      "--conf spark.executor.instances<code class=\"o\">=</code><code class=\"m\">1</code> <code class=\"se\">\\</code>\r\n",
      "--class <code class=\"nv\">$CLASS_NAME</code> <code class=\"se\">\\</code>\r\n",
      "--conf spark.kubernetes.authenticate.driver.serviceAccountName<code class=\"o\">=</code>spark <code class=\"se\">\\</code>\r\n",
      "--name wordcount <code class=\"se\">\\</code>\r\n",
      "<code class=\"nv\">$JAR_URL</code> <code class=\"se\">\\</code>\r\n",
      "inputpath\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>And we can verify the output with:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">kubectl logs <code class=\"o\">[</code>podname-from-spark-submit<code class=\"o\">]</code></pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Handling dependencies in Spark K8s (and accessing your data/code without making it public):</h2>\r\n",
      "\r\n",
      "<p>What if we want to directly read our JARs from the storage engine without using https? Or if we have dependencies that we don’t want to package in our assembly JARs? In that case, can the necessary dependencies to our docker file as follows:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "mkdir /tmp/build <code class=\"o\">&amp;&amp;</code> <code class=\"nb\">echo</code> “FROM <code class=\"nv\">$DOCKER_REPO</code>/spark:<code class=\"nv\">$SPARK_VERSION</code>\r\n",
      "\r\n",
      "<code class=\"c\"># Manually update Guava deleting the old JAR to ensure we don’t have class path conflicts</code>\r\n",
      "RUN rm <code class=\"se\">\\$</code>SPARK_HOME/jars/guava-14.0.1.jar\r\n",
      "ADD http://central.maven.org/maven2/com/google/guava/guava/23.0/guava-23.0.jar <code class=\"se\">\\$</code>SPARK_HOME/jars\r\n",
      "<code class=\"c\"># Add the GCS connectors</code>\r\n",
      "ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar <code class=\"se\">\\$</code>SPARK_HOME/jars\r\n",
      "<code class=\"c\"># Add the Azure Hadoop/Storage JARs</code>\r\n",
      "ADD http://central.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.0/hadoop-azure-2.7.0.jar\r\n",
      "ADD http://central.maven.org/maven2/com/microsoft/azure/azure-storage/7.0.0/azure-storage-7.0.0.jar\r\n",
      "\r\n",
      "ENTRYPOINT <code class=\"o\">[</code> <code class=\"s1\">'/opt/entrypoint.sh'</code> <code class=\"o\">]</code>” &gt; /tmp/build/dockerfile\r\n",
      "docker build -t <code class=\"nv\">$DOCKER_REPO</code>/spark:<code class=\"nv\">$SPARK_VERSION</code>-with-deps -f /tmp/build/dockerfile /tmp/build\r\n",
      "\r\n",
      "Push to our registry:\r\n",
      "\r\n",
      "docker push <code class=\"nv\">$DOCKER_REPO</code>/spark:<code class=\"nv\">$SPARK_VERSION</code>-with-deps\r\n",
      "</pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For Azure folks wanting to launch using Azure Storage rather than https:</p>\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\">\r\n",
      "<code class=\"nb\">export </code><code class=\"nv\">JAR_URL</code><code class=\"o\">=</code>wasbs://<code class=\"nv\">$FOLDER_NAME</code>@<code class=\"nv\">$STORAGE_ACCT</code>.blob.core.windows.net/<code class=\"nv\">$MYJAR</code>\r\n",
      "</pre>\r\n",
      "\r\n",
      "<p>For Google folks wanting to launch using GCS rather than https:</p>\r\n",
      "\r\n",
      "<pre data-type=\"programlisting\" data-code-language=\"bash\" data-highlighted=\"true\"><code class=\"nb\">export </code><code class=\"nv\">JAR_URL</code><code class=\"o\">=</code>gs://<code class=\"nv\">$JARBUCKETNAME</code>/<code class=\"nv\">$MYJAR</code></pre>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>And then run the same spark-submit as shown previously.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Wrapping up</h2>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Notably, each vendor has a more detailed guide to running Spark jobs on hosted K8s focused on their own platforms (e.g., <a href=\"https://docs.microsoft.com/en-us/azure/aks/spark-job\">Azure’s guide</a>, <a href=\"https://cloud.google.com/blog/big-data/2018/03/testing-future-apache-spark-releases-and-changes-on-google-kubernetes-engine-gke-and-cloud-dataproc\">Google’s guide</a>, etc.), but hopefully this cross-vendor version shows you the relative portability between the different hosted K8s engines and our respective APIs with Spark. If you’re interested in helping join in Spark code reviews, you can see <a href=\"https://spark.apache.org/contributing.html\">the contributing guide</a> and also watch Karau’s <a href=\"https://www.youtube.com/watch?v=_SdNu7MezL4&amp;list=PLRLebp9QyZtYF46jlSnIu2x1NDBkKa2uw\">past streamed code reviews</a> on YouTube (and subscribe to her <a href=\"https://www.youtube.com/user/holdenkarau\">YouTube</a> or <a href=\"https://www.twitch.tv/holdenkarau\">Twitch</a> channels for new livestreams). You can also follow the authors on their respective Twitter accounts: <a href=\"https://twitter.com/lenadroid\">Alena Hall</a> and <a href=\"http://www.twitter.com/holdenkarau\">Holden Karau</a>.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/how-to-run-a-custom-version-of-spark-on-hosted-kubernetes'>How to run a custom version of Spark on hosted Kubernetes.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/86Q45ikq5BE\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 19 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ye_WK_vX2oY/four-short-links-19-april-2018\n",
      "<p><em>Free Multics, Community Relevance, Speech Synthesis, and Dandelion Data</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://ban.ai/multics/\">BAN.AI Multics</a> -- free multiuser Multics (predecessor to Unix) emulation. <a href=\"http://swenson.org/multics_wiki/index.php?title=Using_Multics\">This Multics guide</a> will be useful.</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://www.artofrelevance.org/\">The Art of Relevance</a> -- <i>explores how mission-driven organizations can matter more to more people. The book is packed with inspiring examples, rags-to-relevance case studies, research-based frameworks, and practical advice on how your work can be more vital to your community.</i> Should be read by startups (relevant to your customers?) and anyone who is trying to build a community around their software. Text available for free online, <a href=\"http://www.artofrelevance.org/buy/\">print versions</a> still available for purchase.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://ytaigman.github.io/loop/\">VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop</a> -- <i>We present a new neural text to speech (TTS) method that is able to transform text to speech in voices that are sampled in the wild. Unlike other systems, our solution is able to deal with unconstrained voice samples and without requiring aligned phonemes or linguistic features.</i> The Presidential voices are impressive. Code and paper available.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://freedom-to-tinker.com/2018/04/18/no-boundaries-for-facebook-data-third-party-trackers-abuse-facebook-login/\">No Boundaries for Facebook Data</a> -- <i>Today we report yet another type of surreptitious data collection by third-party scripts that we discovered: the exfiltration of personal identifiers from websites through “login with Facebook” and other such social login APIs. Specifically, we found two types of vulnerabilities: seven third parties abuse websites’ access to Facebook user data; one third party uses its own Facebook “application” to track users around the web.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2541?intcmp=il-data-confreg-lp-steu18_new_site_4SL_cta\">Check out the \"Emerging technologies and case studies\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-19-april-2018'>Four short links: 19 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/ye_WK_vX2oY\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 18 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/6Zwqwy8dli4/four-short-links-18-april-2018\n",
      "<p><em>Open Source Slack-alike, Open Source MailChimp-alike, DeepFake PSA, and Secure Devices</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.github.com/zulip/zulip/\">Zulip</a> -- FOSS Slack-type chat.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/Mailtrain-org/mailtrain\">Mailtrain</a> -- self-hosted GPLv3 MailChimp-style newsletter service that you can hook up to your favorite mail service (e.g., <a href=\"https://www.mailgun.com/\">Mailgun</a>).</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.buzzfeed.com/davidmack/obama-fake-news-jordan-peele-psa-video-buzzfeed\">Fake News PSA</a> -- DeepFake video of Barack Obama saying things that Obama never said (made for Buzzfeed).</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/SevenPropertiesofHighlySecureDevices.pdf\">Seven Properties of Highly Secure Devices</a> (Microsoft) -- <i>Hardware-based root of trust; small trusted computing base; defense in depth; compartmentalization; certificate-based authentication; renewable security; failure reporting.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/topic/2525?intcmp=il-data-confreg-lp-ainy18_new_site_4SL_cta\">Check out the \"Impact of AI on Business and Society\" sessions</a> at the AI Conference in New York, April 29-May 2, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-18-april-2018'>Four short links: 18 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/6Zwqwy8dli4\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "From USENET to Facebook: The second time as farce\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/AwGHYMGH7dg/from-usenet-to-facebook-the-second-time-as-farce\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/honore_daumier_-_lower_the_curtain_the_farce_is_ended_baissez_le_rideau_la_farce_est_jouee_-_plate_421_-_google_art_project_crop-a7c87ecb68c445ac2b0e8ea9a0ced841.jpg'/></p><p><em>Demanding and building a social network that serves us and enables free speech, rather than serving a business metric that amplifies noise, is the way to end the farce.</em></p><p>Re-interpreting Hegel, Marx <a href=\"https://www.gutenberg.org/files/1346/1346-h/1346-h.htm\">said</a> that everything in history happens twice, the first time as tragedy, the second as farce. That’s a fitting summary of Facebook’s Very Bad Month. There’s nothing here we haven’t seen before, nothing about abuse, trolling, racism, spam, porn, and even bots that hasn’t already happened. This time as farce? Certainly <a href=\"https://www.wired.com/story/why-zuckerberg-15-year-apology-tour-hasnt-fixed-facebook/\">Zuckerberg’s 14-year Apology Tour</a>, as Zeynep Tufecki calls it, has the look and feel of a farce. He just can’t stop apologizing for Facebook’s messes.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Except that the farce isn’t over yet. We’re in the middle of it. As Tufekci points out, 2018 isn’t the first time Zuckerberg has said “we blew it, we’ll do better.” Apology has been a roughly biennial occurrence since Facebook’s earliest days. So, the question we face is simple: how do we bring this sad history to an endpoint that isn’t farce? The third time around, should there be one, it isn’t even farce; it’s just stupidity. We don’t have to accept future apologies, whether they come from Zuck or some other network magnate, as inevitable.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>I want to think about what we can learn from the forerunners of modern social networks—specifically about <a href=\"https://en.wikipedia.org/wiki/Usenet\">USENET</a>, the proto-internet of the 1980s and 90s. (The same observations probably apply to <a href=\"https://en.wikipedia.org/wiki/Bulletin_board_system\">BBSs</a>, though I’m less familiar with them.) USENET was a decentralized and unmanaged system that allowed Unix users to exchange “posts” by sending them to hundreds of newsgroups. It started in the early 80s, peaked sometime around 1995, and arguably ended as tragedy (though it went out with a whimper, not a bang).</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>As a no-holds-barred Wild West sort of social network, USENET was filled with everything we rightly complain about today. It was easy to troll and be abusive; all too many participants did it for fun. Most groups were eventually flooded by spam, long before spam became a problem for email. Much of that spam distributed pornography or pirated software (“warez”). You could certainly find newsgroups in which to express your inner neo-Nazi or white supremacist self. Fake news? We had that; we had malicious answers to technical questions that would get new users to trash their systems. And yes, there were bots; that technology isn’t as new as we’d like to think.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>But there was a big divide on USENET between moderated and unmoderated newsgroups. Posts to moderated newsgroups had to be approved by a human moderator before they were pushed to the rest of the network. Moderated groups were much less prone to abuse. They weren’t immune, certainly, but moderated groups remained virtual places where discussion was mostly civilized, and where you could get questions answered. Unmoderated newsgroups were always spam-filled and frequently abusive, and the alt.* newsgroups, which could be created by anyone, for any reason, matched anything we have now for bad behavior.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>So, the first thing we should learn from USENET is the importance of moderation. Fully human moderation at Facebook scale is impossible. With seven billion pieces of content shared per day, even a million moderators would have to scan seven thousand posts each: roughly 4 seconds per post. But we don’t need to rely on human moderation. After USENET’s decline, <a href=\"https://www.microsoft.com/en-us/research/publication/observed-behavior-and-perceived-value-of-authors-in-usenet-newsgroups-bridging-the-gap/\">research</a> <a href=\"https://books.google.com/books?id=2IltDgAAQBAJ&amp;pg=PA21&amp;lpg=PA21&amp;dq=usenet+trolls+analysis&amp;source=bl&amp;ots=JEiHPslBRm&amp;sig=gerJGs1tO0WvIt9zsOgeSPSH6Ys&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiryMmo_bzaAhUlWN8KHRZkArsQ6AEIUjAE#v=onepage&amp;q=usenet%20trolls%20analysis&amp;f=false\">showed</a> that it was possible to classify users as newbies, helpers, leaders, trolls, or flamers, purely by their communications patterns—with only minimal help from the content. This could be the basis for automated moderation assistants that kick suspicious posts over to human moderators, who would then have the final word. Whether automated or human, moderators prevent many of the bad posts from being made in the first place. It’s no fun being a troll if you can’t get through to your victims.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Automated moderation can also do fact checking. The technology that won Jeopardy a decade ago is more than capable of checking basic facts. It might not be capable of checking complex logic, but most “fake news” centers around facts that can easily be evaluated. And automated systems are very capable of detecting bots: Google’s Gmail has successfully throttled spam.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>What else can we learn from USENET? Trolls were everywhere, but the really obnoxious stuff stayed where it was supposed to be. I’m not naive enough to think that neo-Nazis and white supremacists will dry up and go away, on Facebook or elsewhere. And I’m even content to allow them to have their own Facebook pages: Facebook can let these people talk to each other all they want, because they’re going to do that anyway, whatever tools you put in place. The problem we have now is that Facebook’s engagement metric paves the road to their door. Once you give someone a hit of something titillating, they’ll come back for more. And the next hit has to be stronger. That’s how you keep people engaged, and that’s (<a href=\"https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html\">as Tufekci has argued about YouTube</a>) how you radicalize them.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>USENET had no engagement metrics, no means of linking users to stronger content. Islands of hatred certainly existed. But in a network that didn’t optimize for engagement, hate groups didn’t spread. Neo-Nazis and their like were certainly there, but you had to search them out, you weren’t pushed to them. The platform didn’t lead you there, trying to maximize your “engagement.” I can’t claim that was some sort of brilliant design on USENET’s part; it just wasn’t something anyone thought about at the time. And as a free service, there was a need to maximize profit. Facebook’s obsession with engagement is ultimately more dangerous than their sloppy handling of personal data. “Engagement” allows—indeed, encourages—hate groups to metastasize.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Engagement metrics harm free speech, another ideal carried to the modern internet from the USENET world. But in an “attention economy,” where the limiting factor is attention, not speech, we have to rethink what those values mean. I’ve said that USENET ended in a “whimper”—but what drained the energy away? The participants who contributed real value just got tired of wading through the spam and fighting off the trolls. They went elsewhere. USENET’s history gives us a warning: good speech was crowded off the stage by bad speech.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Speech that exists to crowd out other speech isn’t the unfettered interchange of ideas. Free speech doesn’t mean the right to a platform. Indeed, the U.S. Constitution already makes that distinction: “freedom of the press” is about platforms, and you don’t get freedom of the press unless you have a press. Again, Zeynep Tufekci has it: in “<a href=\"https://www.wired.com/story/free-speech-issue-tech-turmoil-new-censorship/\">It’s the (Democracy-Poisoning) Golden Age of Free Speech</a>,” she writes “The most effective forms of censorship today involve meddling with trust and attention, not muzzling speech itself.” Censorship isn’t about arresting dissidents; it’s about generating so much noise that voices you don’t like can’t be heard.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>If we’re to put an end to the farce, we need to understand what it means to enable speech, rather than to drown it out. Abandoning “engagement” is part of the solution. We will be better served by a network that, like USENET, doesn’t care how people engage, and that allows them to make their own connections. Automated moderation can be a tool that makes room for speech, particularly if we can take advantage of communication patterns to moderate those whose primary goal is to be the loudest voice.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Marx certainly would have laid blame at the feet of Zuckerberg, for naively and profitably commoditizing the social identities of his users. But blame is not a solution. As convenient a punching bag as Zuckerberg is, we have to recognize that Facebook’s problems extend to the entire social world. That includes Twitter and YouTube, many other social networks past and present, and many networks that are neither online nor social. Expecting Zuck to “<a href=\"https://www.facebook.com/zuck/posts/10104380170714571\">fix Facebook</a>” may be the best way to guarantee that the farce plays on.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>History is only deterministic in hindsight, and it doesn’t have to end in farce (or worse). We all build our social networks, and Mark Zuckerberg isn’t the only player on history’s stage. We need to revisit, reassess, and learn from all of our past social networks. Demanding and building a social network that serves us and enables free speech, rather than serving a business metric that amplifies noise, is the way to end the farce.</p>\n",
      "\n",
      "\n",
      "\n",
      "<p>Is that a revolution? We have nothing to lose but our chains.</p>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/from-usenet-to-facebook-the-second-time-as-farce'>From USENET to Facebook: The second time as farce.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/AwGHYMGH7dg\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 17 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/o0BAi-S-2pw/four-short-links-17-april-2018\n",
      "<p><em>Dubsteganography, Parsing History, Hackin' the Jack In, and Model Bias</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/benjojo/dubstep-data\">Hide Data in Dubstep Drops</a> -- the <a href=\"https://blog.benjojo.co.uk/post/encoding-data-into-dubstep-drops\">blog post</a> shows how to use it. Skrillex meets steganography!</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://jeffreykegler.github.io/personal/timeline_v3\">Parsing Timeline</a> -- wonderfully detailed, yet it reads almost chatty. Interesting and informative.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.esat.kuleuven.be/cosic/publications/article-2803.pdf\">Securing Wireless Neurostimulators</a> -- a hack and discussion of the risk of insecure implantable medical devices that interface with the brain. (via <a href=\"https://blog.acolyer.org/2018/04/17/securing-wireless-neurostimulators/\">Paper a Day</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html\">Text Embedding Models Contain Bias</a> (Google) -- great to see this making its way to research outputs, instead of being the province of damage control and bad PR. The <a href=\"https://research.google.com/semanticexperiences/for-developers.html\">Developers section of the Semantic Experiences microsite</a> talks about \"unwanted associations\": <i>In Semantris, the list of words we're showing are hand curated and reviewed. To the extent possible, we've excluded topics and entities that we think particularly invite unwanted associations, or can easily complement them as inputs. In Talk to Books, while we can't manually vet each sentence of 100,000 volumes, we use a popularity measure which increases the proportion of volumes that are published by professional publishing houses. There are additional measures that could be taken. For example, a toxicity classifier or sensitive topics classifier could determine when the input or the output is something that may be objectionable or party to an unwanted association. We recommend taking bias-impact mitigation steps when crafting end-user applications built with these models.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2544?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Law, ethics, and governance\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-17-april-2018'>Four short links: 17 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/o0BAi-S-2pw\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Relato: Turking the business graph\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/QkT3gevXwkg/relato-turking-the-business-graph\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/graphic-2668161_1920_crop-f21b8ce1cb8c4501e70a21228bfff9c9.jpg'/></p><p><em>A failed analytics startup post-mortem.</em></p><p>In order to conquer a market, you must first understand it. We often speak of markets in the abstract, as addressable segments of the economy, defining them by examples of companies and by comparisons to others engaged in similar activities. Sales and marketing leaders have richer internal models of markets they use to guide their organizations as they fight for their share of the markets they contest. In January 2015, I set out to build an external representation of a market every bit as rich as those in the minds of leading executives driving successful companies; I founded an analytics startup called <a href=\"http://www.relato.io/\">Relato</a>—a startup that, unfortunately, did not succeed. In this post, I’ll present the story of the company and the work I did there, the entrepreneurship and network science involved in my work, and some insight into how <em>not to</em> run a young analytics startup, and a little about how to do so as well.</p>\r\n",
      "\r\n",
      "<p>My mission with Relato was to build a deeper understanding of the modern networked economy, a vast network in which companies are best defined according to their business relationships with other companies. When it comes to understanding companies, it’s “who you know.” These relationships translate to connections in the <em>business graph</em>, made up of connections between customer, partner, competitor, and investor.</p>\r\n",
      "\r\n",
      "<h2>Mission: Mapping markets</h2>\r\n",
      "\r\n",
      "<p>I started Relato with an experiment to see how much market intelligence I could gather from the business web. Having worked at LinkedIn, I missed their social graph. I wondered, “Could a copy of the business graph be collected from the open web?” The answer to that research question is what led me to found Relato.</p>\r\n",
      "\r\n",
      "<p>I started by surveying the state of the market for data on companies. I discovered that while basic firmographic data was available—things like address, industry code, website technologies—there was nothing that captured the actual business activity of companies. By contrast, when I surveyed the websites of businesses, I found a treasure trove of information about their business relationships with other companies. Starting with a market I knew—big data—I manually transcribed the partnership pages of the major players: Hortonworks, Cloudera, MapR, and Pivotal. The combined list came to hundreds of companies—not a bad survey of the big data market.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-6nGie\"><img alt=\"Hortonworks’ partnership page\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure1-f8873f18b0249782f1b6ced0361d70bf.png\"><figcaption><span class=\"label\">Figure 1. </span>Where it all started: Hortonworks’ partnership page. Screenshot by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "\r\n",
      "<p>I saw opportunity! I could transcribe the companies listed on partnership pages to learn how companies actually did business. Then I could use graph analytics on this data to extract next-generation profiles on companies. This data could be used in lead scoring and lead generation systems to provide a breakthrough in their level of performance. In short, I could provide leads for enterprise customers that would convert to sales at a rate never before seen! I got excited. What if sales calls only came to people who wanted your product, because Relato told you so? I could optimize the economy and change the world! I was inspired.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>I figured out roughly how I could collect this data using natural language processing, an area that I know a little bit about but is not one of my core skills. Building this model and making it good enough to be saleable would take at least a year. I did not have a year of cash to burn in the bank. Fortunately, there was a faster alternative.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There was a shift in “big data” to hybrid human/machine processing, where humans located in places where wages are low would perform many instances of simple tasks to cheaply create data sets for machine learning systems. Using this method, there would ultimately be a higher cost per record collected because I would be paying real humans wages, but the up-front cost in development time was much lower. This way, I could get started much faster, shipping a beta product to customers using money from angel investors instead of venture capitalists.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-5dEi7\"><img alt=\"human/machine hybrid data processing\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure2-3170793246b9f1241f5a1265d31591d7.png\"><figcaption><span class=\"label\">Figure 2. </span>A slide from our first deck on human/machine hybrid data processing. Image by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "<p>Most data scientists use application programing interfaces (APIs) like Amazon’s Mechanical Turk to automate delegating data processing tasks to humans. Rather than using an API, I built a data collection web application that was used by people I actually got to know: “mechanical Turks” I hired on a website called UpWork. They would transcribe partnership pages and enter them into a database using an application that did things like autocomplete company names to ensure the data was clean and consistent. If the page had thousands of partnerships, I automated the collection process using utilities I developed to extract the names and domains of companies.</p>\r\n",
      "\r\n",
      "<p>The system worked beautifully, and the workers became good at their jobs. I would spot check their work, offering corrections. Instead, they often corrected me! There is truly a wealth of talent available from underemployed individuals in areas of the world with less opportunity than we enjoy in the Bay Area. They are talented and hardworking, and you can get great work humanely—if you treat them with the respect they deserve—rather than as human computers at the other end of an API.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-67XiM\"><img alt=\"The Relato data collection system\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure3-247cd1e74ddae5fe31fc3ba1bfaa74ee.png\"><figcaption><span class=\"label\">Figure 3. </span>The Relato data collection system. Image by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "<p>Data in hand, I developed algorithms using a graph database to calculate metrics describing the way companies worked together. I started by basing Relato’s lead scoring system on data from several commercial APIs to ensure my model had everything my competitors used. Then I added to the model the features I derived from the partnership graph. It worked! The accuracy of the model increased dramatically. I knew I was on to something when the model indicated that the partnership centrality, a graph metric, was by far the most important feature determining lead scoring accuracy. It was even more important than the 650-area code (Silicon Valley) for a company selling to technology startups! Relato was off to a good start, algorithmically at least. This became a slide in our pitch deck (see Figure 4).</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-6QZiw\"><img alt=\"efficacy of partnership data in lead scoring\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure4-49b4b3d6b90579f0376bf50ad9da558a.png\"><figcaption><span class=\"label\">Figure 4. </span>A slide from Relato’s deck on the efficacy of partnership data in lead scoring. Image by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "<h2>Lead generation: Minting hot leads</h2>\r\n",
      "\r\n",
      "<p>Relato built two things: a lead generation system for B2B sales, and MarketMaps, a business graph visualization tool. A lead scoring system is just a predictive model for inbound business contacts, scoring them by how likely they are to become customers. A lead generation system adds a database already populated with a “universe” full of contacts, plus a lot of plumbing to shuffle the data through the system, through the algorithms and back out to the customer. In goes a current customer list, out go recommended leads.</p>\r\n",
      "\r\n",
      "<p>The idea of algorithmic lead generation is to take a database of business contacts representing the entire world of business and select those few “hot leads” for a specific company that will convert into real customers. In the process, this generates a ton of value. If you can pull this off, it’s a good business model—see Figure 5, which describes the price amplification as contacts travel through the marketing funnel on their way to becoming sales. Contacts sell for a fraction of a dollar. Good leads sell for $50 to as much as $500. That makes a lot of room for profit!</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-30yix\"><img alt=\"Minting leads and dollars using graph analytics\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure6-68c2f8f83d235b7b35a2902512139198.png\"><figcaption><span class=\"label\">Figure 5. </span>Minting leads and dollars using graph analytics. Image by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "\r\n",
      "<p>Figure 6 shows the marketing funnel itself for a business-to-business (B2B) company with a $4 million marketing budget. At the top of the funnel, contacts for a person sell for a fraction of a dollar. The sale of lead lists is still a thriving market, although the accuracy of the contact information varies greatly.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>It takes many leads in the top of the funnel to result in one sale at the bottom. In this case, we’ll need 80,000 responses from contacts to meet our revenue goal. A response would be opening or clicking on a link in an email or coming to the website from a web search.</p>\r\n",
      "\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-RWxiA\"><img alt=\"B2B sales and marketing funnel\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure7-836d66fe5feb27f0795dea74a49116df.png\"><figcaption><span class=\"label\">Figure 6. </span>A B2B sales and marketing funnel for a B2B company with a sales goal of $10 million, a marketing budget of $4 million and a sale price of $100K. Image by Russell Jurney.</figcaption></figure>\r\n",
      "\r\n",
      "\r\n",
      "<p>The next step in the funnel is a marketing qualified lead (MQL). A lead becomes an MQL when an analysis of the lead’s behavior determines the lead is likely enough to buy that they merit attention from a real person. MQLs for enterprise companies go for about $50 a piece. Marketo and other marketing automation systems calculate a lead score based on a lead’s behavior, such as when they interact with your website (10 points!) or download a white paper (50 points!). When the score becomes high enough, contact is made by a lead qualifier. A lead qualifier is a junior salesperson who makes initial sales contacts by phone and email. The problem with marketing automation systems is that the scoring algorithm is usually arbitrary. As a result, there is widespread dissatisfaction in the results generated by the large investments enterprises have made in marketing automation over the last decade. This is what drives the lead scoring market.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Predictive lead scoring systems substitute machine learning for manual behavioral analysis to optimize the lead scoring algorithm. They replace arbitrary lead scores with artificial intelligence. Lead scoring systems then become a black box within the marketing funnel that uses past results (sales) to score future leads. Firmographic data or data characterizing a business is limited, so behavior is the primary training data for these systems, which work better than marketing automation alone. These systems have return on investment (ROI), but so far, this has turned out to be an iterative improvement that has not revolutionized sales and marketing.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The next step in the marketing funnel is a sales qualified lead (SQL). A lead becomes an SQL when a live, in-person salesperson has spoken to the person behind the potential lead, and through conversation has determined they are a good fit for the company’s product. Top salespeople are fed SQLs from junior salespeople and they use their extensive skills to build relationships and close deals. The lead funnel has side branches: a good senior salesperson will also do his or her own lead generation, often bringing a rolodex from job to job, selling different products to the same contacts over and over.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For B2B sales, it is necessary to pour a lot of contacts (200 in our slide in Figure 6) into the sales and marketing funnel to generate a single sale. Most leads fail to mature to the next level at each level of the funnel. Relato’s mission was ambitious: to skip levels in the sales and marketing funnel, in order to <em>mint</em> contacts directly into leads every bit as good as sales qualified leads. Many other lead generation companies have attempted this task, but none that I could find used anything like Relato’s business graph to do so.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>In the next post, we’ll talk more about the products I built at Relato, and where I went wrong in steering the business.</p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/relato-turking-the-business-graph'>Relato: Turking the business graph.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/QkT3gevXwkg\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "The eight rules of good documentation\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/eX305rPUBmg/the-eight-rules-of-good-documentation\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/escribano_crop-9b7ffc10d8e22a013994a61b1ae9ac73.jpg'/></p><p><em>Like good code, good documentation is difficult and time consuming to write.</em></p><p>Imagine for a moment two common scenarios in the life of a web developer.</p>\r\n",
      "\r\n",
      "<p>In the first scenario, meet Harlow. Today is Harlow's first day on a new project. The team has a well-established codebase, a great working environment, and a robust test suite. As Harlow sits down at her desk, she's excited to get up to speed with the team. After the morning stand-up meeting she's pointed to the project's documentation for installation with a slight grimace from her colleague Riley. He mentions that the docs \"might be a little out of date, but should hopefully be enough to get you going.\" Harlow then spends the rest of the day following the documentation until she gets stuck, at which point she is forced to dig through code or ask colleagues for guidance. What might have taken a few minutes becomes a day-long exercise in frustration, tampering Harlow's initial excitement.</p>\r\n",
      "\r\n",
      "<p>In the second scenario, meet Harrison. He's working on a web app and finds a library that, at first glance, seems incredibly useful for his project. As he attempts to integrate it with his codebase he discovers that parts of the API seem to be glossed over in the documentation or even undocumented. In the end, he walks away from the project in favor of another solution.</p>\r\n",
      "\r\n",
      "<p>Though these scenarios may be slightly exaggerated, I'm reasonably certain that many of us can relate. These problems were not primarily caused by low-quality code, but rather by poor documentation.</p>\r\n",
      "\r\n",
      "<p>If useful documentation is so important to the success of projects and developer well-being, why don't all projects have it? The answer, I believe, is that like good code, good documentation is difficult and time consuming to write.</p>\r\n",
      "\r\n",
      "<p>In my eyes, there are eight rules that we can follow to produce good documentation:</p>\r\n",
      "\r\n",
      "<ol>\r\n",
      "\t<li>Write documentation that is inviting and clear</li>\r\n",
      "\t<li>Write documentation that is comprehensive, detailing all aspects of the project</li>\r\n",
      "\t<li>Write documentation that is skimmable</li>\r\n",
      "\t<li>Write documentation that offers examples of how to use the software</li>\r\n",
      "\t<li>Write documentation that has repetition, when useful</li>\r\n",
      "\t<li>Write documentation that is up-to-date</li>\r\n",
      "\t<li>Write documentation that is easy to contribute to</li>\r\n",
      "\t<li>Write documentation that is easy to find</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The most important rule of good documentation is for it to <em>be as inviting as possible</em>. This means that we should aim to write it in the clearest terms possible without skipping over any steps. We should avoid making assumptions about what our users may know. Sometimes this can seem to be overkill, and we may be tempted to say something like \"every X developer knows about Y,\" but we each bring our own background and set of experiences to a project. Though this may result in more verbose documentation, it is ultimately simpler, as there is less guesswork involved for developers with all levels of experience.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Documentation should aim to <em>be comprehensive</em>. This means that all aspects of the project are documented. Undocumented features or exceptions can lead to frustration and become a time suck as users and other developers are forced to read through code to find the answers they need. Fully documenting all features takes away this kind of ambiguity.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>When we write documentation that is <em>skimmable</em>, we help users find the content they need quickly. Making documentation skimmable can be accomplished by using clear headings, bulleted lists, and links. For large project documentation, a table of contents or clear navigation will help users to skip straight to what they need, rather than scrolling through a single long document.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Documentation that features <em>examples</em> allows users to see how they might use the code themselves. Aim to provide examples of the most common use cases for the project, while letting the comprehensive documentation detail every possibility.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>It is perfectly acceptable to <em>include some repetition</em> in  documentation, which the <a href=\"http://www.writethedocs.org/guide/writing/docs-principles/#arid\">Write the Docs project</a> terms \"ARID\" (accepts (some) repetition in documentation). Doing so acknowledges that users may not read the full docs or that some information is relevant in multiple places in the documentation. While good code may be DRY, good writing aims to be clear, and sometimes this means repeating ourselves. The Write the Docs project calls out the difference between writing that is ARID, DRY, and WET in this way:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<blockquote><p>The pursuit of minimizing repetition remains valiant! ARID does not mean <a href=\"https://en.wikipedia.org/wiki/Don't_repeat_yourself#DRY_vs_WET_solutions\">WET</a>, hence the word choice. It means: try to keep things as DRY as possible, but also recognize that you’ll inevitably need some amount of “moisture” to produce documentation.</p></blockquote>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Effective documentation is <em>kept up-to-date</em>. This is surprisingly challenging. We may begin our project with the best of intentions and great documentation, but as our software evolves and we are quickly iterating, it can be easy to fall out of step. If you are working as part of an agile development team, I recommend adding documentation to your team's \"definition of done.\" For independent projects, try to treat documentation as an important final step.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Documentation that is <em>easy to contribute to</em> is also easy to keep up-to-date. The simplest way to make documentation easy to contribute to is to treat it as code, storing it as text in source control. The site and book <a href=\"http://docslikecode.com/about/\">Docs Like Code</a> advocates for treating our docs like our code by using source control, automating builds, and applying software development tools and techniques to our documentation practices.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Documentation is only as helpful as it is <em>easy to find</em>. Keeping an updated README file and linking to more extensive documentation at the top of the README when necessary helps to keep discoverability simple.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>I hope these guidelines are useful as you draft your project's documentation. Sometimes it is helpful to remember that documentation isn't just for other developers, but often for our future selves as well. When we return to a project after a number of months, we will appreciate the work we put into clear and up-to-date documentation.</p>\r\n",
      "\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/the-eight-rules-of-good-documentation'>The eight rules of good documentation.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/eX305rPUBmg\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Stephen Gates on the growing risks posed by malicious bots\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Tz2Uj3LxvxM/stephen-gates-on-the-growing-risks-posed-by-malicious-bots\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/internet_map_1024_crop-a43b7803eb72ee32ec9b3c7dbfc4b2e8.jpg'/></p><p><em>The O’Reilly Podcast: Protecting your organization against current and future threats.</em></p><p>In this episode of the O’Reilly podcast, I spoke with Stephen Gates of Oracle Dyn. Gates joined the Oracle Dyn Global Business Unit from Zenedge, the web application security company recently acquired by Oracle. Gates and I discussed how growing malicious bot activity impacts organizations.</p>\r\n",
      "\r\n",
      "<iframe src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/430678605&amp;auto_play=false&amp;hide_related=false&amp;show_artwork=true\" height=\"166\" width=\"100%\" frameborder=\"no\" scrolling=\"no\"></iframe>\r\n",
      "\r\n",
      "<p>Here are some highlights:</p>\r\n",
      "\r\n",
      "<h2>The rise of malicious bots</h2>\r\n",
      "<blockquote>\r\n",
      "<p>One of the factors driving the current proliferation of malicious bots is the <a href=\"https://en.wikipedia.org/wiki/Mirai_(malware)\">Mirai malware</a>. It works by using a list of default usernames and passwords (from previous data breaches) to take control of IoT devices. One key differentiator with Mirai is that it’s self-propagating—each infected device has the ability to scan the internet to find similar devices and subsequently infect them. This has also spurred other self-propagating, copycat malware.</p>\r\n",
      "\r\n",
      "<p>Another key factor driving malicious bot growth is the increase in malware that focuses on exploiting vulnerabilities (versus relying on usernames and passwords). The malware automates the process of scanning and infecting IoT devices for known vulnerabilities. Then, they're exploiting those known software vulnerabilities, which are quite common in IoT devices. The volume of attack traffic these devices can generate is a huge differentiator because many of these devices have access to pretty sizeable CPUs, and they've got access to a lot of bandwidth. As a result, we're seeing DDoS attacks being launched by these botnets in excess of 1.5 terabits per second. That's enough traffic to take a small country offline.</p>\r\n",
      "</blockquote>\r\n",
      "\r\n",
      "<h2>Mitigating malicious bot threats</h2>\r\n",
      "<blockquote>\r\n",
      "<p>To manage the risks malicious bots pose, organizations need to be aware that, realistically, bots represent a significant portion of site and application traffic. They must understand the threats against their specific business models and recognize the need to build systems that can differentiate between good bot traffic, bad bot traffic, and human activity. Sites and applications must allow good bots to continue performing critical activities (scrape data for Google search queries, for example), but also mitigate malicious bot activity. Done poorly, you could reduce the effectiveness of your SEO, or worse yet, block paying customers from your sites or applications.</p>\r\n",
      "\r\n",
      "<p>An effective DDoS incident response plan includes detection and mitigation of these bot-driven attacks. Defenses should be layered, including cloud-based defenses for volumetric attacks, and most likely web application firewalls for the more measured attacks. Without having a DDoS response plan in place, you’re a sitting duck and effectively just waiting for one of these attacks to take your organization offline.</p>\r\n",
      "</blockquote>\r\n",
      "\r\n",
      "<h2>Preparing for the malicious bot attacks of the future</h2>\r\n",
      "<blockquote><p>The malicious bot threat landscape will continue to evolve rapidly. To prepare, organizations must embrace advanced data capabilities, such as AI and supervised machine learning, to detect and defeat sophisticated malicious bot attacks. Additionally, businesses need to focus on hiring security intelligence analysts. Embracing AI and machine learning is only helpful if we have the capabilities to analyze the output. Accordingly, security analyst skills will be in very high demand.</p></blockquote>\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between O'Reilly and Oracle Dyn. <a href=\"http://www.oreilly.com/about/editorial_independence.html\">See our statement of editorial independence</a>.</em></p>\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/stephen-gates-on-the-growing-risks-posed-by-malicious-bots'>Stephen Gates on the growing risks posed by malicious bots.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/Tz2Uj3LxvxM\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Simon Moss on using artificial intelligence to fight financial crimes\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Z9wOokDjPqw/simon-moss-on-using-artificial-intelligence-to-fight-financial-crimes\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/217357080_5bba748a2a_o_crop-352b74bd2ba8b0621d3ec9db882608da.jpg'/></p><p><em>Innovations that increase detection of, and response to, criminal attacks of financial systems.</em></p><p>In this episode of the O’Reilly Podcast, I talk with Simon Moss, vice president of industry consulting and solutions, Americas, at <a href=\"https://www.teradata.com/Consulting\">Teradata</a>. We discuss how machine learning and deep learning techniques are being used to fight financial crimes, such as credit card fraud, identity theft, health care fraud, and money laundering.</p>\r\n",
      "\r\n",
      "<iframe src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/430459620&amp;auto_play=false&amp;hide_related=false&amp;show_artwork=true\" height=\"166\" width=\"100%\" frameborder=\"no\" scrolling=\"no\"></iframe>\r\n",
      "\r\n",
      "<p><strong>Discussion points:</strong></p>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>Moss says AI techniques are “a new set of weapons” against perpetrators of financial crime. “If we use them right, we can finally at least slow down the constant tide of financial crime.”</li>\r\n",
      "\t<li>AI can be more effective than traditional methods of combatting identity fraud, health care fraud, and money laundering, because for those issues, he explains, “you’re not looking for a needle in a haystack. You’re looking for a needle in a stack of needles. You are trying to find individuals whose nefarious activity is disguised and hidden in pure normality, by completely innocuous activity.”</li>\r\n",
      "\t<li>Moss compares the use of machine learning techniques to a rules engine: “a rules engine looks for behaviors that have already happened, whereas machine learning is trying to connect different bread crumbs. It’s running multiple scenarios at the same time to try to look at the problem from multiple different angles.”</li>\r\n",
      "\t<li>Machine learning can add efficiency to the detection process: “It can take multiple data sources, map the data to a case, analyze it, and then in seconds, make a decision on whether it’s a false positive, whether it’s normal business activity, whether it’s something that needs further investigation, or whether it is outright criminality,” Moss says.</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<p><strong>Other links:</strong></p>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.thinkbiganalytics.com/\">Think Big Analytics</a>, the consulting division of Teradata</li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://www.linkedin.com/pulse/have-billions-dollars-organizations-technology-regulatory-simon-moss?trk=portfolio_article-card_title\">Moss’ recent LinkedIn article </a>on the fight against money laundering</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between Teradata and O’Reilly. See our <a href=\"http://www.oreilly.com/about/editorial_independence.html\">statement of editorial independence</a>.</em></p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/simon-moss-on-using-artificial-intelligence-to-fight-financial-crimes'>Simon Moss on using artificial intelligence to fight financial crimes.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/Z9wOokDjPqw\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 16 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/gGC3adn_Fzw/four-short-links-16-april-2018\n",
      "<p><em>Light-Powered Camera, Government Blogging, TensorFlow.js, and Metanotation</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://techxplore.com/news/2018-04-imaging-sensor-powered-sunlight.html\">Light-Powered Camera</a> -- prototype gets 15 frames/second, no external power. The light is used for both image sensing and solar power.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.publicstrategist.com/2018/02/government-blogs-and-government-bloggers/\">Government Blogs and Government Bloggers</a> (Public Strategist) -- the blogging spectrum 2x2 is solid and explains why government blogs are often about prototypes, not operations.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db\">Introducing TensorFlow.js</a> -- <i>an open source library you can use to define, train, and run machine learning models entirely in the browser, using Javascript and a high-level layers API.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://bit.ly/2H0rqjx\">It's Time for a New Old Programming Language</a> (YouTube) -- Guy L. Steele Jr.'s talk about the Computer Science Metanotation that CS papers use to indicate programs without having to use a specific programming language. This is one for your inner CS meta-nerd.</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2537?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Data Science &amp; Machine Learning\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-16-april-2018'>Four short links: 16 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/gGC3adn_Fzw\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 13 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/SmyvsxSzbVg/four-short-links-13-april-2018\n",
      "<p><em>Compositing, Exfiltrating, Listening, and Munging</em></p><ol>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1804.03189\">Deep Painterly Harmonisation</a> -- composite and preserve the style of the destination image. The <a href=\"https://github.com/luanfujun/deep-painterly-harmonization\">examples</a> are impressive.</li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1804.04014\">PowerHammer: Exfiltrate Data Over Power Lines</a> -- <i>In this case, a malicious code running on a compromised computer can control the power consumption of the system by intentionally regulating the CPU utilization. Data is modulated, encoded, and transmitted on top of the current flow fluctuations, and then it is conducted and propagated through the power lines.</i>\r\n",
      "</li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1804.03619\">Learn To Listen At The Cocktail Party</a> -- <i>We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality.</i>\r\n",
      "</li>\r\n",
      "\t<li>\r\n",
      "<a href=\"https://github.com/uber/prototool\">prototool</a> -- <i>a Swiss Army Knife for protocol buffers</i>.</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2537?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Data Science &amp; Machine Learning\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-13-april-2018'>Four short links: 13 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/SmyvsxSzbVg\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Jupyter is where humans and data science intersect\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/L2RhtI1GEno/jupyter-is-where-humans-and-data-science-intersect\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/callum-wale-325611-unsplash_crop-e81366bd180b764ccb03f6e69d20e523.jpg'/></p><p><em>Discover how data-driven organizations are using Jupyter to analyze data, share insights, and foster practices for dynamic, reproducible data science.</em></p><p>I'm grateful to join Fernando Pérez and Brian Granger as a program co-chair for <a href=\"https://conferences.oreilly.com/jupyter/jup-ny?intcmp=il-data-confreg-lp-jpny18_new_site_jupyter_is_where_humans_and_data_science_intersect_body_text_link\">JupyterCon 2018</a>. Project Jupyter, NumFOCUS, and O'Reilly Media will present the second annual JupyterCon in New York City August 21–25, 2018.</p>\r\n",
      "<p>Timing for this event couldn't be better. The human side of data science, machine learning/AI, and scientific computing is more important than ever. This is seen in the broad adoption of data-driven decision-making in human organizations of all kinds, the increasing importance of human centered design in tools for working with data, the urgency for better data insights in the face of complex socioeconomic conditions worldwide, as well as dialogue about the social issues these technologies bring to the fore: collaboration, security, ethics, data privacy, transparency, propaganda, etc.</p>\r\n",
      "<p>To paraphrase our co-chairs, Brian Granger:</p>\r\n",
      "<blockquote><p>Jupyter is where humans and data science intersect.</p></blockquote>\r\n",
      "<p>And Fernando Perez:</p>\r\n",
      "<blockquote><p>The better the technology, the more important that human judgement becomes.</p></blockquote>\r\n",
      "<p>Consequently, we'll explore three main themes at JupyterCon 2018:</p>\r\n",
      "<ul>\r\n",
      "<li>\r\n",
      "<strong>Interactive computing with data at scale:</strong> the technical best practices and organizational challenges of supporting interactive computing in companies, universities, research collaborations, etc., (JupyterHub)</li>\r\n",
      "<li>\r\n",
      "<strong>Extensible user interfaces</strong> for data science, machine learning/AI, and scientific computing (JupyterLab)</li>\r\n",
      "<li>\r\n",
      "<strong>Computational communication:</strong> taking the artifacts of interactive computing and communicating them to different audiences</li>\r\n",
      "</ul>\r\n",
      "<p>A meta-theme that ties these together is extensible software architecture for interactive computing with data. Jupyter is built on a set of flexible, extensible, and re-usable building blocks that can be combined and assembled to address a wide range of usage cases. These building blocks are expressed through the various open protocols, APIs, and standards of Jupyter.</p>\r\n",
      "<p>The Jupyter community has much to discuss and share this year. For example, success stories such as the <a href=\"https://youtu.be/xuNj5paMuow?t=12m17s\">data science program at UC Berkeley</a> illustrate the power of <a href=\"https://jupyterhub.readthedocs.io/en/latest/\">JupyterHub</a> deployments at scale in education, research, and industry. As universities and enterprise firms learn to handle the technical challenges of rolling out hands-on, interactive computing at scale, a cohort of organizational challenges come to the fore: practices regarding collaboration, security, compliance, data privacy, ethics, etc. These points are especially poignant in verticals such as health care, finance, and education, where the handling of sensitive data is rightly constrained by ethical and legal requirements (HIPAA, FERPA, etc.). Overall, this dialogue is extremely relevant—it is happening at the intersection of contemporary political and social issues, industry concerns, new laws (GDPR), the evolution of computation, plus good storytelling and communication in general—as we'll explore with practitioners throughout the conference.</p>\r\n",
      "<p>The recent <a href=\"https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906\">beta release of JupyterLab</a> embodies the meta-theme of extensible software architecture for interactive computing with data. While many people think of Jupyter as a \"notebook,\" that's merely one building block needed for interactive computing with data. Other building blocks include terminals, file browsers, LaTeX, markdown, rich outputs, text editors, and renderers/viewers for different data formats. JupyterLab is the next-generation user interface for Project Jupyter, and provides these different building blocks in a flexible, configurable, customizable environment. This opens the door for Jupyter users to build custom workflows, and also for organizations to extend JupyterLab with their own custom functionality.</p>\r\n",
      "<p>Thousands of organizations require data infrastructure for reporting, sharing data insights, reproducing results of analytics, etc. Recent <a href=\"https://www.oreilly.com/ideas/the-state-of-ai-adoption\">business studies</a>estimate that more than half of all companies globally are precluded from adopting AI technologies due to a lack of digital infrastructure— often because their efforts toward data and reporting infrastructure are buried in technical debt. So much of that infrastructure was built from scratch, even when organizations needed essentially the same building blocks. JupyterLab's primary goal is to make it <em>routine</em> to build highly customized, interactive computing platforms, while supporting more than 90 different popular programming environments.</p>\r\n",
      "\r\n",
      "<figure class=\"center\" id=\"id-67XiM\"><img alt=\"JupyterLab\" src=\"https://d3ansictanv2wj.cloudfront.net/JupyterConFigure1-f38a7cba048cb78c03a5be1cd4a39825.png\"><figcaption><span class=\"label\">Figure 1. </span>Screenshot from the <a href=\"https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906\">JupyterLab</a> beta release. Image used with permission from Project Jupyter contributors.</figcaption></figure>\r\n",
      "\r\n",
      "<p>A third major theme builds on top of the other two: computational communication. For data and code to be useful for humans, who need to make decisions, it has to be embedded into a narrative—a story— that that can be communicated to others. Examples of this pattern include: data journalism, reproducible research and open science, computational narratives, open data in society and government, citizen science, and really any area of scientific research (physics, zoology, chemistry, astronomy, etc.), plus the range of economics, finance, and econometric forecasting.</p>\r\n",
      "<p>Another growing segment of use cases involves Jupyter as a \"last-mile\" layer for leveraging AI resources in the cloud. This becomes especially important in light of new hardware emerging for AI needs, vying with competing demand from online gaming, virtual reality, cryptocurrency mining, etc.</p>\r\n",
      "<p>Please take the following as personal opinion, observations, perspectives: we've reached a point where hardware appears to be evolving more rapidly than software, while software appears to be evolving more rapidly than effective process. At O'Reilly Media, we work to <a href=\"https://youtu.be/s3ha6vHapcI\">map the emerging themes</a> in industry, in a process nicknamed \"radar.\" This perspective about hardware is a theme I've been mapping, and meanwhile comparing notes with industry experts. A few data points to consider: Jeff Dean's talk at NIPS 2017, \"<a href=\"http://learningsys.org/nips17/assets/slides/dean-nips17.pdf\">Machine Learning for Systems and Systems for Machine Learning</a>\" about comparisons of CPUs/GPUs/TPUs, and how AI is transforming the design of computer hardware; <a href=\"https://www.arxiv-vanity.com/papers/1712.01208/\"><em>The Case for Learned Index Structures</em></a>, also from Google, about the impact of \"branch vs. multiple\" costs on decades of database theory; this podcast interview \"<a href=\"https://www.oreilly.com/ideas/scaling-machine-learning\">Scaling machine learning</a>\" with Reza Zadeh about the critical importance of hardware/software interfaces in AI apps; the <a href=\"https://www.youtube.com/watch?v=Q7y9l-L8yiU\">video interview</a> that Wes McKinney and I recorded at JupyterCon 2017 about how <a href=\"https://arrow.apache.org/\">Apache Arrow</a> presents a much different take on how to leverage hardware and distributed resources.</p>\r\n",
      "<p>The notion that \"hardware &gt; software &gt; process\" contradicts the past 15–20 years of software engineering practice. It's an inversion of the general assumptions we make. In response, industry will need to rework approaches for building software within the context of AI— which was articulated succinctly by Lenny Pruss from Amplify Partners in \"<a href=\"https://venturebeat.com/2017/11/28/infrastructure-3-0-building-blocks-for-the-ai-revolution/\">Infrastructure 3.0: Building blocks for the AI revolution</a>.\" In this light, Jupyter provides an abstraction layer— a kind of buffer to help \"future proof\"— for complex use cases in NLP, machine learning, and related work. We're seeing this from most of the public cloud vendors, who are also leaders in AI, Google, Amazon, Microsoft, IBM, etc., and who will be represented at the conference in August.</p>\r\n",
      "<p><a href=\"https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/presentations?intcmp=il-data-confreg-lp-jpny18_new_site_jupyter-is-where-humans-and-data-science-intersect_body_text_link\">Our program at JupyterCon</a> will feature expert speakers across all of these themes. However, to me, that's merely the tip of the iceberg. So much of the real value that I get from conferences happens in the proverbial \"Hallway Track,\" where you run into people who are riffing off news they've just learned in a session— perhaps in line with your thinking, perhaps in a completely different direction. Those conversations have space to flourish when people get immersed in the community, the issues, the possibilities.</p>\r\n",
      "<p>It'll be a busy week. We'll have two days of training courses: intensive, hands-on coding, a lot of interaction with expert instructors. Training will overlap with one day of tutorials: led by experts, generally larger than training courses (though more detailed than session talks), featuring a lot of Q&amp;A.</p>\r\n",
      "<p>Then we'll have two days of keynotes and session talks, expo hall, lunches and sponsored breaks, plus Project Jupyter sponsored events. Events include Jupyter User Testing, author signings, \"Meet the Experts\" office hours, demos in the vendor expo hall— plus related meetups in the evenings. Last year, the Poster Session was one of the biggest surprises to me: it was such a hit that it was difficult to move through the room; walkways were packed with people asking presenters questions about their projects.</p>\r\n",
      "<p>This year, we'll introduce a Business Summit, similar to the popular summits at the <a href=\"https://conferences.oreilly.com/strata\">Strata Data Conferences</a> and <a href=\"https://conferences.oreilly.com/artificial-intelligence/\">The AI Conference</a>. This will include high-level presentations on the most promising and important developments in Jupyter for executives and decision-makers. Brian Granger and I will be hosting the Business Summit, along with Joel Horwitz of IBM. One interesting data point: among the regional events, we've seen much more engagement this year from enterprise and government than we'd expected, more emphasis on business use cases and new product launches. The ecosystem is growing, and will be represented well at JupyterCon!</p>\r\n",
      "<p>We will also feature an Education Track in the main conference, expanding on the well-attended Education Birds-of-a-Feather and related talks during JupyterCon 2017. Use of Jupyter in education has grown rapidly across many contexts: middle/high-school, universities, corporate training, and online courses. Lorena Barba and Robert Talbert will be organizing this track.</p>\r\n",
      "<p>Following our schedule of conference talks, the week wraps up with a community sprint day on Saturday. You can work side-by-side with leaders and contributors in the Jupyter ecosystem to implement that feature you've always wanted, fix bugs, work on design, write documentation, test software, or dive deep into the internals of something in the Jupyter ecosystem. Be sure to bring your laptop.</p>\r\n",
      "<p>Note that we believe true innovation depends on hearing from, and listening to, people with a variety of perspectives. Please read our <a href=\"http://www.oreilly.com/diversity/\" data-href=\"http://www.oreilly.com/diversity/\">Diversity Statement</a> for more details. Also, we're committed to creating a safe and productive environment for everyone at all of our events. Please read our <a href=\"https://conferences.oreilly.com/jupyter/jup-ny/public/content/conduct\">Code of Conduct</a>. Last year, we were able to work with the community plus matching donations to provide several \"Diversity &amp; Inclusion\" scholarships, as well as more than dozen student scholarships. We're looking forward to building on that this year!</p>\r\n",
      "<p>That's a sample of what's coming up for JupyterCon in NYC this August. In preparation for the event, we'll also help present and sponsor a regional community event—check out <a href=\"https://www.eventbrite.com/e/jupyter-pop-up-dc-tickets-44090939186\">Jupyter Pop-up DC, May 15, 2018</a>. We look forward to many opportunities to showcase new work and ideas, to meet each other, to learn about the architecture of the project itself, and to contribute to the future of Jupyter.</p>\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/jupyter-is-where-humans-and-data-science-intersect'>Jupyter is where humans and data science intersect.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/L2RhtI1GEno\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "The importance of transparency and user control in machine learning\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/JN_Jmuznbzo/the-importance-of-transparency-and-user-control-in-machine-learning\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/2000px-peripheral_drift_illusion_rotating_snakes_crop-a4fda3549d2a681fa7639e3845c103fa.jpg'/></p><p><em>The O’Reilly Data Show Podcast: Guillaume Chaslot on bias and extremism in content recommendations.</em></p><p>In this episode of the <a href=\"https://www.oreilly.com/ideas/topics/oreilly-data-show-podcast\">Data Show</a>, I spoke with <a href=\"https://www.linkedin.com/in/guillaume-chaslot-6774b982/\">Guillaume Chaslot</a>, an ex-YouTube engineer and founder of <a href=\"https://algotransparency.org\">AlgoTransparency</a>, an organization dedicated to helping the public understand the profound impact algorithms have on our lives. We live in an age when many of our interactions with companies and services are governed by algorithms. At a time when their impact continues to grow, there are many settings where these algorithms are far from transparent. There is growing awareness about the vast amounts of <a href=\"https://amp.theguardian.com/commentisfree/2018/mar/28/all-the-data-facebook-google-has-on-you-privacy\">data companies are collecting on their users and customers</a>, and people are starting to demand control over their data. A similar conversation is starting to happen about algorithms—users are wanting more control over what these models optimize for and an understanding of how they work.</p>\r\n",
      "\r\n",
      "<p>I first came across Chaslot through a series of articles about the <a href=\"https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html\">power and impact of YouTube on politics and society</a>. Many of the articles I read relied on <a href=\"https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot\">data and analysis supplied by Chaslot</a>. We talked about his work trying to decipher how YouTube’s recommendation system works, filter bubbles, transparency in machine learning, and data privacy.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/the-importance-of-transparency-and-user-control-in-machine-learning'>The importance of transparency and user control in machine learning.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/JN_Jmuznbzo\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 12 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/UEgbKhQNWYc/four-short-links-12-april-2018\n",
      "<p><em>Probabilistic Programming, Bad Copyright, Technical Debt, and Video Data Set</em></p><p></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245\">TensorFlow Probability</a> -- <i>a probabilistic programming toolbox for machine learning.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.eff.org/deeplinks/2018/04/european-copyright-law-isnt-great-it-could-soon-get-lot-worse\">European Copyright Law Isn't Great. It Could Soon Get a Lot Worse.</a> (EFF) -- <i>The practical effect of this could be to make it impossible for a news publisher to publish their stories for free use, for example by using a Creative Commons license.</i> (via <a href=\"https://boingboing.net/2018/04/11/evidence-free-zone.html\">BoingBoing</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://engineering.riotgames.com/news/taxonomy-tech-debt\">A Taxonomy of Technical Debt</a> -- you can argue about whether his categories are your categories, but it's useful to have words for the nuance.</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://moments.csail.mit.edu/\">Moments in Time Data Set</a> -- <i>A large-scale data set for recognizing and understanding action in videos.</i> (via <a href=\"http://news.mit.edu/2018/mit-ibm-watson-ai-lab-computers-dynamic-events-0405\">MIT News</a>)</li>\r\n",
      "</ol>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2537?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Data Science &amp; Machine Learning\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-12-april-2018'>Four short links: 12 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/UEgbKhQNWYc\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Using qualitative and quantitative data to design better user experiences\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-gA93-tgDlQ/using-qualitative-and-quantitative-data-to-design-better-user-experiences\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/questions-2110967_1920_crop-17aa816fe92fa65e3613740a6efdc40f.jpg'/></p><p><em>It's important to know the \"why\" in addition to the \"what\" in UX design.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/using-qualitative-and-quantitative-data-to-design-better-user-experiences'>Using qualitative and quantitative data to design better user experiences.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/-gA93-tgDlQ\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Probing the pill box: Repurposing drugs for new treatments\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/OqElFEaWaaw/probing-the-pill-box-repurposing-drugs-for-new-treatments\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/headache-1540220_1920_crop-dae734f2316785b8955881c5afbe1428.jpg'/></p><p><em>As the cost of developing new drugs rises, researchers are investigating new ways to use existing medicines</em></p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>With the development of new medicines often exceeding <a href=\"https://www.ncbi.nlm.nih.gov/books/NBK202175/\">10 years and topping $1 billion</a>, faster and cheaper methods of drug development are sorely needed. Drug repurposing, whereby drugs approved for one condition are used to treat a completely different disease, is a fast-emerging strategy aimed at circumventing this daunting pipeline.  Several biotechnology companies now specialize in identifying new drug-disease pairs by interrogating massive biomedical datasets with ever-evolving artificial intelligence algorithms. This systematic approach to repurposing, augmenting the chance application of drugs to new diseases, represents a powerful tool for producing new medicines. Indeed, major drug companies such as GlaxoSmithKline have committed to probing “<a href=\"https://resources.sharevault.com/ai-in-drug-discovery-and-development\">dark data</a>,” or failed trial data, to achieve this goal.</p>\r\n",
      "\r\n",
      "\r\n",
      "<h2>The Promise of Drug Repositioning</h2>\r\n",
      "\r\n",
      "<p>The time and money required to bring a new drug to market severely limits the number of new treatments, in part explaining the <a href=\"http://www.nature.com/articles/nrd3681\">steady decline</a> in the number of approved drugs entering use each year. The drug development process, overseen in the United States by the Food and Drug Administration (FDA), has three main stages. Candidate compounds, which are being generated in abundance by discovery science and have established mechanisms of action, safe dosages, and a pharmacokinetic profile, begin in Phase I trials. In this trial stage, the safety profile of the compound is established, usually in healthy volunteers. Promising candidates are selected for Phase II trials, which determine the effectiveness of the compound at treating a particular disease or indication in a small sample group. Positive candidates are admitted to a Phase III trial, which takes place in a larger patient population, where the efficacy of the drug and its side-effects are fully assessed.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/probing-the-pill-box-repurposing-drugs-for-new-treatments'>Probing the pill box: Repurposing drugs for new treatments.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/OqElFEaWaaw\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Data engineers vs. data scientists\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/h07p-vrbmtI/data-engineers-vs-data-scientists\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/comparison_of_truncated_icosahedron_and_soccer_ball_crop-4e276182b0f32fd8a36700aa937f7d33.jpg'/></p><p><em>The two positions are not interchangeable&mdash;and misperceptions of their roles can hurt teams and compromise productivity.</em></p><p>It’s important to understand the differences between a data engineer and a data scientist. Misunderstanding or not knowing these differences are making teams fail or underperform with big data.</p>\n",
      "<p>A key misunderstanding is the strengths and weaknesses of each position. I think some of these misconceptions come from the diagrams that are used to describe data scientists and data engineers.</p>\n",
      "<figure class=\"center\" id=\"id-6YOix\"><img alt=\"venn diagram with data scientists and data engineers\" width=\"60%\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure1-42ac4b8503ed9b17c941d48f6c81f147.png\"><figcaption><span class=\"label\">Figure 1. </span>Overly simplistic venn diagram with data scientists and data engineers. Illustration by Jesse Anderson.</figcaption></figure>\n",
      "\n",
      "\n",
      "<p>Venn diagrams like Figure 1 oversimplify the complex positions and how they’re different. It makes the two positions seem interchangeable. Yes, both positions work on big data. However, what each position does to create value or data pipelines with big data is very different. This difference comes from the base skills of each position.</p>\n",
      "<section data-type=\"sect1\" id=\"what-are-data-scientists-and-data-engineers-RDVsG\">\n",
      "<h2>What are data scientists and data engineers?</h2>\n",
      "\n",
      "<p>When I work with organizations on their team structures, I don’t use a Venn diagram to illustrate the relationship between a data engineer and a data scientist. I draw the diagram as shown in Figure 2.</p>\n",
      "<figure class=\"center\" id=\"id-N4iaIk\"><img alt=\"core competencies of data scientists and data engineers and their overlapping skills\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure2-514e275cca0a0450485a49c805aeb321.png\"><figcaption><span class=\"label\">Figure 2. </span>Diagram showing the core competencies of data scientists and data engineers and their overlapping skills. Illustration by Jesse Anderson and the Big Data Institute.</figcaption></figure>\n",
      "\n",
      "\n",
      "<section data-type=\"sect2\" id=\"data-scientists-skills-JMsvtL\">\n",
      "<h3>Data scientists’ skills</h3>\n",
      "\n",
      "<p>At their core, data scientists have a math and statistics background (sometimes physics). Out of this math background, they’re creating advanced analytics. On the extreme end of this applied math, they’re creating machine learning models and artificial intelligence.</p>\n",
      "<p>Just like their software engineering counterparts, data scientists will have to interact with the business side. This includes understanding the domain enough to make insights. Data scientists are often tasked with analyzing data to help the business, and this requires a level of business acumen. Finally, their results need to be given to the business in an understandable fashion. This requires the ability verbally and visually communicate complex results and observations in a way that the business can understand and act on them.</p>\n",
      "<p>My one sentence definition of a data scientist is: a data scientist is someone who has augmented their math and statistics background with programming to analyze data and create applied mathematical models.</p>\n",
      "<p>A common data scientist trait is that they’ve picked up programming <strong>out of necessity</strong> to accomplish what they couldn’t do otherwise. When I talk to data scientists, this is a common thing they tell me. In order to accomplish a more complicated analysis or because of an otherwise insurmountable problem, they learned how to program. Their programming and system creation skills aren’t the levels that you’d see from a programmer or data engineer—<strong>nor should they be</strong>.</p>\n",
      "<section data-type=\"sect2\" id=\"data-engineers-skills-V7svTjt1\">\n",
      "<h3>Data engineers’ skills</h3>\n",
      "\n",
      "<p>At their core, data engineers have a programming background. This background is generally in Java, Scala, or Python. They have an emphasis or specialization in distributed systems and big data. A data engineer has advanced programming and system creation skills.</p>\n",
      "<p>My one sentence definition of a data engineer is: a data engineer is someone who has specialized their skills in creating software\n",
      "solutions around big data.</p>\n",
      "<p>Using these engineering skills, they create data pipelines. Creating a data pipeline may sound easy or trivial, but at big data scale, this means bringing together 10-30 different big data technologies. More importantly, a data engineer is the one who understands and chooses the <a href=\"https://www.jesse-anderson.com/2017/07/this-is-useless-without-use-cases/\">right tools for the job</a>. A data engineer is the one who understands the various technologies and frameworks in-depth, and how to combine them to create solutions to enable a company’s business processes with data pipelines.</p>\n",
      "<p>In my experience, a data engineer is only tangentially involved in the operations of the cluster (in contrast to what’s said about data engineers <a href=\"https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers\">here</a>). Though some data science technologies really require a DevOps or DataOps set up, the majority of technologies don't. Just like with most programers, I wouldn't allow them direct access to the production system. That's primarily the job for system administrators or DevOps.</p>\n",
      "</section>\n",
      "<section data-type=\"sect2\" id=\"overlapping-skills-rWsNcNtW\">\n",
      "<h3>Overlapping skills</h3>\n",
      "\n",
      "<p>There is an overlap between a data scientist and a data engineer. However, the overlap happens at the ragged edges of each one’s abilities.</p>\n",
      "<p>For example, they overlap on analysis. However, a data scientist’s analytics skills will be far more advanced than a data engineer’s analytics skills. A data engineer can do some basic to intermediate level analytics, but will be hard pressed to do the advanced analytics that a data scientist does.</p>\n",
      "<p>Both a data scientist and a data engineer overlap on programming. However, a data engineer’s programming skills are well beyond a data scientist’s programming skills. Having a data scientist create a data pipeline is at the far edge of their skills, but is the bread and butter of a data engineer. In this way, the two roles are complementary, with data engineers supporting the work of data scientists.</p>\n",
      "<p>You’ll notice that there is another overlap between a data scientist and a data engineer—that of big data. Understanding each positions’ skills better, you can now understand the overlap. Data engineers use their programming and systems creation skills to create big data pipelines. Data scientists use their more limited programming skills and apply their advanced math skills to create advanced data products <strong>using those existing data pipelines</strong>. This difference between creating and using lies at the core of a team's failure or underperforming with big data. A team that expects their data scientists to create the data pipelines will be woefully disappointed.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"when-organizations-get-it-wrong-9psxhe\">\n",
      "<h2>When organizations get it wrong</h2>\n",
      "\n",
      "<p>It’s unfortunately common for organizations to misunderstand the core skills and roles of each position. Some organizations believe that a data scientist can create data pipelines. A data scientist can create a data pipeline after a fashion. The issues with a data scientist creating a data pipeline are several fold. Remember that a data scientist has only learned programming and big data out of necessity. They’re smart people and can figure things out—eventually. Creating a data pipeline isn’t remotely their core competency.</p>\n",
      "<p>From the managerial point of view, the data science team will appear stuck. You’ll look around or hear about other teams and compare their progress to your team’s progress. It will appear as if the data science team isn’t performing or greatly under performing. This is an unfair evaluation based on misunderstanding the core competency of a data scientist.</p>\n",
      "<section data-type=\"sect2\" id=\"data-scientists-doing-data-engineering-yDsgtZhn\">\n",
      "<h3>Data scientists doing data engineering</h3>\n",
      "\n",
      "<p>I’ve seen companies task their data scientists with things you’d have a data engineer do. The data scientists were running at 20-30% efficiency. The data scientist doesn’t know things that a data engineer knows off the top of their head. Creating a data pipeline isn’t an easy task—it takes advanced programming skills, big data framework understanding, and systems creation. These aren’t skills that an average data scientist has. A data scientist can acquire these skills; however, the return on investment (ROI) on this time spent will rarely pay off. Don’t misunderstand me: a data scientist does need programming and big data skills, just not at the levels that a data engineer needs them.</p>\n",
      "<p>There is also the issue of data scientists being relative amateurs in this data pipeline creation. A data scientist will make mistakes and wrong choices that a data engineer would (should) not. A data scientist often doesn’t know or understand the right tool for a job. Everything will get collapsed to using a single tool (usually the wrong one) for every task. The reality is that many different tools are needed for different jobs. A qualified data engineer will know these, and data scientists will often not know them.</p>\n",
      "<p>A recent example of this was a data scientist using Apache Spark to process a data set in the 10s of GB. Yes, Spark can process that amount of data. However, a small data program would have been much, much faster and better. Their Spark job was taking 10-15 minutes to execute, but the small data RDBMS took 0.01 seconds to accomplish the same thing. In this case, the data scientist solved the problem after a fashion, but didn’t understand what the right tool for the job was. Times that 15 minutes spent running that job by 16 times in a day (that’s on the low end for analysis), and your data scientist is spending four hours a day waiting because they’re using the wrong tool for the job.</p>\n",
      "<p>At another organization, their data scientists didn’t have any data engineering resources. The data scientists would work on the problems until they got stuck on a data engineering problem they couldn’t solve. They’d report back to the business that they couldn’t finish things and there it sat, half-finished. This led to the data scientists wasting their time up to that point, and left, by their estimate, millions of dollars on the table because things couldn’t be finished.</p>\n",
      "<p>A more worrisome manifestation of having a data scientist do a data engineer’s work is that the data scientist will get frustrated and quit. I’ve talked to many data scientists at various organizations who were doing data engineer work. The conversation is always the same—the data scientist complains that they came to the company to data science work, not data engineering work. They’ll do data engineering work in a pinch to get something done, but having a data scientist do data engineer work will drive them crazy. They will quit and you will have 3-6 months to get your data engineering act together. I talk more about these issues in <a href=\"http://www.jesse-anderson.com/2017/03/what-happens-when-you-hire-a-data-scientist-without-a-data-engineer/\">another post</a>.</p>\n",
      "</section>\n",
      "<section data-type=\"sect2\" id=\"ratios-of-data-engineers-to-data-scientists-V7sohwh1\">\n",
      "<h3>Ratios of data engineers to data scientists</h3>\n",
      "\n",
      "<p>A common issue is to figure out the ratio of data engineers to data scientists. The general things to consider when choosing a ratio is how complex the data pipeline is, how mature the data pipeline is, and the level of experience on the data engineering team.</p>\n",
      "<p>Having more data scientists than data engineers is generally an issue. It typically means that an organization is having their data scientists do data engineering. As I’ve shown, this leads to all sorts of problems.</p>\n",
      "<p>A common starting point is 2-3 data engineers for every data scientist. For some organizations with more complex data engineering requirements, this can be 4-5 data engineers per data scientist. This includes organizations where data engineering and data science are in different reporting structures. You need more data engineers because more time and effort is needed to create data pipelines than to create the ML/AI portion.</p>\n",
      "<p>I talk more about how data engineering and data science teams should interact with each other in my book <a href=\"http://www.bigdatainstitute.io/books/data-engineering-teams-book/\"><em>Data Engineering Teams</em></a>.</p>\n",
      "</section>\n",
      "<section data-type=\"sect2\" id=\"data-engineers-doing-data-science-rWsJTEhW\">\n",
      "<h3>Data Engineers doing data science</h3>\n",
      "\n",
      "<p>A far less common case is when a data engineer starts doing data science. There is an upward push as data engineers start to improve their math and statistics skills. This upward push is becoming more common as data science becomes more standardized. It’s leading to a brand new type of engineer.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"the-need-for-machine-learning-engineers-ZKs9Te\">\n",
      "<h2>The need for machine learning engineers</h2>\n",
      "\n",
      "<p>Let’s face it—data scientists come from academic backgrounds. They usually have a Ph.D. or master's degree. The issue is that they’d rather write a paper on a problem than get something into production. Other times, their programming abilities only extend to creating something in R. Putting something written in R into production is an issue unto itself. They don’t think in terms of creating systems, like an engineer.</p>\n",
      "<p>The general issue with data scientists is that they’re not engineers who put things into production, create data pipelines, and expose those AI/ML results.</p>\n",
      "<p>To deal with the disparity between an academic mindset and the need to put something in production, we’re seeing a new type of engineer. Right now, this engineer is mostly seen in the U.S. Their title is machine learning engineer.</p>\n",
      "<figure class=\"center\" id=\"id-EaixhKTA\"><img alt=\"where a machine learning engineer fits with a data scientist and data engineer\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure3-7c5de9f92f3406e23d76e6bff3f89818.png\"><figcaption><span class=\"label\">Figure 3. </span>Diagram showing where a machine learning engineer fits with a data scientist and data engineer. Illustration by Jesse Anderson and the Big Data Institute.</figcaption></figure>\n",
      "\n",
      "<p>Machine learning engineers primarily come from data engineering backgrounds. They’re cross-trained enough to become proficient at both data engineering and data science. A less common route is for a data scientist to cross-train on the data engineering side.</p>\n",
      "<p>My one sentence definition of a machine learning engineer is: a machine learning engineer is someone who sits at the crossroads of data science and data engineering, and has proficiency in both data engineering and data science.</p>\n",
      "<p>As you looked at Figure 2, you probably wondered what happens to the gap between data science and data engineering. This exactly where the machine learning engineer fits in, as shown in Figure 3. They’re the conduit between the data pipeline a data engineer creates and what the data scientist creates. A machine learning engineer is responsible for taking what a data scientist finds or creates and making it production worthy (it’s worth noting that most of what a data scientist creates isn’t production worthy and is mostly hacked together enough to work).</p>\n",
      "<p>The machine learning engineer’s job primarily is to create the last mile of the data science pipeline. This might entail several parts. It might be rewriting a data scientist’s code from R/Python to Java/Scala. It might be optimizing the ML/AI code from a software engineering point of view that the data scientist wrote so it runs well (or runs at all). The machine learning engineer has the engineering background to enforce the necessary engineering discipline on a field (data science) that isn’t known for its adherence to good engineering principles.</p>\n",
      "<p>A model running in production requires care and feeding that software doesn’t. A machine learning model can go stale and start giving out incorrect or distorted results. This could be from the nature of the data changing, new data, or a malicious attack. Either way, the machine learning engineer is on the lookout for changes in their model that would require retraining or tweaking.</p>\n",
      "<section data-type=\"sect2\" id=\"machine-learning-engineers-and-data-engineers-jWsdC4TL\">\n",
      "<h3>Machine learning engineers and data engineers</h3>\n",
      "\n",
      "<p>The transition of data engineer to machine learning engineer is a slow-moving process. To be honest, we’re going to see similar revisions to what a machine learning engineer is to what <a href=\"https://www.oreilly.com/ideas/what-are-machine-learning-engineers\">we’ve seen with the definition of data scientists</a>.</p>\n",
      "<p>To explain what I mean by slow moving, I will share the experience of those who I’ve seen make the transition from data engineer to machine learning engineer. They’ve spent years doing development work as a software engineer and then data engineer. They’ve always had an interest in statistics or math. Other times, they just got bored with the constraints of being a data engineer. Either way, this transition took years. I’m not seeing people become machine learning engineers after taking a beginning stats class or after taking a beginning machine learning course.</p>\n",
      "<p>As I much as I razz the data scientists for being academics, data engineers aren’t the right people, either. An engineer loves trues and falses, the black and white, and the ones and zeros of the the world. They don’t like uncertainty. With machine learning, there is a level of uncertainty of the model’s guess (engineers don’t like guessing, either). Unlike most engineers, a machine learning engineer can straddle the certainty of data engineering and the uncertainty of data science.</p>\n",
      "</section>\n",
      "<section data-type=\"sect2\" id=\"the-increasing-value-of-machine-learning-engineers-2ksxsrTX\">\n",
      "<h3>The increasing value of machine learning engineers</h3>\n",
      "\n",
      "<p>The bar for doing data science is gradually decreasing. The best practices are gradually being fleshed out. The most common algorithms are known. Even better, someone has already coded and optimized these algorithms.</p>\n",
      "<p>This increasing maturity is making it easier for both data scientists and machine learning engineers to put things in production without having to code them. We’re also seeing data science become a more automatic and automated process. Google’s <a href=\"https://cloud.google.com/automl/\">AutoML</a> is one such trend where it will find the best algorithm for you automatically and give results without requiring the work of a full-fledged data scientist. <a href=\"https://www.datarobot.com/\">DataRobot</a> is another technology that is automating the process of finding the right data science algorithm for the data. It will also aid the machine learning engineers in putting that algorithm into production.</p>\n",
      "<p>These tools aren’t going to replace hardcore data science, but it will allow data scientists to focus on the more difficult parts of data science. It will allow machine learning engineers to become more and more productive. We’ll see a gradually increasing amount of offloading to machine learning engineers and automation of algorithms.</p>\n",
      "<p>I’m torn on what level of productivity we should expect from machine learning engineers in the future. To grossly oversimplify things, will machine learning engineers be the Wordpress configurators to their web developer counterparts? In this scenario, a machine learning engineer can be productive with very known and standard use cases, and only a data scientist can handle the really custom work. Or will machine learning engineers be the database administrator reborn? Given an in-depth knowledge of the model, they can use a known, cookie-cutter approach to configure a model, get correct results 50-80% of the time, and that’s good enough for what was needed. To get truly accurate results, you would need a data scientist.</p>\n",
      "<p>The key to the productivity of machine learning engineers and data scientists will be <a href=\"https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers\">their tools</a>. There’s a lack of maturity now, and that’s why I’m wondering how productive they’ll be in the future.</p>\n",
      "<p>I expect the bar for doing data science to continue to lower. This will make a machine learning engineer able to accomplish more data science without a massive increase in knowledge. I expect the role of machine learning engineer to become <a href=\"https://economicgraph.linkedin.com/research/LinkedIns-2017-US-Emerging-Jobs-Report\">increasingly common</a> in the U.S. and around the world.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"what-to-do-zVsKcy\">\n",
      "<h2>What to do?</h2>\n",
      "\n",
      "<p>Now that you’ve seen the differences between data scientists and data engineers, you need to go back through your organization and see where you need to make changes. This is a change I’ve helped other organizations accomplish, and they’ve seen tremendous results. In cases where the data science group seemed stuck and unable to perform, we created data engineering teams, showed the data science and data engineering teams how to work together, and put the right processes in place. </p>\n",
      "<p>These changes took the data science team from 20-30% productivity to 90%. The teams were able to do more with the same number of people. The data scientists were happier because they weren’t doing data engineering. Management could start delivering value against the promises of big data.</p>\n",
      "<p>You also met a new position, machine learning engineer. As your data science and data engineering teams mature, you’ll want to check the gaps between the teams. You may need to promote a data engineer on their way to becoming a machine learning engineer or hire a machine learning engineer.</p>\n",
      "<p>Finally, most problems with big data are people and team issues. They are not technical issues (at least not initially). Technology usually gets blamed because it’s far easier to blame technology than to look inward at the team itself. Until you solve your personnel issues, you won’t hit the really tough technical issues or create the value with big data you set out to create. Take an honest look at your team and your organization to see where you need to change.</p>\n",
      "<p>A big thanks to Russell Jurney, Paco Nathan, and Ben Lorica for their feedback.</p>\n",
      "</section>\n",
      "</section>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/data-engineers-vs-data-scientists'>Data engineers vs. data scientists.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/h07p-vrbmtI\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 11 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vAe26u3PZmA/four-short-links-11-april-2018\n",
      "<p><em>Assignment, Warranties, Data, and Public Goods</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.hillelwayne.com/post/equals-as-assignment/\">Why Does \"=\" Mean Assignment</a> -- marvellous history lesson.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://motherboard.vice.com/en_us/article/ne9qdq/warranty-void-if-removed-stickers-illegal-ftc\">Warranty Void if Removed Stickers Are Bull</a> -- <i>Federal law says you can repair your own things, and manufacturers cannot force you to use their own repair services.</i> (via <a href=\"https://boingboing.net/2018/04/10/magnuson-moss-warranty-act.html\">BoingBoing</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://nongnu.org/txr/\">TXR</a> -- a pattern language and a Lisp variant for data problems.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://voxeu.org/article/roman-roads-and-persistence-development\">Roman Roads and Persistence Development</a> -- <i>In some ways, the emergence of the Roman road network is almost a natural experiment—in light of the military purpose of the roads, the preferred straightness of their construction, and their construction in newly conquered and often undeveloped regions. This type of public good seems to have had a persistent influence on subsequent public good allocations and comparative development. At the same time, the abandonment of the wheel shock in MENA appears to have been powerful enough to cause that degree of persistence to break down. Overall, our analysis suggests that a public good provision is a powerful channel through which persistence in comparative development comes about.</i> I wonder whether this kind of analysis is even conceivable with internet public policy like broadband, coding classes, and laws. (via <a href=\"https://boingboing.net/2018/04/10/romanes-eunt-domus.html\">BoingBoing</a>)</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2544?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Law, ethics, and governance\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-11-april-2018'>Four short links: 11 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/vAe26u3PZmA\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "4 things business leaders should know as they explore AI and deep learning\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/KKdWWqe41AU/4-things-business-leaders-should-know-as-they-explore-ai-and-deep-learning\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/ai-curved-architecture-crop-8a59f2872ba205bd87d8375dea4fc24a.jpg'/></p><p><em>Our survey reveals how organizations are using tools, techniques, and training to apply AI through deep learning. </em></p><p>We’re at an exciting point with artificial intelligence (AI). Years of research are yielding tangible results, specifically in the area of deep learning. New projects and related technologies are blossoming. Enthusiasm is high.</p>\r\n",
      "\r\n",
      "<p>Yet the path toward real and practical application of AI and deep learning remains unclear for many organizations. Business and technology leaders are searching for clarity. Where do I start? How can I train my teams to perform this work? How do I avoid the pitfalls?</p>\r\n",
      "\r\n",
      "<p>We conducted a survey<a href=\"#ftn1\"><sup>[1]</sup></a> to help leaders better understand how organizations are applying AI through deep learning and where they’re encountering the biggest obstacles. We identified four notable survey findings that apply to organizations.</p>\r\n",
      "\r\n",
      "<h2>1. There’s an AI skills gap</h2>\r\n",
      "\r\n",
      "<p>Of particular note is an AI skills gap revealed in the survey. 28% of respondents are using deep learning now and 54% say it will play a key role in their future projects. Who will do this work? AI talent is scarce, and the increase in AI projects means the talent pool will likely get smaller in the near future.</p>\r\n",
      "\r\n",
      "<h2>2. Companies are addressing the AI skills gap through training</h2>\r\n",
      "\r\n",
      "<p>Deep learning remains a relatively new technique, one that hasn’t been part of the typical suite of algorithms employed by industrial data scientists. So, it’s no surprise that the main factor holding companies back from trying deep learning is the skills gap. To overcome this gap, a majority (75%) of respondents said their company is using some form of in-house or external training program. Almost half (49%) of respondents said their company offered “in-house on-the-job training.” 35% indicated their company used either formal training from a third party or from individual training consultants or contractors.</p>\r\n",
      "\r\n",
      "<h2>3. Initial deep learning projects often focus on safe upgrades</h2>\r\n",
      "\r\n",
      "<p>The rise of deep learning can be traced to its success in computer vision, speech technologies, and game playing, but our survey shows developers and data scientists are more likely to use it to work with structured or semistructured data. Why? There are good reasons. Upgrading familiar applications with deep learning is a safer investment than starting something new, businesses have a lot of structured and semistructured data already, and the number of businesses that can currently make use of computer vision (to say nothing of gaming) is limited. That said, our respondents see value in vision technology, and new deep learning applications for vision will grow in tandem with text and semistructured data.</p>\r\n",
      "\r\n",
      "<h2>4. TensorFlow is the most popular deep learning tool</h2>\r\n",
      "\r\n",
      "<p>Most respondents (73%) said they’ve begun playing with deep learning software. <a href=\"https://www.tensorflow.org/\">TensorFlow</a> is by far the most popular tool among our respondents, with <a href=\"https://keras.io/\">Keras</a> in second place, and <a href=\"http://pytorch.org/\">PyTorch</a> in third. Other frameworks like <a href=\"https://mxnet.apache.org/\">MXNet</a>, <a href=\"https://github.com/Microsoft/CNTK\">CNTK</a>, and <a href=\"https://github.com/intel-analytics/BigDL\">BigDL</a> have growing audiences as well. We expect all of these frameworks—including those that are less popular now—to continue to add users and use cases.</p>\r\n",
      "\r\n",
      "<p>Looking for more insight? Download our free report, \"<a href=\"http://www.oreilly.com/data/free/how-companies-are-putting-aI-to-work-through-deep-learning.csp?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=4-things-business-leaders-should-know-body-text-cta\">How companies are putting AI to work through deep learning</a>,\" for full findings from our AI and deep learning survey.</p>\r\n",
      "\r\n",
      "<p>We'll also explore these and related AI topics at <a href=\"https://conferences.oreilly.com/artificial-intelligence/ai-ny?intcmp=il-data-confreg-update-ainy18_new_site_4_things_business_leaders_ai_deep_learning_body_cta\">Artificial Intelligence Conference in New York</a>, April 29-May 2, 2018.</p>\r\n",
      "\r\n",
      "<hr>\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RO0Sn\">\r\n",
      "<div id=\"ftn1\">\r\n",
      "<p><sup>[1]</sup> In early 2018 we conducted a survey of subscribers to our AI, data, and programming <a href=\"http://www.oreilly.com/emails/newsletters/index.html\">newsletters</a> and received more than 3,300 responses. We focused on deep learning and assessed the adoption of tools and techniques needed to build AI applications.</p>\r\n",
      "</div>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/4-things-business-leaders-should-know-as-they-explore-ai-and-deep-learning'>4 things business leaders should know as they explore AI and deep learning.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/KKdWWqe41AU\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Strong feedback loops make strong software teams\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-us1qlRyN5w/strong-feedback-loops-make-strong-software-teams\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/genessa-panainte-184324-unsplash_crop-5ac744a1b50d0a1f9e193b7397be60de.jpg'/></p><p><em>Enhance overall code quality through a blend of interpersonal communication and tool-based analysis.</em></p><p>Software quality takes time. And good quality products come from properly working feedback loops. Timely feedback can mean clarity over confusion; a validation of assumptions can mean shorter development cycles.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>For example, let’s say you have a project that needs to be delivered next month, but you and your development team know it will take at least two more months to complete. How do you communicate this to key stakeholders?</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>First off, you need to establish a shared understanding of goals and quality amongst all involved participants. As a developer, you tend to base your behavior and build products and architectures around values and assumptions. If these values and assumptions are not aligned and validated, you will never end up with what you intended—let alone on time and within budget. Assuming your assumptions are accurate, you get carried away and spend way too much time on something before gathering feedback. But honestly, when would you rather hear all of your effort was a waste: after you spent a day working on it, or after working on it for a week?</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>A feedback loop is straightforward: it uses its input as one of its inputs. In its simplest form, a developer changes a code base and then gets feedback from the system by unit testing. This feedback will now be input for the developer’s next steps to improve the code. However, reality is not that simple. Plus, humans have an irrepressible tendency to include as many people as possible in one loop.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>If you follow such a course, you’ll end up with feedback chaos: massive “loops” including every potential player make it impossible to control, validate assumptions, and create a shared sense of reality. Quite simply, there’s too much going on. But there’s a solution: reflection. Reflection helps you identify existing feedback loops and determine who needs to be included. The shorter the feedback loop, the better.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There are two forms of feedback: personal and tool based. Personal feedback is given on an interpersonal level—people discussing code, products, or processes and identifying where things can be improved. Tool-based feedback, such as static analysis, provides you with code-level feedback and tells you where to improve your code (or specific parts of your code) to increase quality. Personal feedback is often specific for projects, more sensitive to context, and offers concrete suggestions to implement. Tool-based feedback enables faster feedback loops, allows for scalability by iteration, and is more objective. But which form of feedback is better?</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>There is a false dichotomy between full automation and human intervention. Successful quality control combines tool-based measurement with manual review and discussion. At the end of the day, the most effective feedback loops are a mixture of daily best practices, automation, tools, and human intervention.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>In an upcoming follow-up post, I’ll discuss specific practices that integrate personal and tool-based feedback. These practices will help you bolster your code and architectural quality.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between O'Reilly and SIG. </em><a href=\"http://www.oreilly.com/about/editorial_independence.html\"><em>See our statement of editorial independence</em></a><em>.</em></p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/strong-feedback-loops-make-strong-software-teams'>Strong feedback loops make strong software teams.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/-us1qlRyN5w\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 10 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/GDZWkeB2yIE/four-short-links-10-april-2018\n",
      "<p><em>Deep Learning Learnings, Reverse Engineering WhatsApp, Database Client, and Social Science</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"http://amid.fish/reproducing-deep-rl\">Lessons Learned Reproducing a Deep Reinforcement Learning Paper</a> -- REALLY good retrospective on eight months reproducing a paper, with lots of lessons learned, like <i>starting a reinforcement learning project, you should expect to get stuck like you get stuck on a math problem. It’s not like my experience of programming in general so far where you get stuck but there’s usually a clear trail to follow and you can get unstuck within a couple of days at most. It’s more like when you’re trying to solve a puzzle, there are no clear inroads into the problem, and the only way to proceed is to try things until you find the key piece of evidence or get the key spark that lets you figure it out.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/sigalor/whatsapp-web-reveng/blob/master/README.md\">Reverse Engineering WhatsApp</a> -- <i>This project intends to provide a complete description and re-implementation of the WhatsApp Web API, which will eventually lead to a custom client. WhatsApp Web internally works using WebSockets; this project does as well.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://databaseflow.com/\">DatabaseFlow</a> -- <i>an open source self-hosted SQL client, GraphQL server, and charting application that works with your database. Visualize schemas, query plans, charts, and results. You can run Database Flow locally for your own use, or install to a shared server for the whole team.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://web.stanford.edu/~gentzkow/research/CodeAndData.xhtml\">Code and Data for the Social Sciences</a> -- <i>This handbook is about translating insights from experts in code and data into practical terms for empirical social scientists.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2537?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Data Science &amp; Machine Learning\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-10-april-2018'>Four short links: 10 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/GDZWkeB2yIE\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 9 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/XPqSjjkHGQE/four-short-links-9-april-2018\n",
      "<p><em>Monads, GDPR, Blockchain, and Search</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"http://programming-journal.org/2018/2/12/\">What We Talk About When We Talk About Monads</a> -- <i>This paper is not a monad tutorial. It will not tell you what a monad is. Instead, it helps you understand how computer scientists and programmers talk about monads and why they do so.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://baekdal.com/strategy/publishers-havent-realized-just-how-big-a-deal-gdpr-is/\">Publishers and GDPR</a> -- a nice explanation of what GDPR is bringing to companies like Facebook and Google, how it's changing ad-serving, and what it means for content publishers.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/@kaistinchcombe/decentralized-and-trustless-crypto-paradise-is-actually-a-medieval-hellhole-c1ca122efdec\">Blockchain is Not Only Crappy Technology But a Bad Vision for the Future</a> -- <i>There is no single person in existence who had a problem they wanted to solve, discovered that an available blockchain solution was the best way to solve it, and therefore became a blockchain enthusiast.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://typesense.org/\">Typesense</a> -- <a href=\"https://github.com/typesense/typesense\">open source</a> <i>typo tolerant search engine that delivers fast and relevant results out of the box.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "\r\n",
      "<aside data-type=\"sidebar\" id=\"id-RDYSa\">\r\n",
      "<p><em><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2544?intcmp=il-data-confreg-lp-steu18_new_site_4sl_cta\">Check out the \"Law, ethics, and governance\" sessions</a> at the Strata Data Conference in London, May 21-24, 2018.</em></p>\r\n",
      "</aside>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-9-april-2018'>Four short links: 9 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/XPqSjjkHGQE\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 6 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/mWtbID3c7lM/four-short-links-6-april-2018\n",
      "<p><em>Library Management, Flame Graphs, Silent Speech Interface, and Cloud Backup</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"http://bit.ly/2IvYBMg\">Thou Shalt Not Depend on Me</a> (ACM) -- <i>with 37% of websites using at least one known vulnerable library, and libraries often being included in quite unexpected ways, there clearly is room for improvement in library handling on the web.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/Netflix/flamescope\">FlameScope</a> -- Netflix's open source <i>visualization tool for exploring different time ranges as Flame Graphs</i>. (via <a href=\"https://medium.com/netflix-techblog/netflix-flamescope-a57ca19d47bb\">Netflix Tech Blog</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://dam-prod.media.mit.edu/x/2018/03/23/p43-kapur_BRjFwE6.pdf\">AlterEgo: A Personalized Wearable Silent Speech Interface</a> -- <i>The results from our preliminary experiments show that the accuracy of our silent speech system is at par with the reported word accuracies of state-of-the-art speech recognition systems, in terms of being robust enough to be deployed as voice interfaces, albeit on smaller vocabulary sets.</i> (via <a href=\"http://news.mit.edu/2018/computer-system-transcribes-words-users-speak-silently-0404\">MIT News</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://duplicity.nongnu.org/\">Duplicity</a> -- <i>Encrypted bandwidth-efficient backup using the rsync algorithm</i>. Common use case is backing up server to S3, but there's an impressive number of connective services, including Google Drive, Azure, Mega.co, and Dropbox.</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-6-april-2018'>Four short links: 6 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/mWtbID3c7lM\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "It's time to usher in a new era of UX curation\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xdlpSXINxnI/its-time-to-usher-in-a-new-era-of-ux-curation\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/background-626713_1920_crop-5a7aef72433bd7ef9a2f263343e68927.jpg'/></p><p><em>Empowering groups to understand design for themselves will lead to new user experiences for users and designers alike.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/its-time-to-usher-in-a-new-era-of-ux-curation'>It's time to usher in a new era of UX curation.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/xdlpSXINxnI\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Kyle Simpson and Tammy Everts on the challenges of the modern web\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vk3mrYECK3E/kyle-simpson-and-tammy-everts-on-the-challenges-of-the-modern-web\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/chaos-724096_1920_crop-f551d141e7d90ce73294cedb82e41fb9.jpg'/></p><p><em>The O’Reilly Programming Podcast: Rising barriers to entry, the complexity of the modern web, and a preview of upcoming Fluent sessions.</em></p><p>In this episode of the <a href=\"https://www.oreilly.com/topics/oreilly-programming-podcast\">O’Reilly Programming Podcast</a>, I talk with two of the program chairs for the upcoming <a href=\"https://conferences.oreilly.com/fluent/fl-ca?intcmp=il-web-confreg-lp-flca18_new_site_programming_podcast_simpson_everts_body_text_link\">O’Reilly Fluent Conference</a> (July 11-14 in San Jose), <a href=\"https://twitter.com/getify\">Kyle Simpson</a> and <a href=\"https://twitter.com/tameverts\">Tammy Everts</a>. Simpson is co-author of the <a href=\"https://www.safaribooksonline.com/library/view/html5-cookbook/9781449318444/?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=programming_podcast_simpson_everts_body_text_link\"><em>HTML 5 Cookbook</em></a>, and the author of the <a href=\"https://www.amazon.com/You-Dont-Know-Js-Book/dp/B01AY9P0P6\"><em>You Don’t Know JS</em></a> series of books. Everts is the chief experience officer at <a href=\"https://speedcurve.com/\">SpeedCurve</a> and the author of <a href=\"https://www.safaribooksonline.com/library/view/time-is-money/9781491928783/?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=programming_podcast_simpson_everts_body_text_link\"><em>Time is Money: The Business Value of Web Performance</em></a>.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/kyle-simpson-and-tammy-everts-on-the-challenges-of-the-modern-web'>Kyle Simpson and Tammy Everts on the challenges of the modern web.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/vk3mrYECK3E\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 5 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0VI5ueh83o4/four-short-links-5-april-2018\n",
      "<p><em>Interactive Notebooks, Molecule-making AI, Interpersonal Dynamics, and Javascript Motion Library</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://mybinder.org/\">MyBinder</a> -- <i>Turn a GitHub repo into a collection of interactive notebooks</i>. (via <a href=\"https://jvns.ca/blog/2017/11/12/binder--an-awesome-tool-for-hosting-jupyter-notebooks/\">Julia Evans</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.nature.com/articles/d41586-018-03977-w\">Molecule-Making AI</a> (Nature) -- <i>The new AI tool, developed by Marwin Segler, an organic chemist and artificial intelligence researcher at the University of Münster in Germany, and his colleagues, uses deep learning neural networks to imbibe essentially all known single-step organic-chemistry reactions—about 12.4 million of them. This enables it to predict the chemical reactions that can be used in any single step. The tool repeatedly applies these neural networks in planning a multi-step synthesis, deconstructing the desired molecule until it ends up with the available starting reagents.</i> (via <a href=\"https://science.slashdot.org/story/18/03/30/1927243/ai-tool-which-has-digested-nearly-every-reaction-ever-performed-can-invent-new-ways-to-create-complex-molecules\">Slashdot</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://firstround.com/review/clean-up-corrosive-interpersonal-dynamics-on-your-team-with-this-system/\">Interpersonal Dynamics</a> -- The list of common corrosive dynamics rang true: bone-deep competition; fear of being found out; my reality is not the reality; it's no fun being the squeaky wheel; feedback stays at the surface; denial that work is personal.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://popmotion.io/\">Popmotion</a> -- <i>A functional, flexible JavaScript motion library.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-5-april-2018'>Four short links: 5 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/0VI5ueh83o4\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "5 tips for architecting fast data applications\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FAy1Wwr46Sw/5-tips-for-architecting-fast-data-applications\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/scaffolding-1886681_1920_crop-2e8eddeea3193148819fd525f0db37d7.jpg'/></p><p><em>Considerations for setting the architectural foundations for a fast data platform.</em></p><p>We live in the era of the connected experience, where our daily interactions with the world can be digitized, collected, processed, and analyzed to generate valuable insights.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Back in the days of Web 1.0, Google founders figured out smart ways to rank websites by <a href=\"http://ilpubs.stanford.edu:8090/422/\">analyzing their connection patterns</a> and using that information to improve the relevance of search results. Google was among the pioneers that created “web scale” architectures to analyze the massive data sets that resulted from “crawling” the web that gave birth to Apache Hadoop, MapReduce, and NoSQL databases. Those were the days when “connected” meant having some web presence, “interactions” were measured in number of clicks, and the analysis happened in batch overnight processes.</p>\r\n",
      "\r\n",
      "<p>Fast forward to the present day and we find ourselves in a world where the number of connected devices is constantly increasing. These devices not only respond to our commands, but are also able to autonomously interact with each other. Each of these interactions generates data that collectively amount to high-volume data streams. Accumulating all this data to process overnight is not an option anymore. First, we want to generate actionable insights as fast as possible, and second, one night might not be long enough to process all the data collected the previous day. At the same time, our expectations as users have also evolved to the point where we demand that applications deliver personalized user experiences in near real time.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>To remain competitive in a market that demands real-time responses to these digital pulses, organizations are adopting fast data applications as key assets in their technology portfolio. There are many challenges that need to be addressed to create the right architecture to support the range of fast data applications that your enterprise needs.</p>\r\n",
      "\r\n",
      "<p>Here are five considerations every software architect and developer needs to take into account when setting the architectural foundations for a fast data platform.</p>\r\n",
      "\r\n",
      "<h2>1. Determine requirements first</h2>\r\n",
      "\r\n",
      "<p>Although this seems the obvious starting point of every software architecture, there are specific considerations to observe when we define the set of requirements for a software platform to support fast data applications.</p>\r\n",
      "\r\n",
      "<p>Data in motion can be tricky to characterize, as there are usually probabilistic factors involved in the generation, transmission, collection, and processing of messages.</p>\r\n",
      "\r\n",
      "<p>These are some of the questions we need answered in order to help us drive the architecture:</p>\r\n",
      "\r\n",
      "<h3>General data shape</h3>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>How large is each message?</li>\r\n",
      "\t<li>How many messages per time unit do we expect?</li>\r\n",
      "\t<li>Do we expect large changes in the frequency of message delivery? Are there peak hours? Are there “Black Friday” events in our business?</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<h3>Output expectations</h3>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>How fast do we need a result?</li>\r\n",
      "\t<li>Do we need to process each record individually? Or can we process them in small collections (micro-batch)</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<h3>Process tolerance</h3>\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>How “dirty” is the data? What do we do with “dirty” data? Drop it? Report it? Clean and reprocess it?</li>\r\n",
      "\t<li>Do I need to preserve ordering? Are there inherent time relationships in the messages that need to be preserved as they travel across the system?</li>\r\n",
      "\t<li>What message process warranty level do we require? At least once? At most once? Exactly once?</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "<p>The data shape will dictate capacity planning, tuning of the backbone, and scalability analysis for individual components.</p>\r\n",
      "\r\n",
      "<p>The output expectations will assist in the choice of processing engine while the process tolerance will add restrictions in terms of processing semantics and error handling.</p>\r\n",
      "\r\n",
      "<h2>2. Leverage the convergence of fast data and microservices</h2>\r\n",
      "\r\n",
      "<p>Fast data applications are, by nature, focused on a single task. They have a clear input and output definition, and often a schema as well. Wait. Are we describing fast data applications or microservices? There is a blurred line dividing the two, and data processing libraries such as <a href=\"https://doc.akka.io/docs/akka/2.5.5/scala/stream/index.html\">Akka Streams</a> and <a href=\"https://kafka.apache.org/documentation/streams/\">Kafka Streams</a> make that line blur even more, as we can use these libraries to embed data processing capabilities in our microservices.</p>\r\n",
      "\r\n",
      "<p>We can think of combinations of data-processing applications with microservices to deliver specific features and insights from a data stream. For example, we can combine a machine learning job for anomaly detection with a dashboard that summarizes the findings to facilitate further investigation.</p>\r\n",
      "\r\n",
      "<p>From a project perspective, creating small, self-contained, data-driven applications that meld streaming data and microservices together is a good practice to break down large problems and projects into approachable chunks, reduce risk, and deliver value faster.</p>\r\n",
      "\r\n",
      "<h2>3. Get the message across</h2>\r\n",
      "\r\n",
      "<p>We discussed how fast data applications and microservices converge on the conceptual and executional levels. Another element they have in common is that they are both consuming and producing messages. A message-oriented implementation requires an efficient messaging backbone that facilitates the exchange of data in a reliable and secure way with the lowest latency possible.</p>\r\n",
      "\r\n",
      "<p><a href=\"https://kafka.apache.org/\">Apache Kafka</a> is currently the leading project in this area. It delivers a publish/subscribe model backed by a distributed log implementation that provides durability, resilience, fault tolerance, and the ability to replay messages by different consumers. The multi-subscriber approach creates the opportunity to reuse a single data stream for multiple consuming applications.</p>\r\n",
      "\r\n",
      "<h2>4. Leverage your SQL knowledge</h2>\r\n",
      "\r\n",
      "<p>We usually relate SQL to querying tables in relational databases. At first, it might seem odd to issue an SQL query on a stream of data. But what is a table? It’s a collection of records that were added, updated or deleted over time. We can see a table as an consolidated view of a stream of events over time. Likewise, we can create a stream from the observable changes applied to a table, reported as events. As Tyler Akiadu, from Google, explained in his Strata NY 2017 presentation, “<a href=\"https://s.apache.org/streaming-sql-strata-nyc\">Foundations of streaming SQL</a>”: “Streams are the in-motion form of data, both bounded and unbounded.” He goes further to explain how the relational algebra behind SQL can be applied to streams of data when we add time into the algebra in what he calls “time-varying relations.”</p>\r\n",
      "\r\n",
      "<p>In 2016, Apache Spark <a href=\"https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\">introduced Structured Streaming</a>, a new streaming engine based on the SparkSQL abstractions and runtime optimizations. In the same year, Apache Flink <a href=\"https://flink.apache.org/news/2016/05/24/stream-sql.html\">announced streaming SQL support</a>. More recently, Apache Kafka also <a href=\"https://www.confluent.io/blog/ksql-open-source-streaming-sql-for-apache-kafka/\">introduced the KSQL query engine</a>, adding streaming query capabilities to the popular event back end.</p>\r\n",
      "\r\n",
      "<p>The adoption of fast data technologies is on a steep rise. The low-level streaming implementations of the mentioned engines require specialized knowledge in order to program new applications.</p>\r\n",
      "\r\n",
      "<p>The availability of SQL enables a wider range of professionals to participate in the development of streaming data analytics pipelines, alleviating the skill shortage in the market and helping organizations to repurpose their workforces as they evolve in their fast data adoption.</p>\r\n",
      "\r\n",
      "<h2>5. Build on the shoulders of giants</h2>\r\n",
      "\r\n",
      "<p>As we mentioned at the beginning, we expect fast data applications to work reliably, continuously, and deliver results almost in near real time. These requirements impose strong scalability and resilience implications.</p>\r\n",
      "\r\n",
      "<p>Developing standalone applications that fulfill those requirements would be prohibitively expensive, as it would require specialized knowledge of distributed systems, operating systems, and networks, requiring large development and testing efforts to cover the complexity that distributed applications present. Instead, we build those applications on data-oriented frameworks, like <a href=\"https://spark.apache.org/\">Apache Spark</a> and <a href=\"https://flink.apache.org/\">Apache Flink</a>, or we resort to libraries that we can embed in our services, such as <a href=\"https://kafka.apache.org/documentation/streams/\">Kafka Streams</a> and <a href=\"https://doc.akka.io/docs/akka/2.5.5/scala/stream/index.html\">Akka Streams</a>. These data-oriented stacks implement the low-level complexity and take care of the resilience of the application execution. In turn, they offer a high-level abstraction to enable developers to focus on delivering business value.</p>\r\n",
      "\r\n",
      "<p>To run our applications, we require computing system resources like CPU, memory, disk, and network bandwidth to be allocated to the critical data services that power the applications. When we work on a single machine, the operating system takes care of managing the resources allocated to applications. But when we run on a cluster of machines, how can we perform the resource management required by this new generation of distributed data-intensive applications?</p>\r\n",
      "\r\n",
      "<p>Cluster managers, such as <a href=\"http://mesos.apache.org/\">Apache Mesos</a>, are an abstraction that runs on top of any computing infrastructure (public/private cloud, VM, bare metal) to provide a single unified resource pool across multiple infrastructures. Mesos achieves that unification by aggregating the infrastructure resources, and then offering resources slices, like <em>x</em> CPUs, <em>y</em> MB RAM, and <em>z</em> GB disk, to applications. Applications are then able to accept or reject those resources based on their own needs. Mesos can provide resources to execute applications and data services such as Apache Kafka, Apache Spark, and HDFS, or container schedulers such as Kubernetes.</p>\r\n",
      "\r\n",
      "<p>Deploying a cluster management solution like Mesosphere <a href=\"https://dcos.io/\">DC/OS</a> helps us take advantage of Mesos to deliver a complete fast-data platform by adding the deployment of standard components, providing a runtime for applications and delivering foundational services such as security and user management. It enables unbounded scalability as more commodity or specialized hardware can be seamlessly added to existing clusters.</p>\r\n",
      "\r\n",
      "<p>This results in increased enterprise agility as resources can be dynamically redirected to support the varying demands of different applications.</p>\r\n",
      "\r\n",
      "<h2>Conclusion</h2>\r\n",
      "\r\n",
      "<p>Fast data applications are becoming a key asset for enterprises to adopt as they develop competitive advantages in a world where actionable insights need to be produced and consumed in real time.</p>\r\n",
      "\r\n",
      "<p>Building fast data architectures that deliver scalable and resilient real-time applications is a challenging undertaking. The five recommendations that we have collected in this post should help you in your journey from requirements capture to cluster-wide deployment.</p>\r\n",
      "\r\n",
      "<p>A successful implementation of the fast data architecture will give your business the ability to accelerate its data-driven innovation by creating an environment to dynamically create, deploy, and operate end-to-end data-intensive applications. In turn, you will gain increased competitive advantage and the agility to react to your specific market challenges.</p>\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between O'Reilly and Mesosphere. <a href=\"http://www.oreilly.com/about/editorial_independence.html\">See our statement of editorial independence</a>.</em></p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/5-tips-for-architecting-fast-data-applications'>5 tips for architecting fast data applications.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/FAy1Wwr46Sw\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "What becomes of the broken hearted? Blueprint of a donor-free world using custom heart technologies\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jO-w4_kQSdc/what-becomes-of-the-broken-hearted-blueprint-of-a-donor-free-world-using-custom-heart-technologies\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/14777611401_b807ac2e41_k_crop-c5a8c38ca231f9d9ef6ea038dfca18d1.jpg'/></p><p><em>Advances in 3D-printing technology have the potential to lower the cost and increase the availability of organ transplants.</em></p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Imagine feeling like you ran a marathon when you’re actually just getting off the couch.  Imagine the extreme anxiety you might experience from living with bouts of dizziness, chest pain, and accelerated heartbeat until a doctor explains to you that these symptoms are not “nothing”, and that in fact, you have cardiomyopathy. This condition could lead to heart failure and eventually a heart transplant, but this desperately needed organ may not be available in time.</p>\r\n",
      "\r\n",
      "<p><a href=\"https://optn.transplant.hrsa.gov/\">Organ transplants</a> are in high demand in the United States. The heart is the third most requested organ, with 4,000 candidates on the waitlist and over 2,000 heart transplant surgeries performed in 2017.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/what-becomes-of-the-broken-hearted-blueprint-of-a-donor-free-world-using-custom-heart-technologies'>What becomes of the broken hearted? Blueprint of a donor-free world using custom heart technologies.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/jO-w4_kQSdc\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 4 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Yjt_S8HBnTg/four-short-links-4-april-2018\n",
      "<p><em>Forum Software, Data Analytics, Datalog Query, and Online != High-Tech</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://spectrum.chat/\">Spectrum</a> -- <a href=\"https://github.com/withspectrum/spectrum\">open source</a> forum software. (via <a href=\"https://spectrum.chat/thread/556b4915-7269-46a7-96e6-f38446d14146\">announcement</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://macrobase.stanford.edu/\">MacroBase</a> -- <i>a data analytics tool that prioritizes attention in large data sets using machine learning [...] specialized for one task: finding and explaining unusual or interesting trends in data.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/replikativ/datahike\">datahike</a> -- <i>a durable database with an efficient datalog query engine.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.curbed.com/2018/3/28/17164898/bed-in-a-box-online-mattress-brands-why-so-many\">Why So Many Online Mattress Brands</a> -- trigger for a rant: software is eating everything, but that doesn't make everything an innovative company. If you're applying the online sales playbook to product X (kombucha, mattresses, yoga mats) it doesn't make you a Level 9 game-changing disruptive TechCo, it makes you a retail business keeping up with the times. I'm curious where the next <em>interesting</em> bits of tech are—<a href=\"http://twitter.com/gnat\">@gnat</a> me with your ideas.</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-4-april-2018'>Four short links: 4 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/Yjt_S8HBnTg\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "100+ new live online trainings just launched on O'Reilly's learning platform\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lOpaYkY_QT8/100-plus-new-live-online-trainings-just-launched-on-oreillys-learning-platform\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/training-computer-phone-notebook-crop-d8012372a283a27734da3a42c19befad.jpg'/></p><p><em>Get hands-on training in AWS, Python, Java, blockchain, management, and many other topics.</em></p><p>Develop and refine your skills with <a href=\"https://www.safaribooksonline.com/live-training/?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">100+ new live online trainings</a> we opened up for April and May on our learning platform.</p>\r\n",
      "\r\n",
      "<p>Space is limited and these trainings often fill up.</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/creating-serverless-apis-with-aws-lambda-and-api-gateway/0636920170525?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Creating Serverless APIs with AWS Lambda and API Gateway</a></em>, April 6</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/getting-started-with-amazon-web-services-aws/0636920160175?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Getting Started with Amazon Web Services (AWS)</a></em>, April 19-20</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/python-data-handling-a-deeper-dive/0636920170143?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Python Data Handling: A Deeper Dive</a></em>, April 20</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/how-product-management-leads-change-in-the-enterprise/0636920170563?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">How Product Management Leads Change in the Enterprise</a></em>, April 23</p>\r\n",
      "\r\n",
      "<p><em><a class=\"link\" href=\"https://www.safaribooksonline.com/live-training/courses/beyond-python-scripts-logging-modules-and-dependency-management/0636920170778?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Beyond Python Scripts: Logging, Modules, and Dependency Management</a></em>, April 23</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/beyond-python-scripts-exceptions-error-handling-and-command-line-interfaces/0636920170747?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Beyond Python Scripts: Exceptions, Error Handling, and Command-Line Interfaces</a></em>, April 24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/getting-started-with-go/0636920170464?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Getting Started with Go</a></em>, April 24-25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/end-to-end-data-science-workflows-in-jupyter-notebooks/0636920171317?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">End-to-End Data Science Workflows in Jupyter Notebooks</a></em>, April 27</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/getting-started-with-vue-js/0636920170297?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Getting Started with Vue.js</a></em>, April 30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/java-r-full-throttle-with-paul-deitel-a-one-day-code-intensive-java-standard-edition-presentation/0636920162575?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Java Full Throttle with Paul Deitel: A One-Day, Code-Intensive Java Standard Edition Presentation</a></em>, April 30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/building-a-cloud-roadmap/0636920161578?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Building a Cloud Roadmap</a></em>, May 1</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/git-fundamentals/0636920166399?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Git Fundamentals</a></em>, May 1-2</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/aws-certified-sysops-administrator-associate-crash-course/0636920162605?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">AWS Certified SysOps Administrator (Associate) Crash Course </a></em>, May 1-2</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/oca-java-se-8-programmer-certification-crash-course/0636920169949?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">OCA Java SE 8 Programmer Certification Crash Course</a></em>, May 1-3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/getting-started-with-devops-in-90-minutes/0636920143819?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Getting Started with DevOps in 90 Minutes</a></em>, May 2</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/learn-the-basics-of-scala-in-3-hours/0636920155843?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Learn the Basics of Scala in 3 hours</a></em>, May 2</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/ipv4-subnetting/0636920157830?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">IPv4 Subnetting</a></em>, May 2-3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/sql-fundamentals-for-data/0636920109051?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">SQL Fundamentals for Data</a></em>, May 2-3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/safe-4-5-scaled-agile-framework-foundations/0636920172963?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">SAFe 4.5 (Scaled Agile Framework) Foundations</a></em>, May 3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/managing-team-conflict/0636920174752?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Managing Team Conflict</a></em>, May 3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/hands-on-machine-learning-with-python-clustering-dimension-reduction-and-time-series-analysis/0636920175261?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Hands-On Machine Learning with Python: Clustering, Dimension Reduction, and Time Series Analysis</a></em>, May 3</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/google-cloud-platform-professional-cloud-architect-certification-crash-course/0636920170129?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Google Cloud Platform Professional Cloud Architect Certification Crash Course</a></em>, May 3-4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/cyber-security-fundamentals/0636920160717?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Cyber Security Fundamentals</a></em>, May 3-4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/advanced-agile-scaling-in-the-enterprise/0636920140733?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Advanced Agile: Scaling in the Enterprise</a></em>, May 4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/network-troubleshooting-using-the-half-split-and-ooda/0636920158752?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Network Troubleshooting Using the Half Split and OODA</a></em>, May 4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/software-architecture-for-developers/0636920174288?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Software Architecture for Developers</a></em>, May 4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/hands-on-machine-learning-with-python-classification-and-regression/0636920175315?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Hands-On Machine Learning with Python: Classification and Regression</a></em>, May 4</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/building-and-managing-kubernetes-applications/0636920155560?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Building and Managing Kubernetes Applications</a></em>, May 7</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introducing-blockchain/0636920171638?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introducing Blockchain</a></em>, May 7</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/get-started-with-natural-language-processing-in-python/0636920149248?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Get Started with NLP</a></em>, May 7</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-digital-forensics-and-incident-response-dfir/0636920174561?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Digital Forensics and Incident Response (DFIR)</a></em>, May 7</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/essential-machine-learning-and-exploratory-data-analysis-with-python-and-jupyter-notebook/0636920162902?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Essential Machine Learning and Exploratory Data Analysis with Python and Jupyter Notebooks</a></em>, May 7-8</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/building-a-deployment-pipeline-with-jenkins-2/0636920175803?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Building Deployment Pipelines with Jenkins 2</a></em>, May 7 and 9</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-apache-spark-2-x/0636920171461?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Apache Spark 2.x</a></em>, May 7-9</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/deep-learning-fundamentals/0636920160052?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Deep Learning Fundamentals</a></em>, May 8</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/acing-the-ccna-exam/0636920147626?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Acing the CCNA Exam</a></em>, May 8</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/emotional-intelligence-for-managers/0636920169611?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Emotional Intelligence for Managers</a></em>, May 8</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/scala-core-programming-methods-classes-and-traits/0636920175216?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Scala Core Programming: Methods, Classes, and Traits</a></em>, May 8</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/design-patterns-boot-camp/0636920175001?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Design Patterns Boot Camp</a></em>, May 8-9</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-lean/0636920167129?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Lean</a></em>, May 9</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/beginners-guide-to-creating-prototypes-in-sketch/0636920110613?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Beginner’s Guide to Creating Prototypes in Sketch</a></em>, May 9</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/aws-certified-solutions-architect-associate-crash-course/0636920170655?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">AWS Certified Solutions Architect Associate Crash Course</a></em>, May 9-10</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/cloud-native-architecture-patterns/0636920111337?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Cloud Native Architecture Patterns</a></em>, May 9-10</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/amazon-web-services-architect-associate-certification-aws-core-architecture-concepts/0636920167945?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Amazon Web Services: Architect Associate Certification - AWS Core Architecture Concepts</a></em>, May 9-11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/blockchain-applications-and-smart-contracts/0636920163541?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Blockchain Applications and Smart Contracts</a></em>, May 10</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/deep-reinforcement-learning/0636920158783?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Deep Reinforcement Learning</a></em>, May 10</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/getting-started-with-machine-learning/0636920174691?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Getting Started with Machine Learning</a></em>, May 10</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-ethical-hacking-and-penetration-testing/0636920160854?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Ethical Hacking and Penetration Testing</a></em>, May 10-11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/explore-visualize-and-predict-using-pandas-and-jupyter/0636920175353?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Explore, Visualize, and Predict using pandas and Jupyter</a></em>, May 10-11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/scalable-web-development-with-angular/0636920174332?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Scalable Web Development with Angular</a></em>, May 10-11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/apache-hadoop-spark-and-big-data-foundations/0636920171744?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Apache Hadoop, Spark, and Big Data Foundations</a></em>, May 11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/visualizing-software-architecture-with-the-c4-model/0636920174127?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Visualizing Software Architecture with the C4 Model</a></em>, May 11</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/write-your-first-hadoop-mapreduce-program/0636920152279?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Write Your First Hadoop MapReduce Program</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/write-your-first-spark-program-in-java/0636920152408?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Write Your First Spark Program in Java</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/interactive-java-with-java-9s-jshell-full-day/0636920172420?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Interactive Java with Java 9’s JShell</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/bash-shell-scripting-in-3-hours/0636920172536?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Bash Shell Scripting in 3 Hours</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/learn-linux-in-3-hours/0636920172581?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Learn Linux in 3 Hours</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/cybersecurity-blue-teams-vs-red-teams/0636920174585?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Cybersecurity Blue Teams vs. Red Teams</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/next-generation-java-testing-with-junit-5/0636920174714?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Next-Generation Java testing with JUnit 5</a></em>, May 14</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/product-management-in-practice/0636920156550?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Product Management in Practice</a></em>, May 14-15</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/iot-fundamentals/0636920172352?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">IoT Fundamentals</a></em>, May 14-15</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/porting-from-python-2-to-python-3/0636920146896?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Porting from Python 2 to Python 3</a></em>, May 15</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/red-hat-certified-system-administrator-rhcsa-crash-course/0636920164609?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Red Hat Certified System Administrator (RHCSA) Crash Course</a></em>, May 15-18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-analytics-for-product-managers/0636920106807?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Analytics for Product Managers</a></em>, May 16</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/architecture-without-an-end-state/0636920157359?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Architecture Without an End State</a></em>, May 16-17</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/deploying-container-based-microservices-on-aws/0636920170624?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Deploying Container-Based Microservices on AWS</a></em>, May 16-17</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/agile-for-everybody/0636920156673?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Agile for Everybody</a></em>, May 17</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-google-cloud-platform/0636920147046?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Google Cloud Platform</a></em>, May 17</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/practical-data-cleaning-with-python/0636920152866?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Practical Data Cleaning with Python</a></em>, May 17-18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/hands-on-introduction-to-apache-hadoop-and-spark-programming/0636920172468?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Hands-on Introduction to Apache Hadoop and Spark Programming</a></em>, May 17-18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/troubleshooting-agile/0636920140818?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Troubleshooting Agile</a></em>, May 18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/managing-your-manager/0636920167655?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Managing your Manager</a></em>, May 18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/building-chatbots-with-aws/0636920156949?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Building Chatbots with AWS</a></em>, May 18</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/your-first-30-days-as-a-manager/0636920160373?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Your First 30 Days as a Manager</a></em>, May 21</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-unreal-engine-4-with-blueprints/0636920167457?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Unreal Engine 4 with Blueprints</a></em>, May 21</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-critical-thinking/0636920152187?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Critical Thinking</a></em>, May 21</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/testing-and-validating-product-ideas-with-lean/0636920167303?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Testing and Validating Product Ideas with Lean</a></em>, May 21</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/from-developer-to-software-architect/0636920157533?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">From Developer to Software Architect</a></em>, May 22-23</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/cissp-crash-course/0636920160519?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">CISSP Crash Course</a></em>, May 22-23</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-kubernetes/0636920175063?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Kubernetes</a></em>, May 22</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/ccnp-r-s-route-300-101-crash-course/0636920133605?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">CCNP R/S ROUTE (300-101) Crash Course</a></em>, May 22-24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/advanced-sql-for-data-analysis-with-python-r-and-java/0636920109419?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Advanced SQL for Data Analysis (with Python, R, and Java)</a></em>, May 23</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/docker-beyond-the-basics-ci-cd/0636920125617?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Docker: Beyond the Basics (CI &amp; CD)</a></em>, May 23-24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-tensorflow/0636920139850?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to TensorFlow</a></em>, May 23-24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/leadership-communication-skills-for-managers/0636920164852?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Leadership Communication Skills for Managers</a></em>, May 24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/cyber-security-defense/0636920155478?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Cyber Security Defense</a></em>, May 24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/end-to-end-data-science-workflows-in-jupyter-notebooks/0636920171379?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">End-to-End Data Science Workflows in Jupyter Notebooks</a></em>, May 24</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/the-devops-toolkit/0636920170914?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">The DevOps Toolkit</a></em>, May 24-25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-cisco-next-generation-firewalls/0636920153894?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Cisco Next-Generation Firewalls</a></em>, May 24-25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/amazon-web-services-architect-associate-certification-aws-core-architecture-concepts/0636920168003?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Amazon Web Services: Architect Associate Certification - AWS Core Architecture Concepts</a></em>, May 24-25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/kubernetes-in-3-hours/0636920172628?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Kubernetes in 3 Hours</a></em>, May 25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/ansible-in-3-hours/0636920172673?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Ansible in 3 Hours</a></em>, May 25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/design-fundamentals-for-non-designers/0636920176732?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Design Fundamentals for Non-Designers</a></em>, May 25</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/python-data-handling-a-deeper-dive/0636920170181?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Python Data Handling - A Deeper Dive</a></em>, May 29</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/introduction-to-modularity-with-the-java-9-platform-module-system-jpms-full-day/0636920172727?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Introduction to Modularity with the Java 9 Platform Module System (JPMS)</a></em>, May 29</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/ccna-security-crash-course/0636920165590?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">CCNA Security Crash Course</a></em>, May 29-30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/scala-beyond-the-basics/0636920151388?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Scala: Beyond the Basics</a></em>, May 29-30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/microservices-architecture-and-design/0636920138587?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Microservices Architecture and Design</a></em>, May 29-30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/docker-up-and-running/0636920124788?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Docker: Up and Running</a></em>, May 29-30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/high-performance-tensorflow-in-production-hands-on-with-gpus-and-kubernetes/0636920142669?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">High Performance TensorFlow in Production: Hands on with GPUs and Kubernetes</a></em>, May 29-30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/rethinking-rest-a-hands-on-guide-to-graphql-and-queryable-apis/0636920174301?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Rethinking REST: A Hands-On Guide to GraphQL and Queryable APIs</a></em>, May 30</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/pmp-crash-course/0636920159414?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">PMP Crash Course</a></em>, May 31-June 1</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/test-driven-development-in-java/0636920172765?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Test Driven Development in Java</a></em>, May 31-June 1</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/architecture-without-an-end-state/0636920157427?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Architecture Without an End State</a></em>, May 31-June 1</p>\r\n",
      "\r\n",
      "<p><em><a href=\"https://www.safaribooksonline.com/live-training/courses/building-microservices-with-spring-boot-spring-cloud-and-cloud-foundry/0636920170341?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Building Microservices with Spring Boot, Spring Cloud, and Cloud Foundry</a></em>, July 2-3</p>\r\n",
      "\r\n",
      "<p><a href=\"https://www.safaribooksonline.com/live-training/?utm_source=oreilly&amp;utm_medium=newsite&amp;utm_campaign=040418-live-online-training-announcement\">Visit our learning platform</a> for more information on these and other live online trainings.</p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/100-plus-new-live-online-trainings-just-launched-on-oreillys-learning-platform'>100+ new live online trainings just launched on O'Reilly's learning platform.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/lOpaYkY_QT8\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "It's time to rebuild the web\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/POr_sH3BkzI/its-time-to-rebuild-the-web\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/wall-2997629_1920_crop-f1d80cccf2838b3d222e6e7ad3a6d59e.jpg'/></p><p><em>The web was never supposed to be a few walled gardens of concentrated content owned by a few major publishers; it was supposed to be a cacophony of different sites and voices.</em></p><p>Anil Dash's <a href=\"https://medium.com/@anildash/the-missing-building-blocks-of-the-web-3fa490ae5cbc\">\"The Missing Building Blocks of the Web\"</a> is an excellent article about the web as it was supposed to be, using technologies that exist but have been neglected or abandoned. It's not his first take on the <a href=\"http://anildash.com/2012/12/the-web-we-lost.html\">technologies the web has lost</a>, or on the possibility of <a href=\"http://anildash.com/2012/12/rebuilding-the-web-we-lost.html\">rebuilding the web</a>, and I hope it's not his last. And we have to ask ourselves what would happen if we brought back those technologies: would we have a web that's more humane and better suited to the future we want to build?</p>\r\n",
      "<p>I've written several times (and will no doubt write more) about rebuilding the internet, but I've generally assumed the rebuild will need peer-to-peer technologies. Those technologies are inherently much more complex than anything Dash proposes. While many of the technologies I'd use already exist, rebuilding the web around <a href=\"https://en.wikipedia.org/wiki/Blockchain\">blockchains</a> and <a href=\"https://en.wikipedia.org/wiki/Onion_routing\">onion routing</a> would require a revolution in user interface design to have a chance; otherwise it will be a playground for the technology elite. In contrast, Dash's \"missing building blocks\" are fundamentally simple. They can easily be used by people who don't have a unicorn's worth of experience as web developers and security administrators.</p>\r\n",
      "<p>Dash writes about the demise of the View Source browser feature, which dispays the HTML from which the web page is built. View Source isn't dead, but it's sick. He's right that the web succeeded, in part, because people with little background could look at the source for the pages they liked, copy the code they wanted, and end up with something that looks pretty good. Today, you can no longer learn by copying; while View Source still exists on most browsers, the complexity of modern web pages have made it next to useless. The bits you want are wrapped in megabytes (literally) of JavaScript and CSS.</p>\r\n",
      "<p>But that doesn't have to be the end of the story. HTML can be functional without being complex. Most of what I write (including this piece) goes into a first draft as very simple HTML, using only a half-dozen tags. Simple editors for basic web content still exist. Dash points out that Netscape Gold (the paid version of Netscape) had one, back in the day, and that there are many free editors for basic HTML. We'd have to talk ourselves out of the very complex formatting and layout that, after all, just gets in the way. Ask (almost) any designer: simplicity wins, not a drop-dead gorgeous page. We may have made View Source useless, but we haven't lost simplicity. And if we make enough simple sites, sites from which viewers can effectively copy useful code, View Source will become useful again, too. You can't become a web developer by viewing Facebook's source; but you might by looking at a new site that isn't weighed down by all that CSS and JavaScript.</p>\r\n",
      "<p>The web was never supposed to be a few walled gardens of concentrated content owned by Facebook, YouTube, Twitter, and a few other major publishers. It was supposed to be a cacophony of different sites and voices. And it would be easy to rebuild this cacophony—indeed, it never really died. There are plenty of individual sites out there still, and they provide some (should I say most?) of the really valuable content on the web. The problem with the megasites is that they select and present \"relevant\" content to us. Much as we may complain about Facebook, selecting relevant content from an ocean of random sites is an important service. It's easy for me to imagine relatives and friends building their own sites for baby pictures, announcements, and general talk. That's what we did in the 90s. But would we go to the trouble of reading those all those sites? Probably not. I didn't in the 90s, and neither did you.</p>\r\n",
      "<p>We already have a tool for solving this problem. <a href=\"https://en.wikipedia.org/wiki/RSS\">RSS</a> lets websites provide \"feeds\" of news and new items. Applications like <a href=\"https://feedly.com/\">Feedly</a> and <a href=\"http://reederapp.com/\">Reeder</a> let you build a collection of sites that interest you, and show you what's changed since the last time you visited. While I'd never check a dozen sites each day, I use Feedly to monitor hundreds of websites. I would never check those sites by hand, but I scan Feedly every morning. And, unlike Facebook, Feedly doesn't know anything about its users except for the sites they read.</p>\r\n",
      "<p>Feedly has a decent user interface, though it could be a improved; it would have to be better to become popular with people who aren't technically literate. (Sorry.) Still, though, the UI gap for RSS is much smaller than for technologies like <a href=\"https://tor.eff.org/\">TOR</a>. And if we're going to rebuild the net, we'll probably be better off choosing simple rather than bright, shiny, and complex technologies. Could someone build an RSS reader that made the web of independent sites as approachable as Facebook? I don't see why not—and users would have complete control over what they see. That's important; in a recent <a href=\"http://anildash.com/2012/12/the-web-we-lost.html\">tweet</a>, Dash says:</p>\r\n",
      "<blockquote>Google’s decision to kill Google Reader [their RSS client] was a turning point in enabling media to be manipulated by misinformation campaigns. The difference between individuals choosing the feeds they read and companies doing it for you affects all other forms of media.</blockquote>\r\n",
      "<p>Yes, there would still be plenty of sites for every conspiracy theory and propaganda project around; but in a world where you choose what you see rather than letting a third party decide for you, these sites would have trouble gaining momentum.</p>\r\n",
      "<p>I don't want to underestimate the difficulty of this project, or overestimate its chances of success. We'd certainly have to get used to sites that aren't as glossy or complex as the ones we have now. We might have to revisit some of the most hideous bits of the first-generation web, including those awful <a href=\"https://en.wikipedia.org/wiki/Yahoo!_GeoCities\">GeoCities</a> pages. We would probably need to avoid fancy, dynamic websites; and, before you think this will be easy, remember that one of the first extensions to the static web was CGI Perl. We would be taking the risk that we'd re-invent the same mistakes that brought us to our current mess. Simplicity is a discipline, and not an easy one. However, by losing tons of bloat, we'd end up with a web that is much faster and more responsive than what we have now. And maybe we'd learn to prize that speed and that responsiveness.</p>\r\n",
      "<p>We'd also need to avoid many of the privacy and security flaws that were rampant in the early internet, and for which we're still paying. That technical debt came due a long time ago. Paying off that debt may require some complex technology, and some significant UI engineering. All too often, solutions to security problems make things more difficult for both users and attackers. <a href=\"https://shift.newco.co/sure-the-internet-is-broken-so-lets-go-fix-it-shall-we-5c27e294c25c\">Crowdflare's new 1.1.1.1 service</a> addresses some basic problems with our DNS infrastructure and privacy, and their CEO proposes some more basic changes, like DNS over HTTPS. But even simple changes like this require non-technical users to change configuration settings that they don't understand. This is where we really need the help of UX designers. We can't afford to make \"safe\" difficult.</p>\r\n",
      "<p>And we'd have to admit that our current web, with all its flaws, evolved from these simple building blocks. To some extent, then, it's what we wanted—or, perhaps, what we deserved. It's certainly what we accepted, and begs the question: \"why wouldn't we accept the same thing again?\" Starting over means little if we're destined to repeat the mistakes we've already made. So, we would need to develop and incorporate technology for preventing abuse; we would need to build a public space that really is a public space, not someone else's private property; and above all, we would need to divest ourselves of the arrogance that assumes \"because we've built it, it is good.\" As Dash said six years ago, well before Facebook's Very Bad Month, we would need to \"take responsibility and accept blame.\"</p>\r\n",
      "<p>Regardless of how it happens, it's time to start thinking about rebuilding the web. That project is only likely to succeed if the rebuilt web is compatible with what we have today, including Facebook and YouTube. And it's only likely to succeed if it's simple enough for anyone to use. Anil Dash has outlined a way forward. It's not what I would have suggested, but it has a much higher chance of succeeding. Time to (re)build it.</p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/its-time-to-rebuild-the-web'>It's time to rebuild the web.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/POr_sH3BkzI\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "It’s time for data ethics conversations at your dinner table\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jPVnvLAOl3Q/its-time-for-data-ethics-conversations-at-your-dinner-table\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/23862250382_9aa41a059c_k_crop-a2d605ed99b1d2dd3894dba1202f386f.jpg'/></p><p><em>In an era where fake news travels faster than the truth, our communities are at a critical juncture.</em></p><p>With 2.5 quintillion records of data created every day, people are being defined by how they travel, surf the internet, eat, and live their lives. We are in the midst of a “data revolution,” where individuals and organizations can store and analyze massive amounts of information. Leveraging data can allow for surprising discoveries and innovations with the power to fundamentally alter society: from <a href=\"http://learningtocure.csail.mit.edu/\">applying machine learning to cancer research </a>to <a href=\"https://www.columbus.gov/smartcolumbus/home/\">harnessing data to create “smart” cities,</a> data science efforts are increasingly surfacing new insights—and new questions.</p>\r\n",
      "<p>Working with large databases, new analytical tools, and data-enabled methods promises to bring many benefits to society. However, “data-driven technologies also challenge the fundamental assumptions upon which our societies are built,” says <a href=\"http://sts.hks.harvard.edu/people/fellows/margarita-boenig-liptsin/\">Margo Boenig-Liptsin</a>, co-instructor of UC Berkeley’s “<a href=\"https://data.berkeley.edu/education/courses/hce\">Human Contexts and Ethics of Data</a>” course. Boenig-Liptsin notes, “In this time of rapid social and technological change, concepts like ‘privacy,’ ‘fairness,’ and ‘representation’ are reconstituted.” Indeed, bias in algorithms may favor some groups over others, as evidenced by notorious cases such as the <a href=\"https://www.ajlunited.org/the-coded-gaze\">finding by MIT Researcher Joy Buolamwini</a> that certain facial recognition software fails to work for those with dark skin tones. Moreover, lack of transparency and data misuse at ever-larger scales has prompted calls for greater scrutiny on behalf of more than <a href=\"https://www.nytimes.com/2018/03/18/us/cambridge-analytica-facebook-privacy-data.html\">50 million Facebook users</a>.</p>\r\n",
      "<p>In an era where <a href=\"http://science.sciencemag.org/content/359/6380/1146\">fake news travels faster than the truth</a>, our communities are at a critical juncture, and we need to be having difficult conversations about our individual and collective responsibility to handle data ethically. These conversations, and the principles and outcomes that emerge as a result, will benefit from being intentionally inclusive.</p>\r\n",
      "<p>What does responsible data sharing and use look like—for a data scientist, a parent, or a business? How are our socioeconomic structures and methods of interaction shaping behavior? How might we ensure that our technologies and practices are fair and unbiased?</p>\r\n",
      "<p>One idea that has gained traction is the need for a ‘Hippocratic Oath’ for data scientists. Just as medical professionals pledge to “do no harm,” individuals working with data should sign and abide by one or a set of <a href=\"https://medium.com/doteveryone/oaths-pledges-and-manifestos-a-master-list-of-ethical-tech-values-26e2672e161c\">pledges, manifestos, principles, or codes of conduct.</a> At Bloomberg’s <a href=\"https://www.bloomberg.com/company/d4gx/\">Data for Good Exchange (D4GX)</a> in New York City in September 2017, the company announced a partnership with <a href=\"http://datafordemocracy.org/projects/ethics.html\">Data for Democracy</a> and <a href=\"https://brighthive.io/\">BrightHive</a>to bring the data science community together to explore this very topic. More than 100 volunteers from universities, nonprofits, local and federal government agencies, and tech companies participated, drafting a set of guiding principles that could be adopted as a code of ethics. Notably, this is an ongoing and iterative process that must be community driven, respecting and recognizing the value of diverse thoughts and experiences.</p>\r\n",
      "<p>The group re-convened on February 6, 2018, at the inaugural D4GX event in San Francisco, again open to the public. Notable attendees included <a href=\"https://medium.com/@dpatil/a-code-of-ethics-for-data-science-cda27d1fac1\">DJ Patil</a>, who served as the chief data scientist of the United States from 2015-2017, Doug Cutting, co-creator of Hadoop and advocate for open source, as well as representation from the National Science Foundation-funded <a href=\"https://www.bigdatahubs.io/\">Regional Big Data Innovation Hubs and Spokes</a> program. At this event, participants reviewed more than 75 drafted ethics principles formulated by several working groups, with the goal of distilling a larger group of tenets into a streamlined set of principles for an ethics code.</p>\r\n",
      "<p>Efforts such as Bloomberg’s D4GX can be situated in a growing movement, with increased interest in the ethical aspects of technology, particularly related to advances in data science and artificial intelligence (AI) systems. For example, <a href=\"https://ainowinstitute.org/AI_Now_2017_Report.pdf\">AI Now 2017</a>, <a href=\"http://standards.ieee.org/develop/indconn/ec/ead_v1.pdf\">IEEE</a>, <a href=\"https://futureoflife.org/ai-principles/\">The Future of Life Institute</a>, <a href=\"https://metrolabnetwork.org/wp-content/uploads/2018/01/Ethical-Guidelines-for-Applying-Predictive-Tools-within-Human-Services-2017.pdf\">Metro Lab Network</a>, the <a href=\"https://ethics.acm.org/2018-code-draft-2/\">ACM</a>, and the <a href=\"https://www.cs.ox.ac.uk/efai/towards-a-code-of-ethics-for-artificial-intelligence/\">Oxford Internet Institute</a> have all issued reports on these topics. Plus, Microsoft’s Brad Smith and Harry Shum <a href=\"https://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society/\">published a book</a> entitled <a href=\"https://msblob.blob.core.windows.net/ncmedia/2018/02/The-Future-Computed_2.8.18.pdf\"><em>The Future Computed: Artificial intelligence and its role in society</em></a> earlier this year.</p>\r\n",
      "<p>A recent <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1741047\">National Science Foundation grant</a> focusing on the responsible use of big data was awarded to a group of researchers led by <a href=\"http://drexel.edu/cci/news-events/news/2017/December/drexel-cci-faculty-spotlight-julia-stoyanovich-phd-computer-science/\">Julia Stoyanovich</a>, an assistant professor at Drexel University, who has participated regularly in D4GX events in New York and San Francisco. The goal of this research project is to understand how legal and ethical norms can be embedded into technology, and to create tools that enable responsible collection, sharing, and analysis of data. These issues have also been a topic of discussion at multiple recent workshops. Last week, <a href=\"http://sites.nationalacademies.org/PGA/guirr/PGA_184431\">a workshop at the National Academy of Sciences</a> focused on ethics and data in the context of international research collaborations. Similarly, <a href=\"https://cra.org/ccc/events/fair-representations-fair-interactive-learning/\">another recent workshop on fairness in machine learning</a> aimed to identify key challenges and open questions that limit fairness, both in theory and in practice.</p>\r\n",
      "<p>As noted in the AI Now 2017 report, there are powerful incentives for the commercial sector to disregard these initiatives in favor of business as usual. It is not clear how compliance and accountability could be incentivized, monitored, or enforced in both the public and private sectors, <a href=\"https://www.eugdpr.org/\">although new European Union regulations pertaining to data privacy</a> will affect organizations globally beginning in May 2018. “Top-down” regulations as well as “grassroots” efforts are increasingly raising questions about how we might define fairness, combat bias, and create ethics guidelines in data science and AI.</p>\r\n",
      "<p>Yet, widespread adoption of ethical data collection and data analysis practices requires more than business penalties and awareness of these issues on the part of data science practitioners and the general public. Ultimately, data scientists and our broader community of data users must be equipped with the right tools and methodologies, and help each other leverage guidance effectively. Boenig-Liptsin notes, “We need to understand how our values shape our data tools and, reciprocally, how our data tools inform our values.” Successful efforts will require thoughtful and sustainable collaboration to apply insights and refine solutions.</p>\r\n",
      "<p>We are seeing an increasing number of <a href=\"https://amp.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election\">data practitioners</a> and <a href=\"https://www.npr.org/sections/thetwo-way/2018/03/13/593104213/ice-spokesman-quits-over-leaders-use-of-misleading-facts-to-discuss-calif-arrest\">leaders</a> stand up and speak about the questionable and often outright illegal collection, sharing, and use of sensitive data. For their voices to drive change, and for our society to truly harness the positive impacts of data innovation, while mitigating unintended consequences, we will need a collective effort. This effort needs to reach beyond academia and policymakers to anyone who can contribute—from both the public and private sectors.</p>\r\n",
      "<p>Our community needs to collectively voice expectations for responsible data use, bringing data practitioners together to examine existing research and evidence. By creating environments, <a href=\"https://boingboing.net/2017/11/20/robo-socrates-what-is-virtue.html\">curricula</a>, and tools that support community dialogue around ethics challenges, we can hope to translate findings into actionable <a href=\"https://datapractices.org/community-principles-on-ethical-data-sharing/\">principles</a>—and to hold each other accountable. In addition to working with regulatory bodies, the shaping of social norms can transform these principles into enforceable standards for the responsible use of data. As Barbara C. Jordan, a former member of the U.S. House of Representatives from Texas, and professor of ethics at the University of Texas at Austin, eloquently stated in 1976:</p>\r\n",
      "\r\n",
      "<blockquote>\r\n",
      "<p>There is no executive order; there is no law that can require the American people to form a national community. This we must do as individuals, and if we do it as individuals, there is no President of the United States who can veto that decision... We must define the 'common good' and begin again to shape a common future.</p>\r\n",
      "</blockquote>\r\n",
      "\r\n",
      "<p>Fully harnessing the data revolution requires that we not only explore what can be done with data, but also that we understand the broader impacts of how any individual or organization’s contribution affects others. We should be having these conversations early and often, bringing in a diverse range of perspectives. We should be having these conversations not just at academic conferences and in <a href=\"https://www.nytimes.com/2018/02/12/business/computer-science-ethics-courses.html\">tech and ethics courses</a>, but around dinner tables, everywhere.</p>\r\n",
      "  \r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/its-time-for-data-ethics-conversations-at-your-dinner-table'>It’s time for data ethics conversations at your dinner table.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/jPVnvLAOl3Q\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "How companies around the world apply machine learning\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hbg9aPjilzI/how-companies-around-the-world-apply-machine-learning\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/training_of_the_ats_crop-74510e6dd9721dcaa8d939621d261e3c.jpg'/></p><p><em>Strata Data London will introduce technologies and techniques; showcase use cases; and highlight the importance of ethics, privacy, and security.</em></p><p>The growing role of data and machine learning cuts across domains and industries. Companies continue to use data to improve decision-making (business intelligence and analytics) and for automation (machine learning and AI). At the Strata Data Conference in London, we’ve assembled a program that introduces technologies and techniques, showcases use cases across many industries, and highlights the importance of ethics, privacy, and security.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p>We are bringing back the <a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2546?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\">Strata Business Summit</a>, and this year, we have two days of executive briefings.</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2537?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Data Science and Machine Learning</em></a> sessions will cover tools, techniques, and case studies. This year, we have many sessions on managing and deploying models to production, and applications of deep learning in enterprise applications.</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p>This year’s sessions on <a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2538?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Data Engineering and Architecture</em></a> showcases streaming and real-time applications, along with the data platforms used at several leading companies.</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Privacy and security</h2>\r\n",
      "\r\n",
      "<p>The enforcement date for the General Data Protection Regulation (<a href=\"https://www.eugdpr.org/\">GDPR</a>) is the day after the end of the conference (May 25, 2018) and for the past few months, companies have been scrambling to learn this new set of regulations. We have <a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/64634?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\">a tutorial</a> and sessions to help companies learn how to comply with <a href=\"https://www.eugdpr.org/\">GDPR</a>. Implementing data security and privacy remain foundational, but one of the key changes advanced by GDPR—<a href=\"https://www.eugdpr.org/key-changes.html\">“privacy-by-design”</a>—will require companies to reassess how they design and architect products.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2636?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Security and Privacy</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2721?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Visualization, Design, and UX</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Unlocking popular data types: Text, temporal data, and graphs</h2>\r\n",
      "\r\n",
      "<p>The need for scaleout and streaming infrastructure can often be traced back to the importance of text, temporal data, and graphs. After one sets up infrastructure for collecting, storing, and querying these data types, the next step is to uncover interesting patterns or to use them to make predictions. Over the past year, companies have been turning to machine learning, in many cases to deep learning, when faced with large amounts of text, graphs, or temporal data. On the infrastructure side, we have sessions from members of some of the leading stream processing and storage communities.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2722?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Text and Natural Language</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2723?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Time-series and Graphs</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/topic/2539?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Stream Processing and Real-time Applications</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Data platforms</h2>\r\n",
      "\r\n",
      "<p>How do some of the best companies architect and develop data platforms that help accelerate innovation and digital transformation? In a series of sessions, companies will share their internal platforms for business intelligence and machine learning. These are battle-tested platforms used in production, some at extremely large scale. </p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Many of these data platforms encourage collaboration and sharing of data, features, and models. In addition, since we’re very much in an empirical era for machine learning, tools for running and reproducing experiments, and for exploring the space of algorithms, are essential. Security and privacy become even more critical in light of the upcoming enforcement date for <a href=\"https://www.eugdpr.org/\">GDPR</a>.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2724?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Data Platforms</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Machine learning: From data preparation and integration, to model deployment and management</h2>\r\n",
      "\r\n",
      "<p>Media articles on machine learning over emphasize algorithms and models. The reality is that model building is just one aspect of building products that rely on machine learning. In a vast majority of cases, machine learning applications require training data (<a href=\"https://www.oreilly.com/ideas/creating-large-training-data-sets-quickly\">“labeled data is the new, new oil”</a>). To that end, the first step is to bring together existing data sources and when appropriate, enrich them with other data sets. In most cases, data needs to be refined and prepared before it’s ready for analytic applications.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>While prototypes are easy to cook up, building production-grade applications requires serious engineering. Companies have come to realize that deploying, monitoring, and managing models in production requires different skills and a different mindset. So much so that a new breed of workers, <a href=\"https://www.oreilly.com/ideas/what-are-machine-learning-engineers\">machine learning engineers</a>, was recently forecasted to be the <a href=\"https://economicgraph.linkedin.com/research/LinkedIns-2017-US-Emerging-Jobs-Report\">fastest growing, emerging job in 2018</a>.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2725?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Managing and Deploying Machine Learning</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2726?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Data Integration and Data Pipelines</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<h2>Industry- and domain-specific tools, methods, and use cases</h2>\r\n",
      "\r\n",
      "<p>The best way to understand the potential of data technologies and machine learning is to see how companies are using them in real-world applications. One of the great things about Strata Data London is that one gets to hear case studies from many different industries and countries. You will learn how a variety of companies from across the globe have designed their data infrastructures; how they are incorporating machine learning; and how they’re approaching data privacy, security, and ethics. This year, we have keynotes and sessions on important topics in public policy, health care, manufacturing, and many other sectors of the economy. Here are some examples:</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<ul>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/full/data-case-studies?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Data Case Studies</em></a> (12 presentations)</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/full/findata?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Findata Day</em></a> and <a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2727?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Financial Services</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2728?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Media and Advertising</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2730?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>E-commerce</em></a>, and <a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2729?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Transportation and Logistics</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "\t<li>\r\n",
      "\t<p><a href=\"https://conferences.oreilly.com/strata/strata-eu/public/schedule/stopic/2731?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_link\"><em>Telecom</em></a> sessions</p>\r\n",
      "\t</li>\r\n",
      "</ul>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><a href=\"https://conferences.oreilly.com/strata/strata-eu?intcmp=il-data-confreg-lp-steu18_new_site_learn-how-companies-around-the-globe-put-machine-learning-to-work_body_end_cta\">The Strata Data Conference</a> is happening in London from May 21-24, 2018—best price ends April 6.</p>\r\n",
      "\r\n",
      "\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/how-companies-around-the-world-apply-machine-learning'>How companies around the world apply machine learning.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/hbg9aPjilzI\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 3 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/rtT2vajk28Y/four-short-links-3-april-2018\n",
      "<p><em>Internet of Battle Things, Program Fuzzing, Data Sheets for Data Sets, and Retro Port</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://arxiv.org/ftp/arxiv/papers/1803/1803.11256.pdf\">Challenges and Characteristics of Intelligent Autonomy for Internet of Battle Things in Highly Adversarial Environments</a> -- <i>Numerous artificially intelligent, networked things will populate the battlefield of the future, operating in close collaboration with human warfighters, and fighting as teams in highly adversarial environments. This paper explores the characteristics, capabilities, and intelligence required of such a network of intelligent things and humans—Internet of Battle Things (IOBT). It will experience unique challenges that are not yet well addressed by the current generation of AI and machine learning.</i> (via <a href=\"https://news.slashdot.org/story/18/04/02/2243228/military-documents-reveal-how-the-us-army-plans-to-deploy-ai-in-future-wars\">Slashdot</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://nebelwelt.net/publications/files/18Oakland.pdf\">T-Fuzz: Fuzzing by Program Transformation</a> -- clever! <i>To improve coverage, existing approaches rely on imprecise heuristics or complex input mutation techniques (e.g., symbolic execution or taint analysis) to bypass sanity checks. Our novel method tackles coverage from a different angle: by removing sanity checks in the target program. T-Fuzz leverages a coverage-guided fuzzer to generate inputs. Whenever the fuzzer can no longer trigger new code paths, a lightweight, dynamic tracing-based technique detects the input checks that the fuzzer-generated inputs fail. These checks are then removed from the target program. Fuzzing then continues on the transformed program, allowing the code protected by the removed checks to be triggered and potential bugs discovered.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1803.09010\">Data Sheets for Data Sets</a> -- <i>Currently there is no standard way to identify how a data set was created, and what characteristics, motivations, and potential skews it represents. To begin to address this issue, we propose the concept of a data sheet for data sets, a short document to accompany public data sets, commercial APIs, and pretrained models.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://bitshifters.github.io/posts/prods/bs-pop-beeb.html\">Porting Prince of Persia to the BBC Master</a> -- the author of the original 1980s game, Jordan Mechner, found and posted the source code to the Apple II version. These fine folks ported it to a different 1980s computer. I love the creativity of people who hack on small retro systems. I find big web stuff lacks that these days: it's all up-to-your-elbows in frameworks.</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-3-april-2018'>Four short links: 3 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/rtT2vajk28Y\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 2 April 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lGbSuXSAv4s/four-short-links-2-april-2018\n",
      "<p><em>Game Networking, Grep JSON, Voting Ideas, and UIs from Pictures</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/ValveSoftware/GameNetworkingSockets\">Valve's Networking Code</a> -- <i>a basic transport layer for games. The features are: connection-oriented protocol (like TCP)...but message-oriented instead of stream-oriented; mix of reliable and unreliable messages; messages can be larger than underlying MTU, the protocol performs fragmentation and reassembly, and retransmission for reliable; bandwidth estimation based on TCP-friendly rate control (RFC 5348); encryption; AES per packet, Ed25519 crypto for key exchange and cert signatures; the details for shared key derivation and per-packet IV are based on Google QUIC; tools for simulating loss and detailed stats measurement.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/tomnomnom/gron/\">gron</a> -- grep JSON from the command line.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/@nayafia/the-problem-with-voting-8cff39f771e8\">The Problem With Voting</a> -- I don't agree with all of the analysis, but the proposed techniques are interesting. I did like the term \"lazy consensus\" <i>where consensus is assumed to be the default state (i.e., “default to yes”). The underlying theory is that most proposals are not interesting enough to discuss. But if anyone does object, a consensus seeking process begins.</i> (via <a href=\"https://danielbachhuber.com/2018/03/30/nothing-is-sacred/\">Daniel Bachhuber</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://arxiv.org/abs/1705.07962\">pix2code</a> -- <a href=\"https://github.com/tonybeltramelli/pix2code\">open source</a> code that generates Android, iOS, and web source code for a UI from just a photo. It's not coming for your job any time soon (<i>over 77% of accuracy</i>), but it's still a nifty idea. (via <a href=\"http://bit.ly/2uGxWu3\">Two Minute Papers</a>)</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-2-april-2018'>Four short links: 2 April 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/lGbSuXSAv4s\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "6 creative ways to solve problems with Linux containers and Docker\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/uC88mH5OUwA/6-creative-ways-to-solve-problems-with-linux-containers-and-docker\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/cmglee_container_city_2_crop-415ff13d94ec7ae32ac86a8ce9c7db18.jpg'/></p><p><em>An outside-the-box exploration of how containers can be used to provide novel solutions.</em></p><p>Most people are introduced to Docker and Linux containers as a way to approach solving a very specific problem they are experiencing in their organization. The problem they want to solve often revolves around either making the dev/test cycle faster and more reliable while simultaneously shortening the related feedback loops, or improving the packaging and deploying of applications into production in a very similar fashion. Today, there are a lot of tools in the ecosystem that can significantly decrease the time it takes to accomplish these tasks while also vastly improving the ability of individuals, teams, and organizations to reliably perform repetitive tasks successfully.</p>\n",
      "<p>That being said, tools have become such a big focus in the ecosystem that there are many people who haven’t really spent much time thinking about all the ways containers alone can provide interesting solutions to problems that can occur in the course of any technical task.</p>\n",
      "<p>To get the creative juices flowing and help folks start thinking outside the box, we’ll examine a few scenarios and explore how containers can be used to provide possible solutions. You'll notice that many of these examples utilize file mounts to access data stored on local machines.</p>\n",
      "<p>Note that all of these were tested on Mac OS X running a current stable release of <a href=\"https://www.docker.com/community-edition\">Docker: Community Edition</a>. Also, most of the examples assume you have a unix-based operating system, but they can often be adjusted to work on Windows.</p>\n",
      "<section data-type=\"sect1\" id=\"preparation-RDVsG\">\n",
      "<h2>Preparation</h2>\n",
      "\n",
      "<p>If you are planning on running these examples, go ahead and download the following images ahead of time so you can see how the commands run without the additional time required to pull down the images the first time:</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">acleancoder</code><code class=\"o\">/</code><code class=\"n\">imagemagick</code><code class=\"o\">-</code><code class=\"n\">full</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">jasperla</code><code class=\"o\">/</code><code class=\"n\">docker</code><code class=\"o\">-</code><code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">cross</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">spkane</code><code class=\"o\">/</code><code class=\"n\">dell</code><code class=\"o\">-</code><code class=\"n\">openmanage</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">debian</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">spkane</code><code class=\"o\">/</code><code class=\"n\">train</code><code class=\"o\">-</code><code class=\"n\">os</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">alpine</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">pull</code> <code class=\"n\">jess</code><code class=\"o\">/</code><code class=\"n\">firefox</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "</pre>\n",
      "<section data-type=\"sect1\" id=\"scenario-1-JMsvtL\">\n",
      "<h2>Scenario 1</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-for-console-commands-9psdFOtK\">\n",
      "<h3>Using containers for console commands</h3>\n",
      "\n",
      "<p>There are often applications that are very useful to have but don't run or are very difficult to compile on the platform we are using. Containers can provide a very easy way to run these applications, despite the apparent barriers (and even if we can run the application natively, containers can be a very compelling approach to packaging and distributing programs). In this example, we are using an <a href=\"https://github.com/ImageMagick/ImageMagick\">ImageMagick</a> container to resize an image. Although this particular example is easy to accomplish in other ways, it should give some insight into how a container can be used to take advantage of a wide variety of similar console-based tools.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">curl</code> <code class=\"o\">-</code><code class=\"n\">o</code> <code class=\"n\">docker</code><code class=\"o\">.</code><code class=\"n\">png</code> \\\n",
      "  <code class=\"n\">https</code><code class=\"p\">:</code><code class=\"o\">//</code><code class=\"n\">www</code><code class=\"o\">.</code><code class=\"n\">docker</code><code class=\"o\">.</code><code class=\"n\">com</code><code class=\"o\">/</code><code class=\"n\">sites</code><code class=\"o\">/</code><code class=\"n\">default</code><code class=\"o\">/</code><code class=\"n\">files</code><code class=\"o\">/</code><code class=\"n\">vertical</code><code class=\"o\">-</code><code class=\"n\">whitespace</code><code class=\"o\">.</code><code class=\"n\">png</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">ls</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">ti</code> <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"err\">$</code><code class=\"p\">(</code><code class=\"n\">pwd</code><code class=\"p\">):</code><code class=\"o\">/</code><code class=\"n\">data</code> <code class=\"n\">acleancoder</code><code class=\"o\">/</code><code class=\"n\">imagemagick</code><code class=\"o\">-</code><code class=\"n\">full</code><code class=\"p\">:</code><code class=\"n\">latest</code> \\\n",
      "  <code class=\"n\">convert</code> <code class=\"o\">/</code><code class=\"n\">data</code><code class=\"o\">/</code><code class=\"n\">docker</code><code class=\"o\">.</code><code class=\"n\">png</code> <code class=\"o\">-</code><code class=\"n\">resize</code> <code class=\"mi\">50</code><code class=\"o\">%</code> <code class=\"o\">/</code><code class=\"n\">data</code><code class=\"o\">/</code><code class=\"n\">half_docker</code><code class=\"o\">.</code><code class=\"n\">png</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">ls</code>\n",
      "</pre>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"scenario-2-9psxhe\">\n",
      "<h2>Scenario 2</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-for-development-environments-ZKs1Feha\">\n",
      "<h3>Using containers for development environments</h3>\n",
      "\n",
      "<ul>\n",
      "<li>\n",
      "<p>Have you ever needed to set up your development environment at a new job or on a new computer and struggled to get it right?</p>\n",
      "</li>\n",
      "<li>\n",
      "<p>Have you ever had problems because your development and Q/A environments used slightly different versions of the compiler?</p>\n",
      "</li>\n",
      "</ul>\n",
      "<p>By using containers, it is possible to ensure your builds are repeatable, even for complex development environments. In this example, we are using a Docker image that contains a robust <a href=\"https://golang.org/\">Go</a> development environment to compile a small console game for Linux, OS X, and Windows.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">git</code> <code class=\"n\">clone</code> <code class=\"n\">https</code><code class=\"p\">:</code><code class=\"o\">//</code><code class=\"n\">github</code><code class=\"o\">.</code><code class=\"n\">com</code><code class=\"o\">/</code><code class=\"n\">spkane</code><code class=\"o\">/</code><code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">paranoia</code><code class=\"o\">.</code><code class=\"n\">git</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">cd</code> <code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">paranoia</code><code class=\"o\">/</code><code class=\"n\">paranoia</code>\n",
      "</pre>\n",
      "<section data-type=\"sect3\" id=\"mac-os-x-rWsZhYFWhM\">\n",
      "<h4>Mac OS X</h4>\n",
      "\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">ti</code> <code class=\"o\">--</code><code class=\"n\">rm</code> <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">APPNAME</code><code class=\"o\">=</code><code class=\"n\">paranoia</code> \\\n",
      "  <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">GOLANG_TARGET_PLATFORM</code><code class=\"o\">=</code><code class=\"s2\">\"darwin/amd64\"</code> <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"s2\">\"$(pwd):/go/src/app\"</code> \\\n",
      "  <code class=\"n\">jasperla</code><code class=\"o\">/</code><code class=\"n\">docker</code><code class=\"o\">-</code><code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">cross</code>\n",
      "<code class=\"err\">$</code> <code class=\"o\">./</code><code class=\"n\">paranoia</code><code class=\"o\">-</code><code class=\"n\">darwin</code><code class=\"o\">-</code><code class=\"n\">amd64</code>\n",
      "</pre>\n",
      "</section>\n",
      "<section data-type=\"sect3\" id=\"linux-wPsvTPFphJ\">\n",
      "<h4>Linux</h4>\n",
      "\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">ti</code> <code class=\"o\">--</code><code class=\"n\">rm</code> <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">APPNAME</code><code class=\"o\">=</code><code class=\"n\">paranoia</code> \\\n",
      "  <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">GOLANG_TARGET_PLATFORM</code><code class=\"o\">=</code><code class=\"s2\">\"linux/amd64\"</code> <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"s2\">\"$(pwd):/go/src/app\"</code> \\\n",
      "  <code class=\"n\">jasperla</code><code class=\"o\">/</code><code class=\"n\">docker</code><code class=\"o\">-</code><code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">cross</code>\n",
      "<code class=\"err\">$</code> <code class=\"o\">./</code><code class=\"n\">paranoia</code><code class=\"o\">-</code><code class=\"n\">linux</code><code class=\"o\">-</code><code class=\"n\">amd64</code>\n",
      "</pre>\n",
      "</section>\n",
      "<section data-type=\"sect3\" id=\"windows-xqspcBFphk\">\n",
      "<h4>Windows</h4>\n",
      "\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"o\">&gt;</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">ti</code> <code class=\"o\">--</code><code class=\"n\">rm</code> <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">APPNAME</code><code class=\"o\">=</code><code class=\"n\">paranoia</code> \\\n",
      "  <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">GOLANG_TARGET_PLATFORM</code><code class=\"o\">=</code><code class=\"s2\">\"windows/amd64\"</code> <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"s2\">\"$(pwd):/go/src/app\"</code> \\\n",
      "  <code class=\"n\">jasperla</code><code class=\"o\">/</code><code class=\"n\">docker</code><code class=\"o\">-</code><code class=\"n\">go</code><code class=\"o\">-</code><code class=\"n\">cross</code>\n",
      "<code class=\"o\">&gt;</code> <code class=\"o\">.</code>\\<code class=\"n\">paranoia</code><code class=\"o\">-</code><code class=\"n\">windows</code><code class=\"o\">-</code><code class=\"n\">amd64</code>\n",
      "</pre>\n",
      "</section>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"scenario-3-ZKs9Te\">\n",
      "<h2>Scenario 3</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-to-solve-os-version-incompatibilities-zVskFpTJ\">\n",
      "<h3>Using containers to solve OS version incompatibilities</h3>\n",
      "\n",
      "<p>This example will only work on a Linux server running on Dell hardware, but it provides a good example of how containers can make it easier to run certain classes of software.</p>\n",
      "<p>Dell's OpenManage Server Administrator (OMSA) is critical for monitoring and configuring Dell hardware, but Dell supports only a few Linux distributions and can be slow to provide updates for newer releases. By using containers, we can ensure that OMSA is packaged with the Linux platform (e.g., CentOS 7) and libraries that it requires, while still having the freedom to run it on the Linux platform (e.g., CoreOS) that we require.</p>\n",
      "<p>In the example below, we launch a container that runs continuously in the background with a few processes that Dell uses to facilitate communication between the Dell tools and the underlying hardware. We then wait 40 seconds while the container finishes starting up all the background process that it launches.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">--</code><code class=\"n\">privileged</code> <code class=\"o\">-</code><code class=\"n\">d</code> <code class=\"o\">-</code><code class=\"n\">p</code> <code class=\"mi\">161</code><code class=\"p\">:</code><code class=\"mi\">161</code><code class=\"o\">/</code><code class=\"n\">udp</code> <code class=\"o\">-</code><code class=\"n\">p</code> <code class=\"mi\">1311</code><code class=\"p\">:</code><code class=\"mi\">1311</code> \\\n",
      "  <code class=\"o\">--</code><code class=\"n\">restart</code><code class=\"o\">=</code><code class=\"n\">always</code> <code class=\"o\">--</code><code class=\"n\">net</code><code class=\"o\">=</code><code class=\"n\">host</code> <code class=\"o\">--</code><code class=\"n\">name</code><code class=\"o\">=</code><code class=\"n\">omsa</code> \\\n",
      "  <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"o\">/</code><code class=\"n\">lib</code><code class=\"o\">/</code><code class=\"n\">modules</code><code class=\"o\">/</code><code class=\"sb\">`uname -r`</code><code class=\"p\">:</code><code class=\"o\">/</code><code class=\"n\">lib</code><code class=\"o\">/</code><code class=\"n\">modules</code><code class=\"o\">/</code><code class=\"sb\">`uname -r`</code> \\\n",
      "  <code class=\"n\">spkane</code><code class=\"o\">/</code><code class=\"n\">dell</code><code class=\"o\">-</code><code class=\"n\">openmanage</code><code class=\"p\">:</code><code class=\"n\">latest</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">sleep</code> <code class=\"mi\">40</code><code class=\"n\">s</code>\n",
      "</pre>\n",
      "<p>Once the container is running, we can utilize <code>docker exec</code> to treat the running container like a simple command line tool and query the hardware, just as if the OMSA tools were installed and running in a more traditional manner. With this command, we retrieve the chassis info.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"k\">exec</code> <code class=\"n\">omsa</code> <code class=\"n\">omreport</code> <code class=\"n\">chassis</code> <code class=\"n\">info</code>\n",
      "\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Information</code>\n",
      "\n",
      "<code class=\"n\">Index</code>                                    <code class=\"p\">:</code> <code class=\"mi\">0</code>\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Name</code>                             <code class=\"p\">:</code> <code class=\"n\">Main</code> <code class=\"n\">System</code> <code class=\"n\">Chassis</code>\n",
      "<code class=\"n\">Host</code> <code class=\"n\">Name</code>                                <code class=\"p\">:</code> <code class=\"n\">dell</code><code class=\"o\">-</code><code class=\"n\">host</code><code class=\"o\">.</code><code class=\"n\">example</code><code class=\"o\">.</code><code class=\"n\">net</code>\n",
      "<code class=\"n\">iDRAC8</code> <code class=\"n\">Version</code>                           <code class=\"p\">:</code> <code class=\"mf\">2.30</code><code class=\"o\">.</code><code class=\"mf\">30.30</code> <code class=\"p\">(</code><code class=\"n\">Build</code> <code class=\"mi\">50</code><code class=\"p\">)</code>\n",
      "<code class=\"n\">Lifecycle</code> <code class=\"n\">Controller</code> <code class=\"n\">Version</code>             <code class=\"p\">:</code> <code class=\"mf\">2.30</code><code class=\"o\">.</code><code class=\"mf\">30.30</code>\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Model</code>                            <code class=\"p\">:</code> <code class=\"n\">PowerEdge</code> <code class=\"n\">R430</code>\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Lock</code>                             <code class=\"p\">:</code> <code class=\"n\">Present</code>\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Service</code> <code class=\"n\">Tag</code>                      <code class=\"p\">:</code> <code class=\"mi\">245</code><code class=\"n\">XWE2</code>\n",
      "<code class=\"n\">Express</code> <code class=\"n\">Service</code> <code class=\"n\">Code</code>                     <code class=\"p\">:</code> <code class=\"mo\">0000</code><code class=\"mi\">924034</code>\n",
      "<code class=\"n\">Chassis</code> <code class=\"n\">Asset</code> <code class=\"n\">Tag</code>                        <code class=\"p\">:</code> <code class=\"n\">Unknown</code>\n",
      "<code class=\"n\">Flash</code> <code class=\"n\">chassis</code> <code class=\"n\">identify</code> <code class=\"n\">LED</code> <code class=\"n\">state</code>         <code class=\"p\">:</code> <code class=\"n\">Off</code>\n",
      "<code class=\"n\">Flash</code> <code class=\"n\">chassis</code> <code class=\"n\">identify</code> <code class=\"n\">LED</code> <code class=\"n\">timeout</code> <code class=\"n\">value</code> <code class=\"p\">:</code> <code class=\"mi\">300</code>\n",
      "</pre>\n",
      "<p>And then we can immediately run another command to clear the ESM log, or whatever else we might need.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"k\">exec</code> <code class=\"n\">omsa</code> <code class=\"n\">omconfig</code> <code class=\"n\">system</code> <code class=\"n\">esmlog</code> <code class=\"n\">action</code><code class=\"o\">=</code><code class=\"n\">clear</code>\n",
      "\n",
      "<code class=\"n\">Embedded</code> <code class=\"n\">System</code> <code class=\"n\">Management</code> <code class=\"p\">(</code><code class=\"n\">ESM</code><code class=\"p\">)</code> <code class=\"n\">log</code> <code class=\"n\">cleared</code> <code class=\"n\">successfully</code><code class=\"o\">.</code>\n",
      "</pre>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"scenario-4-zVsKcy\">\n",
      "<h2>Scenario 4</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-to-explore-the-underlying-host-yDsyFXcn\">\n",
      "<h3>Using containers to explore the underlying host</h3>\n",
      "\n",
      "<p>Docker: Community Edition (CE) does a great job of making the Docker server feel like it runs natively on Mac OS X and Windows. Honestly, it does too good a job. When you are first trying to learn how Docker works, it can actually be very deceiving because Docker: CE launches a lightweight Linux virtual machine (VM) on both of these platforms, but it is not obvious to the end user that this is the case, and there is no way to log in to this VM and take a look around. So, given all this, how do you learn more about how the Docker VM works?</p>\n",
      "<p>In this scenario, we can utilize a partially privileged container and Linux namespaces to launch a container that will allow us to see all the processes that are running on the underlying host and explore its filesystem.</p>\n",
      "<pre data-type=\"programlisting\">$ docker run --rm -it --cap-add SYS_ADMIN --cap-add SYS_PTRACE \\\n",
      "  --pid=host debian:latest nsenter -t 1 -m -u -n -i sh\n",
      "\n",
      "/ # cat /etc/os-release\n",
      "PRETTY_NAME=\"Docker for Mac\"\n",
      "\n",
      "/ # exit\n",
      "</pre>\n",
      "<p>Note: This is an important example of why running privileged containers in production can be very dangerous. Although we have only shared the host's PID namespace with this container and given it two <a href=\"http://man7.org/linux/man-pages/man7/capabilities.7.html\">Linux capabilities</a>, it can easily access the host's underlying filesystem.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"scenario-5-yDsBiZ\">\n",
      "<h2>Scenario 5</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-to-sidestep-a-read-only-filesystem-V7spFJi1\">\n",
      "<h3>Using containers to sidestep a read-only filesystem</h3>\n",
      "\n",
      "<p>Another quirk of Docker: Community Edition is that the virtual machine's root filesystem has been made read-only in recent releases. This was done to help prevent people from breaking Docker by fooling around inside the VM, utilizing techniques like the one we just showed. This is understandable since they need to be able to support their product in such a wide range of environments, but it can also be problematic.</p>\n",
      "<p>While teaching classes about Docker and specifically trying to demonstrate how Linux Control Groups (cgroups) impact the resources that are available to a container, it is often desirable to run a simple tool on the Linux host that makes it easy to monitor the processes that are running and see how they are performing. </p>\n",
      "<p>In class, we will often run a container that stresses the underlying VM by generating some load on the CPU and memory so that we can see what effect it has on the underlying VM.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">d</code> <code class=\"n\">spkane</code><code class=\"o\">/</code><code class=\"n\">train</code><code class=\"o\">-</code><code class=\"n\">os</code><code class=\"p\">:</code><code class=\"n\">latest</code> <code class=\"n\">stress</code> <code class=\"o\">-</code><code class=\"n\">v</code> <code class=\"o\">--</code><code class=\"n\">cpu</code> <code class=\"mi\">2</code> <code class=\"o\">--</code><code class=\"n\">io</code> <code class=\"mi\">1</code> <code class=\"o\">--</code><code class=\"n\">vm</code> <code class=\"mi\">2</code> <code class=\"o\">--</code><code class=\"n\">vm</code><code class=\"o\">-</code><code class=\"nb\">bytes</code> <code class=\"mi\">128</code><code class=\"n\">M</code> <code class=\"o\">--</code><code class=\"n\">timeout</code> <code class=\"mi\">480</code><code class=\"n\">s</code>\n",
      "</pre>\n",
      "<p>A simple and visually appealing tool, like <code>htop</code> is ideal for observing the impact on the VM, but since the root filesystem of the virtual machine is read-only, there is no way to install <code>htop</code> directly into the VM. </p>\n",
      "<pre data-type=\"programlisting\">$ docker run --rm -it --cap-add SYS_ADMIN --cap-add SYS_PTRACE \\\n",
      "  --pid=host debian:latest nsenter -t 1 -m -u -n -i sh\n",
      "\n",
      "/ # apk update\n",
      "ERROR: Unable to lock database: Read-only file system\n",
      "ERROR: Failed to open apk database: Read-only file system\n",
      "\n",
      "/ # exit\n",
      "</pre>\n",
      "<p>So, instead of using the VM directly, we can run a container that shares the host's PID (process) namespace so that <code>htop</code> can see all the processes running on the host and allow us to understand how our <code>stress</code> program is impacting the Docker server.</p>\n",
      "<pre data-type=\"programlisting\">$ docker run --rm -it --pid=host alpine:latest sh\n",
      "\n",
      "/ # apk update\n",
      "fetch http://dl-cdn.alpinelinux.org/alpine/v3.6/main/x86_64/APKINDEX.tar.gz\n",
      "...\n",
      "OK: 8437 distinct packages available\n",
      "\n",
      "/ # apk add htop\n",
      "(1/4) Installing ncurses-terminfo-base (6.0_p20171125-r0)\n",
      "...\n",
      "OK: 11 MiB in 15 packages\n",
      "\n",
      "/ # htop -p $(pgrep stress | tr '\\n' ',')   # Press q to exit\n",
      "\n",
      "/ # exit\n",
      "</pre></section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"scenario-6-V7sVuw\">\n",
      "<h2>Scenario 6</h2>\n",
      "\n",
      "<section data-type=\"sect2\" id=\"using-containers-to-run-x11-graphical-applications-rWspFpuW\">\n",
      "<h3>Using containers to run X11 graphical applications</h3>\n",
      "\n",
      "<p>This specific example is designed for Mac OS X, but it can be easily modified for Linux and Windows. On Windows, you will also need to install a third-party X11 server, like <a href=\"https://xming.en.softonic.com/\">Xming</a>, <a href=\"https://x.cygwin.com/\">Cygwin/X</a> or <a href=\"https://mobaxterm.mobatek.net/\">MobaXterm</a>.</p>\n",
      "<p>On Mac OS X, if you do not already have the <code>homebrew</code> package manager installed, you can get it from <a href=\"https://brew.sh/\">Homebrew</a>.</p>\n",
      "<p>To make this <a href=\"https://en.wikipedia.org/wiki/X_Window_System\">X11</a> container work, we need to prepare our system the first time by installing <a href=\"https://www.linux.com/news/socat-general-bidirectional-pipe-handler\">socat</a> and <a href=\"https://www.xquartz.org/\">Xquartz</a>, an X11 server, on the Mac. Once Xquartz is installed, we need to reboot the Mac so that the X11 server is set up properly for the current user.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">brew</code> <code class=\"n\">install</code> <code class=\"n\">socat</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">brew</code> <code class=\"n\">cask</code> <code class=\"n\">install</code> <code class=\"n\">xquartz</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">shutdown</code> <code class=\"o\">-</code><code class=\"n\">r</code> <code class=\"n\">now</code>\n",
      "</pre>\n",
      "<p>After this one-time setup, we can now run a Linux X11 graphical application by running <code>socat</code> to facilitate communication between the container and the Mac's X11 server, and then launching the desired container.</p>\n",
      "<pre data-code-language=\"python\" data-type=\"programlisting\" data-highlighted=\"true\"><code class=\"err\">$</code> <code class=\"n\">socat</code> <code class=\"n\">TCP</code><code class=\"o\">-</code><code class=\"n\">LISTEN</code><code class=\"p\">:</code><code class=\"mi\">6000</code><code class=\"p\">,</code><code class=\"n\">reuseaddr</code><code class=\"p\">,</code><code class=\"n\">fork</code> <code class=\"n\">UNIX</code><code class=\"o\">-</code><code class=\"n\">CLIENT</code><code class=\"p\">:</code>\\<code class=\"s2\">\"$DISPLAY</code><code class=\"se\">\\\"</code><code class=\"s2\"> &amp;</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">IP</code><code class=\"o\">=</code><code class=\"err\">$</code><code class=\"p\">(</code><code class=\"n\">ifconfig</code> <code class=\"n\">en0</code> <code class=\"o\">|</code> <code class=\"n\">grep</code> <code class=\"n\">inet</code> <code class=\"o\">|</code> <code class=\"n\">awk</code> <code class=\"s1\">'$1==\"inet\" {print $2}'</code><code class=\"p\">)</code>\n",
      "<code class=\"err\">$</code> <code class=\"n\">docker</code> <code class=\"n\">run</code> <code class=\"o\">-</code><code class=\"n\">it</code> <code class=\"o\">--</code><code class=\"n\">rm</code> <code class=\"o\">-</code><code class=\"n\">e</code> <code class=\"n\">DISPLAY</code><code class=\"o\">=</code><code class=\"err\">$</code><code class=\"p\">{</code><code class=\"n\">IP</code><code class=\"p\">}:</code><code class=\"mi\">0</code> <code class=\"n\">jess</code><code class=\"o\">/</code><code class=\"n\">firefox</code>\n",
      "</pre>\n",
      "<p>After a few moments you should see a usable Firefox browser window open on your system.</p>\n",
      "<p>Note: On the Mac, it is actually possible to set the DISPLAY to <code>docker.for.mac.host.internal:0</code> instead of using the primary IP address. Also, don't forget to kill the <code>socat</code> process when you are done playing around with this.</p>\n",
      "</section>\n",
      "</section>\n",
      "<section data-type=\"sect1\" id=\"conclusion-rWsgUE\">\n",
      "<h2>Conclusion</h2>\n",
      "\n",
      "<p>Hopefully this article has helped expose you to some of the less obvious ways that containers can be used. I can't recommend enough that you take the time to become familiar with the underlying technologies that enable containers and how things like namespaces, cgroups, Linux capabilities, and even hardware virtualization can be combined to solve problems in new and creative ways.</p>\n",
      "<aside data-type=\"sidebar\" id=\"id-XVSKIZUZ\">\n",
      "<p>Sean Kane will be teaching an in-depth, hands-on, two-day Docker workshop at the O'Reilly Velocity Conference in San Jose, June 11-14, 2018. During the first day, students will be introduced to Linux containers and Docker, with deep dives later in the day that will help explain the functionality that makes all of the above techniques possible. During the second day of the workshop, students will build a functional DevOps CI/CD pipeline using Docker and get hands-on experience with how Docker and Linux containers can be used to streamline processes, and improve both reliability and repeatability on projects. <a href=\"https://conferences.oreilly.com/velocity/vl-ca/public/schedule/detail/66897?intcmp=il-prog-confreg-lp-vlca18_new_site_creatively-leveraging-linux-containers-and-docker_bottom_note_cta\">Register now to participate in this hands-on training</a>.</p>\n",
      "</aside>\n",
      "\n",
      "<figure class=\"center\" id=\"id-KGiZtVUw\"><a href=\"https://conferences.oreilly.com/velocity/vl-ca?intcmp=il-webops-confreg-lp-vlca18_new_site_velocity_banner_ad_link_creatively-leveraging-linux-containers-and-docker\"><img alt=\"Velocity 2018\" src=\"https://d3ansictanv2wj.cloudfront.net/Velocity_Image1-d067b3fa9eb7c92a09e82a9fde0e812f.jpg\"></a></figure>\n",
      "</section>\n",
      "</section>\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/6-creative-ways-to-solve-problems-with-linux-containers-and-docker'>6 creative ways to solve problems with Linux containers and Docker.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/uC88mH5OUwA\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 30 March 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/cvoY43pM-y8/four-short-links-30-march-2018\n",
      "<p><em>Data Literacy, Data Science Readings, Bloated Data Architectures, and AI Ruins Everything</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"http://mediasmarts.ca/digital-media-literacy/educational-games/data-defenders-grades-4-6\">Data Defenders</a> -- game for grade 4-6 <i>that teaches children and pre-teens the concept of personal information and its economic value, and introduces them to ways to manage and protect their personal information on the websites and apps they enjoy.</i> (via <a href=\"https://boingboing.net/2018/03/28/timely-ludens.html\">BoingBoing</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://github.com/hadley/stats337#readings\">Readings in Applied Data Science</a> -- pointers to interesting papers, via Hadley Wickham's Stanford class.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf\">COST: Configuration that Outperforms a Single Thread</a> -- <i>The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. [...] We survey measurements of data-parallel systems recently reported in SOSP and OSDI, and find that many systems have either a surprisingly large COST, often hundreds of cores, or simply underperform one thread for all their reported configurations.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://public.tepper.cmu.edu/jnh/scales2.pdf\">Finding Alternative Musical Scales</a> -- is there nothing that AI cannot improve/ruin? <i>We search for alternative musical scales that share the main advantages of classical scales: pitch frequencies that bear simple ratios to each other, and multiple keys based on an underlying chromatic scale with tempered tuning. We conduct the search by formulating a constraint satisfaction problem that is well suited for solution by constraint programming. We find that certain 11-note scales on a 19-note chromatic stand out as superior to all others. These scales enjoy harmonic and structural possibilities that go significantly beyond what is available in classical scales and therefore provide a possible medium for innovative musical composition.</i> (via <a href=\"https://twitter.com/mjntendency/status/978283090580733952\">Mark J. Nelson</a>)</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-30-march-2018'>Four short links: 30 March 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/cvoY43pM-y8\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "What machine learning engineers need to know\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0T4QvUO0r6k/what-machine-learning-engineers-need-to-know\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/fractal-1280084_1920_crop-780f3c66e05b5ddf89efddd9d27b3646.jpg'/></p><p><em>The O’Reilly Data Show Podcast: Jesse Anderson and Paco Nathan on organizing data teams and next-generation messaging with Apache Pulsar.</em></p><p>In this episode of the <a href=\"https://www.oreilly.com/ideas/topics/oreilly-data-show-podcast\">Data Show</a>, I spoke with&nbsp;<a href=\"https://www.linkedin.com/in/eljefe6a/\">Jesse Anderson</a>, managing director of <a href=\"http://www.bigdatainstitute.io/\">the Big Data Institute</a>, and my colleague <a href=\"https://www.linkedin.com/in/ceteri/\">Paco Nathan</a>, who recently became co-chair of <a href=\"https://conferences.oreilly.com/jupyter/jup-ny\">Jupytercon</a>. This conversation grew out of a recent email thread the three of us had on <a href=\"https://www.oreilly.com/ideas/what-are-machine-learning-engineers\">machine learning engineers</a>, a new job role that LinkedIn recently pegged as <a href=\"https://business.linkedin.com/talent-solutions/blog/trends-and-research/2017/here-are-the-20-fastest-growing-jobs-in-the-us\">the fastest growing job in the U.S.</a> In our email discussion, there was some disagreement on whether such a specialized job role/title was needed in the first place. As Eric Colson pointed out in his beautiful <a href=\"https://www.oreilly.com/ideas/differentiating-via-data-science\">keynote at Strata Data San Jose</a>, when done too soon, creating specialized roles can slow down your data team.</p><p>Continue reading <a href='https://www.oreilly.com/ideas/what-machine-learning-engineers-need-to-know'>What machine learning engineers need to know.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/0T4QvUO0r6k\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "UX challenges in the Internet of Things\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3QEaQCyj4_E/ux-challenges-in-the-internet-of-things\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/system-2660914_1920_crop-c163bf9aaec4d79ee4575b5d8390f46e.jpg'/></p><p><em>A look the various ways the IoT asks consumers to think like programmers, and the risks inherent in exponentially increasing educators.</em></p><p>Continue reading <a href='https://www.oreilly.com/ideas/ux-challenges-in-the-internet-of-things'>UX challenges in the Internet of Things.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/3QEaQCyj4_E\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 29 March 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/LC2N2TJ_CzA/four-short-links-29-march-2018\n",
      "<p><em>Facebook Container, Publishing Future, Social Media Ethics, and Online Virality</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://blog.mozilla.org/blog/2018/03/27/facebook-container-add-on/\">Facebook Container</a> -- Firefox add-on that <i>isolates your Facebook identity from the rest of your web activity. When you install it, you will continue to be able to use Facebook normally. Facebook can continue to deliver their service to you and send you advertising. The difference is that it will be much harder for Facebook to use your activity collected off Facebook to send you ads and other targeted messages.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://blogs.harvard.edu/doc/2018/03/23/nothing/\">What's Coming for Online Publishing</a> (Doc Searls) -- <i>What will happen when the Times, the New Yorker, and other pubs own up to the simple fact that they are just as guilty as Facebook of leaking its readers’ data to other parties, for—in many if not most cases—God knows what purposes besides “interest-based” advertising?</i> (via <a href=\"https://twitter.com/Piers/status/977800836511277056\">Piers Harding</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://freedom-to-tinker.com/2018/03/26/is-affiliate-marketing-disclosed-to-consumers-on-social-media/\">Affiliate Marketing Not Disclosed on Social Media</a> (Freedom to Tinker) -- <i>Of all the YouTube videos and Pinterest pins that contained affiliate links, only ~10% and ~7% respectively contained accompanying disclosures.</i> (<a href=\"https://arxiv.org/pdf/1803.08488.pdf\">paper</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://5harad.com/papers/twiral.pdf\">The Structural Virality of Online Diffusion</a> -- <i> Indeed, the very label “viral hit” implies precisely the exponential spreading of the sort observed in contagion models in their supercritical regime. It is therefore notable that essentially everything we observe, including the very largest and rarest events, can be accounted for by a simple model operating entirely in the low infectiousness parameter regime.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-29-march-2018'>Four short links: 29 March 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/LC2N2TJ_CzA\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "A graphical user interface to build apps on top of microservices\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/01-SocPxRug/a-graphical-user-interface-to-build-apps-on-top-of-microservices\n",
      "<p><img src='https://d3tdunqjn7n0wj.cloudfront.net/600x450/black-1693156_1920_crop-7be98f4a00a59cc3cc2497bd9a2fed7b.jpg'/></p><p><em>How to enable non-programmer business users to create their own data applications.</em></p><p>Companies are increasingly asking their IT staffs for rapid turn-around on tasks that require programming. The most likely path to attaining quick turn-around would be to let non-programmers create their own applications—an approach that can be achieved with a combination of microservices, APIs, and graphical user interfaces (GUIs).</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The goal of letting non-programmers manipulate data is an old one. When IBM released early versions of the SQL programming language, they intended it for business users. They really expected a mid-level manager to sit down in the morning and type UPDATE PRODUCT.SPEC SET WIDTH = 19, HEIGHT = 12, DEPTH = 4 WHERE PRODUCT_NUM LIKE 'GK145%' onto a green screen. More realistically, though, SQL was hidden behind various forms that eventually moved to the web. But these forms do not offer the full power of the language.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Visual programming, using some technique such as moving boxes around a screen to create control flows, has also been researched for decades. It is best known in children's educational tools such as <a href=\"https://www.alice.org/\">Alice</a> and MIT's <a href=\"https://scratch.mit.edu/\">Scratch</a>. The problem with trying to create large-scale applications using visual programming—and the reason I'm guessing that visual programming hasn't won a large user base—is that programming's complexity lies more in the mind than in the syntax. Exposing the complexity of control flow and math through boxes and lines doesn't make it easier than using words and punctuation.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>It may be quite a different matter, however, when powerful chunks of functionality, already hooked up to an organization's data sets and control structure, are offered in a visual interface, also known as a “low-code development platform.” For instance, an auto insurance assessor can do something really useful if she can automate part of her job by drawing a line between client information and a tool that calculates how much to pay for each part of a car.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Where do microservices enter the picture? They work at a higher level than function or library calls, offering precise access to specific data and services in the organization. I talked about microservices with Bruno Trimouille, a senior marketing executive at <a href=\"https://www.tibco.com/\">TIBCO Software</a>. He spoke of an airline that has microservices for travel services, customer profile data, flight information, and other elements of their workflow. Visual programming exposes all these things to the average employee. If someone in the luggage department thinks up an application that can find a bag more quickly or send a voucher to a customer's mobile phone, he can hook up services to create the application.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>The low-code application environment can tap microservices to digitize a process that previously was done manually, such as submitting expenses or getting travel approval. It can also extend an existing system, or add some logic that stitches two microservices together.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>As another example, Trimouille points to the problem of repricing in insurance. When an auto body shop takes apart a car, it often discovers new damage that the insurance assessor did not originally cover. The insurance company can upgrade its assessment tool to do repricing quickly and save both time and money.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p>Thus, microservices, APIs, and low-code application development may lead to a new level of productivity. They could provide end users with the deep reach into their corporate data promised by the classic UPDATE statement—but now backed up by full insight into the structure and value of the data.</p>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "<p><em>This post is a collaboration between O'Reilly and TIBCO. </em><a href=\"http://www.oreilly.com/about/editorial_independence.html\"><em>See our statement of editorial independence</em></a><em>.</em></p>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/a-graphical-user-interface-to-build-apps-on-top-of-microservices'>A graphical user interface to build apps on top of microservices.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/01-SocPxRug\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 28 March 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/RUcYXYiIfRQ/four-short-links-28-march-2018\n",
      "<p><em>Business Logic, Digital Forgery, Twitter Demetricator, and ML for Kids</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://boingboing.net/2018/03/27/surveillance-dildos.html\">The Business Logic of Silicon Valley</a> (Cory Doctorow) -- <i>Selling hardware does not have exponential growth potential, because people only need so many sex toys, and really successful models are likely to be cloned or imitated, driving down the price. For a \"smart\" sex toy startup to cash out its investors, it will need to have a second, much more profitable business, and that, inevitably, is private data about sex toy users.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://giorgiop.github.io/posts/2018/03/17/AI-and-digital-forgery/\">Commoditization of AI, Digital Forgery, and the End of Trust</a> -- <i>Once tools for fabrication become a commodity, the effects will be more dramatic than the current phenomenon of fake news. In the tech circles, the issue are discussed only at a philosophical level; no clear solution is known at present time. This post discusses pros and cons of two classes of potential solutions: digital signatures and learning-based detection systems.</i>\r\n",
      "</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://bengrosser.com/projects/twitter-demetricator/\">Twitter Demetricator</a> -- a Chrome interface that hides all the gamification numbers in Twitter.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://machinelearningforkids.co.uk/#!/worksheets\">Machine Learning for Kids</a> -- <i>Each project is a stand-alone activity, written to last for a single lesson, and will guide children to create a game or interactive project that demonstrates a real-world use of artificial intelligence and machine learning.</i>\r\n",
      "</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-28-march-2018'>Four short links: 28 March 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/RUcYXYiIfRQ\" height=\"1\" width=\"1\" alt=\"\"/>\n",
      "Four short links: 27 March 2018\n",
      "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/i-PQWT-lu3U/four-short-links-27-march-2018\n",
      "<p><em>Database Attack, Death of Android Predicted, Predicting Recidivism, and Cybersecurity Law and Policy</em></p><ol>\r\n",
      "<li>\r\n",
      "<a href=\"https://www.imperva.com/blog/2018/03/deep-dive-database-attacks-scarlett-johanssons-picture-used-for-crypto-mining-on-postgre-database/\">Deep Dive: Database Attacks</a> -- I enjoyed the description of how this attack worked: using Postgres to write and run executables, smuggling in an executable that sets up a cryptocurrency mining operation on the machine.</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://medium.com/@steve.yegge/who-will-steal-android-from-google-af3622b6252e\">Yegge on Android</a> (Steve Yegge) -- <i>Remember that I said it could take 20 minutes to see a 1-line code change in the regular Android stack? That can happen in the biggest apps like Nest or Facebook, but even for medium-size apps it can be two or three minutes. Whereas with React Native, it’s instantaneous. You make a change; you see the change. And that, folks, means you get to launch features 10x faster, which means faster time to market, which means first-mover advantage, which means you win, win, win. Abandoning native programming in favor of fast-cycle cross-platform frameworks like React Native is a winning strategy.</i> <a href=\"https://www.tbray.org/ongoing/When/201x/2018/03/26/Yegge-Being-Wrong\">Tim Bray</a> disagrees with some of Yegge's points. (Also: Yegge's hiring, which is why he's blogging again)</li>\r\n",
      "<li>\r\n",
      "<a href=\"http://advances.sciencemag.org/content/4/1/eaao5580.full\">The Accuracy, Fairness, and Limits of Predicting Recidivism</a> -- <i>We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. We further show that a simple linear predictor provided with only two features is nearly equivalent to COMPAS with its 137 features.</i> (via <a href=\"https://twitter.com/random_walker/status/977181776530624512\">Aravind Narayanan</a>)</li>\r\n",
      "<li>\r\n",
      "<a href=\"https://lawfareblog.com/teaching-cybersecurity-law-and-policy\">Teaching Cybersecurity Law and Policy</a> -- <i>My syllabus is much more than a one- or two-pager just listing the topics and weekly readings. Though there are a lot of reading assignments, the syllabus itself functions a bit like a casebook in that there also is a ton of narrative text framing each week’s topic, and also extensive questions for consideration matched to each reading.</i> <a href=\"https://assets.documentcloud.org/documents/4421135/Chesney-on-Cybersecurity-Law-and-Policy.pdf\">The full syllabus</a> is 58 pages long. (via <a href=\"https://twitter.com/BobbyChesney/status/978285676973174786\">Bobby Chesney</a>)</li>\r\n",
      "</ol>\r\n",
      "<p>Continue reading <a href='https://www.oreilly.com/ideas/four-short-links-27-march-2018'>Four short links: 27 March 2018.</a></p><img src=\"http://feeds.feedburner.com/~r/oreilly/radar/atom/~4/i-PQWT-lu3U\" height=\"1\" width=\"1\" alt=\"\"/>\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "FEED_URL='http://feeds.feedburner.com/oreilly/radar/atom'\n",
    "\n",
    "fp = feedparser.parse(FEED_URL)\n",
    "\n",
    "for e in fp.entries:\n",
    "    print(e.title)\n",
    "    print(e.links[0].href)\n",
    "    print(e.content[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3. Pseudocode for a breadth-first search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create an empty graph\n",
    "Create an empty queue to keep track of nodes that need to be processed\n",
    "\n",
    "Add the starting point to the graph as the root node\n",
    "Add the root node to a queue for processing\n",
    "\n",
    "Repeat until some maximum depth is reached or the queue is empty:\n",
    "  Remove a node from the queue \n",
    "  For each of the node's neighbors: \n",
    "    If the neighbor hasn't already been processed: \n",
    "      Add it to the queue \n",
    "      Add it to the graph \n",
    "      Create an edge in the graph that connects the node and its neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive sentence detection based on periods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', ' Green killed Colonel Mustard in the study with the candlestick', ' Mr', ' Green is not a very nice fellow', '']\n"
     ]
    }
   ],
   "source": [
    "txt = \"Mr. Green killed Colonel Mustard in the study with the candlestick. Mr. Green is not a very nice fellow.\"\n",
    "print(txt.split(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More sophisticated sentence detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/temp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['Mr. Green killed Colonel Mustard in the study with the candlestick.', 'Mr. Green is not a very nice fellow.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloading nltk packages used in this example\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(txt)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization of sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mr.', 'Green', 'killed', 'Colonel', 'Mustard', 'in', 'the', 'study', 'with', 'the', 'candlestick', '.'], ['Mr.', 'Green', 'is', 'not', 'a', 'very', 'nice', 'fellow', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech tagging for tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /Users/temp/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[[('Mr.', 'NNP'), ('Green', 'NNP'), ('killed', 'VBD'), ('Colonel', 'NNP'), ('Mustard', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('study', 'NN'), ('with', 'IN'), ('the', 'DT'), ('candlestick', 'NN'), ('.', '.')], [('Mr.', 'NNP'), ('Green', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('very', 'RB'), ('nice', 'JJ'), ('fellow', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Downloading nltk packages used in this example\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "\n",
    "pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
    "print(pos_tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named entity extraction/chunking for tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/temp/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/temp/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x128d9e6d0>\n",
      "(S\n",
      "  (PERSON Mr./NNP)\n",
      "  (PERSON Green/NNP)\n",
      "  killed/VBD\n",
      "  (ORGANIZATION Colonel/NNP Mustard/NNP)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  study/NN\n",
      "  with/IN\n",
      "  the/DT\n",
      "  candlestick/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Mr./NNP)\n",
      "  (ORGANIZATION Green/NNP)\n",
      "  is/VBZ\n",
      "  not/RB\n",
      "  a/DT\n",
      "  very/RB\n",
      "  nice/JJ\n",
      "  fellow/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Downloading nltk packages used in this example\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ne_chunks = nltk.ne_chunk_sents(pos_tagged_tokens)\n",
    "print(ne_chunks)\n",
    "#print(ne_chunks[0].pprint()) # You can prettyprint each chunk in the tree\n",
    "\n",
    "for chunk in ne_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4. Harvesting blog data by parsing feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 23 entries from 'All - O'Reilly Media'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulStoneSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5114bc8012f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     blog_posts.append({'title': e.title, 'content'\n\u001b[0;32m---> 22\u001b[0;31m                       : cleanHtml(e.content[0].value), 'link': e.links[0].href})\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ch05-webpages'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feed.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5114bc8012f7>\u001b[0m in \u001b[0;36mcleanHtml\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleanHtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     return BeautifulStoneSoup(clean_html(html),\n\u001b[0m\u001b[1;32m     13\u001b[0m                 convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BeautifulStoneSoup' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "#from BeautifulSoup import BeautifulStoneSoup\n",
    "from nltk import clean_html\n",
    "\n",
    "FEED_URL = 'http://feeds.feedburner.com/oreilly/radar/atom'\n",
    "\n",
    "def cleanHtml(html):\n",
    "    return BeautifulStoneSoup(clean_html(html),\n",
    "                convertEntities=BeautifulStoneSoup.HTML_ENTITIES).contents[0]\n",
    "\n",
    "fp = feedparser.parse(FEED_URL)\n",
    "\n",
    "print(\"Fetched %s entries from '%s'\" % (len(fp.entries[0].title), fp.feed.title))\n",
    "\n",
    "blog_posts = []\n",
    "for e in fp.entries:\n",
    "    blog_posts.append({'title': e.title, 'content'\n",
    "                      : cleanHtml(e.content[0].value), 'link': e.links[0].href})\n",
    "\n",
    "out_file = os.path.join('resources', 'ch05-webpages', 'feed.json')\n",
    "f = open(out_file, 'w')\n",
    "f.write(json.dumps(blog_posts, indent=1))\n",
    "f.close()\n",
    "\n",
    "print('Wrote output file to %s' % (f.name, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5. Using NLTK’s NLP tools to process human language in blog data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "# Customize your list of stopwords as needed. Here, we add common\n",
    "# punctuation and contraction artifacts.\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english') + [\n",
    "    '.',\n",
    "    ',',\n",
    "    '--',\n",
    "    '\\'s',\n",
    "    '?',\n",
    "    ')',\n",
    "    '(',\n",
    "    ':',\n",
    "    '\\'',\n",
    "    '\\'re',\n",
    "    '\"',\n",
    "    '-',\n",
    "    '}',\n",
    "    '{',\n",
    "    u'—',\n",
    "    ]\n",
    "\n",
    "for post in blog_data:\n",
    "    sentences = nltk.tokenize.sent_tokenize(post['content'])\n",
    "\n",
    "    words = [w.lower() for sentence in sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    # Basic stats\n",
    "\n",
    "    num_words = sum([i[1] for i in fdist.items()])\n",
    "    num_unique_words = len(fdist.keys())\n",
    "\n",
    "    # Hapaxes are words that appear only once\n",
    "\n",
    "    num_hapaxes = len(fdist.hapaxes())\n",
    "\n",
    "    top_10_words_sans_stop_words = [w for w in fdist.items() if w[0]\n",
    "                                    not in stop_words][:10]\n",
    "\n",
    "    print post['title']\n",
    "    print '\\tNum Sentences:'.ljust(25), len(sentences)\n",
    "    print '\\tNum Words:'.ljust(25), num_words\n",
    "    print '\\tNum Unique Words:'.ljust(25), num_unique_words\n",
    "    print '\\tNum Hapaxes:'.ljust(25), num_hapaxes\n",
    "    print '\\tTop 10 Most Frequent Words (sans stop words):\\n\\t\\t', \\\n",
    "            '\\n\\t\\t'.join(['%s (%s)'\n",
    "            % (w[0], w[1]) for w in top_10_words_sans_stop_words])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6. A document summarization algorithm based principally upon sentence detection and frequency analysis within sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "N = 100  # Number of words to consider\n",
    "CLUSTER_THRESHOLD = 5  # Distance between words to consider\n",
    "TOP_SENTENCES = 5  # Number of sentences to return for a \"top n\" summary\n",
    "\n",
    "# Approach taken from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "\n",
    "def _score_sentences(sentences, important_words):\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "\n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "\n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "\n",
    "        # For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                # Compute an index for where any important words occur in the sentence.\n",
    "\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError, e: # w not in this particular sentence\n",
    "                pass\n",
    "\n",
    "        word_idx.sort()\n",
    "\n",
    "        # It is possible that some sentences may not contain any important words at all.\n",
    "        if len(word_idx)== 0: continue\n",
    "\n",
    "        # Using the word index, compute clusters by using a max distance threshold\n",
    "        # for any two consecutive words.\n",
    "\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "\n",
    "        # Score each cluster. The max score for any given cluster is the score \n",
    "        # for the sentence.\n",
    "\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                * significant_words_in_cluster / total_words_in_cluster\n",
    "\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "\n",
    "        scores.append((sentence_idx, score))\n",
    "\n",
    "    return scores\n",
    "\n",
    "def summarize(txt):\n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    top_n_words = [w[0] for w in fdist.items() \n",
    "            if w[0] not in nltk.corpus.stopwords.words('english')][:N]\n",
    "\n",
    "    scored_sentences = _score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # Summarization Approach 1:\n",
    "    # Filter out nonsignificant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "\n",
    "    avg = numpy.mean([s[1] for s in scored_sentences])\n",
    "    std = numpy.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    # Summarization Approach 2:\n",
    "    # Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "\n",
    "    # Decorate the post object with summaries\n",
    "\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "for post in blog_data:\n",
    "       \n",
    "    post.update(summarize(post['content']))\n",
    "\n",
    "    print post['title']\n",
    "    print '=' * len(post['title'])\n",
    "    print\n",
    "    print 'Top N Summary'\n",
    "    print '-------------'\n",
    "    print ' '.join(post['top_n_summary'])\n",
    "    print\n",
    "    print 'Mean Scored Summary'\n",
    "    print '-------------------'\n",
    "    print ' '.join(post['mean_scored_summary'])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7. Visualizing document summarization results with HTML output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<html>\n",
    "    <head>\n",
    "        <title>%s</title>\n",
    "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "    </head>\n",
    "    <body>%s</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "for post in blog_data:\n",
    "   \n",
    "    # Uses previously defined summarize function.\n",
    "    post.update(summarize(post['content']))\n",
    "\n",
    "    # You could also store a version of the full post with key sentences marked up\n",
    "    # for analysis with simple string replacement...\n",
    "\n",
    "    for summary_type in ['top_n_summary', 'mean_scored_summary']:\n",
    "        post[summary_type + '_marked_up'] = '<p>%s</p>' % (post['content'], )\n",
    "        for s in post[summary_type]:\n",
    "            post[summary_type + '_marked_up'] = \\\n",
    "            post[summary_type + '_marked_up'].replace(s, '<strong>%s</strong>' % (s, ))\n",
    "\n",
    "        filename = post['title'].replace(\"?\", \"\") + '.summary.' + summary_type + '.html'\n",
    "        f = open(os.path.join('resources', 'ch05-webpages', filename), 'w')\n",
    "        html = HTML_TEMPLATE % (post['title'] + \\\n",
    "          ' Summary', post[summary_type + '_marked_up'],)\n",
    "              \n",
    "        f.write(html.encode('utf-8'))\n",
    "        f.close()\n",
    "\n",
    "        print \"Data written to\", f.name\n",
    "\n",
    "# Display any of these files with an inline frame. This displays the\n",
    "# last file processed by using the last value of f.name...\n",
    "\n",
    "print \"Displaying %s:\" % f.name\n",
    "display(IFrame('files/%s' % f.name, '100%', '600px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8. Extracting entities from a text with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "for post in blog_data:\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(post['content'])\n",
    "    tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "    pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
    "\n",
    "    # Flatten the list since we're not using sentence structure\n",
    "    # and sentences are guaranteed to be separated by a special\n",
    "    # POS tuple such as ('.', '.')\n",
    "\n",
    "    pos_tagged_tokens = [token for sent in pos_tagged_tokens for token in sent]\n",
    "\n",
    "    all_entity_chunks = []\n",
    "    previous_pos = None\n",
    "    current_entity_chunk = []\n",
    "    for (token, pos) in pos_tagged_tokens:\n",
    "\n",
    "        if pos == previous_pos and pos.startswith('NN'):\n",
    "            current_entity_chunk.append(token)\n",
    "        elif pos.startswith('NN'):\n",
    "            if current_entity_chunk != []:\n",
    "\n",
    "                # Note that current_entity_chunk could be a duplicate when appended,\n",
    "                # so frequency analysis again becomes a consideration\n",
    "\n",
    "                all_entity_chunks.append((' '.join(current_entity_chunk), pos))\n",
    "            current_entity_chunk = [token]\n",
    "\n",
    "        previous_pos = pos\n",
    "\n",
    "    # Store the chunks as an index for the document\n",
    "    # and account for frequency while we're at it...\n",
    "\n",
    "    post['entities'] = {}\n",
    "    for c in all_entity_chunks:\n",
    "        post['entities'][c] = post['entities'].get(c, 0) + 1\n",
    "\n",
    "    # For example, we could display just the title-cased entities\n",
    "\n",
    "    print post['title']\n",
    "    print '-' * len(post['title'])\n",
    "    proper_nouns = []\n",
    "    for (entity, pos) in post['entities']:\n",
    "        if entity.istitle():\n",
    "            print '\\t%s (%s)' % (entity, post['entities'][(entity, pos)])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9. Discovering interactions between entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "def extract_interactions(txt):\n",
    "    sentences = nltk.tokenize.sent_tokenize(txt)\n",
    "    tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "    pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
    "\n",
    "    entity_interactions = []\n",
    "    for sentence in pos_tagged_tokens:\n",
    "\n",
    "        all_entity_chunks = []\n",
    "        previous_pos = None\n",
    "        current_entity_chunk = []\n",
    "\n",
    "        for (token, pos) in sentence:\n",
    "\n",
    "            if pos == previous_pos and pos.startswith('NN'):\n",
    "                current_entity_chunk.append(token)\n",
    "            elif pos.startswith('NN'):\n",
    "                if current_entity_chunk != []:\n",
    "                    all_entity_chunks.append((' '.join(current_entity_chunk),\n",
    "                            pos))\n",
    "                current_entity_chunk = [token]\n",
    "\n",
    "            previous_pos = pos\n",
    "\n",
    "        if len(all_entity_chunks) > 1:\n",
    "            entity_interactions.append(all_entity_chunks)\n",
    "        else:\n",
    "            entity_interactions.append([])\n",
    "\n",
    "    assert len(entity_interactions) == len(sentences)\n",
    "\n",
    "    return dict(entity_interactions=entity_interactions,\n",
    "                sentences=sentences)\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "# Display selected interactions on a per-sentence basis\n",
    "\n",
    "for post in blog_data:\n",
    "\n",
    "    post.update(extract_interactions(post['content']))\n",
    "\n",
    "    print post['title']\n",
    "    print '-' * len(post['title'])\n",
    "    for interactions in post['entity_interactions']:\n",
    "        print '; '.join([i[0] for i in interactions])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10. Visualizing interactions between entities with HTML output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display\n",
    "\n",
    "BLOG_DATA = \"resources/ch05-webpages/feed.json\"\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"<html>\n",
    "    <head>\n",
    "        <title>%s</title>\n",
    "        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n",
    "    </head>\n",
    "    <body>%s</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "blog_data = json.loads(open(BLOG_DATA).read())\n",
    "\n",
    "for post in blog_data:\n",
    "\n",
    "    post.update(extract_interactions(post['content']))\n",
    "\n",
    "    # Display output as markup with entities presented in bold text\n",
    "\n",
    "    post['markup'] = []\n",
    "\n",
    "    for sentence_idx in range(len(post['sentences'])):\n",
    "\n",
    "        s = post['sentences'][sentence_idx]\n",
    "        for (term, _) in post['entity_interactions'][sentence_idx]:\n",
    "            s = s.replace(term, '<strong>%s</strong>' % (term, ))\n",
    "\n",
    "        post['markup'] += [s] \n",
    "            \n",
    "    filename = post['title'].replace(\"?\", \"\") + '.entity_interactions.html'\n",
    "    f = open(os.path.join('resources', 'ch05-webpages', filename), 'w')\n",
    "    html = HTML_TEMPLATE % (post['title'] + ' Interactions', \n",
    "                            ' '.join(post['markup']),)\n",
    "    f.write(html.encode('utf-8'))\n",
    "    f.close()\n",
    "\n",
    "    print \"Data written to\", f.name\n",
    "    \n",
    "    # Display any of these files with an inline frame. This displays the\n",
    "    # last file processed by using the last value of f.name...\n",
    "    \n",
    "    print \"Displaying %s:\" % f.name\n",
    "    display(IFrame('files/%s' % f.name, '100%', '600px'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
