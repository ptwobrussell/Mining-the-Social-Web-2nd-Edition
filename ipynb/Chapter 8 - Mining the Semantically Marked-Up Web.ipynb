{
 "metadata": {
  "name": "",
  "signature": "sha256:642ada85f359f43c2bcfb60967e7aa3ec452b564f8c91312040b6fe84d1b3f98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Mining the Social Web, 2nd Edition\n",
      "\n",
      "##Chapter 8: Mining the Semantically Marked-Up Web: Extracting Microformats, Inferencing Over RDF, and More\n",
      "\n",
      "This IPython Notebook provides an interactive way to follow along with and explore the numbered examples from [_Mining the Social Web (2nd Edition)_](http://bit.ly/135dHfs). The intent behind this notebook is to reinforce the concepts from the sample code in a fun, convenient, and effective way. This notebook assumes that you are reading along with the book and have the context of the discussion as you work through these exercises.\n",
      "\n",
      "In the somewhat unlikely event that you've somehow stumbled across this notebook outside of its context on GitHub, [you can find the full source code repository here](http://bit.ly/16kGNyb).\n",
      "\n",
      "## Copyright and Licensing\n",
      "\n",
      "You are free to use or adapt this notebook for any purpose you'd like. However, please respect the [Simplified BSD License](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/LICENSE.txt) that governs its use."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 1. Extracting geo-microformatted data from a Wikipedia page"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests # pip install requests\n",
      "from BeautifulSoup import BeautifulSoup # pip install BeautifulSoup\n",
      "\n",
      "# XXX: Any URL containing a geo microformat...\n",
      "\n",
      "URL = 'http://en.wikipedia.org/wiki/Franklin,_Tennessee'\n",
      "\n",
      "# In the case of extracting content from Wikipedia, be sure to\n",
      "# review its \"Bot Policy,\" which is defined at\n",
      "# http://meta.wikimedia.org/wiki/Bot_policy#Unacceptable_usage\n",
      "\n",
      "req = requests.get(URL, headers={'User-Agent' : \"Mining the Social Web\"})\n",
      "soup = BeautifulSoup(req.text)\n",
      "\n",
      "geoTag = soup.find(True, 'geo')\n",
      "\n",
      "if geoTag and len(geoTag) > 1:\n",
      "    lat = geoTag.find(True, 'latitude').string\n",
      "    lon = geoTag.find(True, 'longitude').string\n",
      "    print 'Location is at', lat, lon\n",
      "elif geoTag and len(geoTag) == 1:\n",
      "    (lat, lon) = geoTag.string.split(';')\n",
      "    (lat, lon) = (lat.strip(), lon.strip())\n",
      "    print 'Location is at', lat, lon\n",
      "else:\n",
      "    print 'No location found'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Location is at 35.92917 -86.85750\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 2. Displaying geo-microformats with Google Maps in IPython Notebook"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import IFrame\n",
      "from IPython.core.display import display\n",
      "\n",
      "# Google Maps URL template for an iframe\n",
      "\n",
      "google_maps_url = \"http://maps.google.com/maps?q={0}+{1}&output=embed\".format(lat, lon)\n",
      "\n",
      "display(IFrame(google_maps_url, '425px', '350px'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"425px\"\n",
        "            height=350px\"\n",
        "            src=\"http://maps.google.com/maps?q=35.92917+-86.85750&output=embed\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.lib.display.IFrame at 0x3e461d0>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next exmaples use Schema.org"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will need a function to extract the HTML tag info or contents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This function was created by Debra Lewis, Graduate student at Florida Atlantic University\n",
      "import bs4.element as BSElementType\n",
      "\n",
      "def getTagInfo(tag):\n",
      "    #make sure it's the right kind of input, or else you'll get all kinds of weird errors.\n",
      "    if type(tag) is not BSElementType.Tag:\n",
      "         return \"INVALID INPUT!\"\n",
      "    \n",
      "    tagInfo = \"\"\n",
      "    \n",
      "    # See if the input tag has a 'content' attribute.\n",
      "    # If it does, use whatever text is stored in that attribute as the name of the recipe\n",
      "    if tag.has_attr('content'):\n",
      "        tagInfo = tag.get('content')\n",
      "        \n",
      "    # See if fn is empty (there was no \"content\" attribute, or it was empty)\n",
      "    if not tagInfo:\n",
      "        # Grab whatever text is in the tag\n",
      "        # For example, for <p>blah</p>, it would grab \"blah\"\n",
      "        tagInfo = tag.get_text()\n",
      "        \n",
      "        #if function STILL hasn't gotten any info by this point, then just set it to \"None\"\n",
      "        if not tagInfo:\n",
      "            tagInfo = \"None\"\n",
      "        \n",
      "    return tagInfo.strip()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 3. Extracting Recipe data from a web page"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import requests\n",
      "import json\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Pass in a URL containing Recipe...\n",
      "\n",
      "URL = 'http://britishfood.about.com/od/recipeindex/r/applepie.htm'\n",
      "\n",
      "# Parse out some of the pertinent information for a recipe.\n",
      "# See http://Schema.org/Recipe\n",
      "\n",
      "def parse_recipe(url):\n",
      "    req = requests.get(URL)\n",
      "    \n",
      "    soup = BeautifulSoup(req.text)\n",
      "    \n",
      "    recipe = soup.find(itemtype=\"http://schema.org/Recipe\")\n",
      "\n",
      "    if recipe and len(recipe) > 1:\n",
      "        fn = getTagInfo(recipe.find(itemprop=\"name\"))\n",
      "        \n",
      "        dataByLine = recipe.find(itemprop=\"author\")\n",
      "        authorHTMLtag = dataByLine.find(itemprop=\"name\")\n",
      "        author = getTagInfo(authorHTMLtag)\n",
      "        \n",
      "        ingredients = [i.string \n",
      "                            for i in recipe.findAll(itemprop=\"ingredients\") \n",
      "                                if i.string is not None]\n",
      "        instructions = []\n",
      "        for i in recipe.find(itemprop=\"recipeInstructions\"):\n",
      "            if type(i) == BSElementType.Tag:\n",
      "                s = ''.join(i.findAll(text=True)).strip()\n",
      "            elif type(i) == BSElementType.NavigableString:\n",
      "                s = i.string.strip()\n",
      "            else:\n",
      "                continue\n",
      "\n",
      "            if s != '': \n",
      "                instructions += [s]\n",
      "\n",
      "                \n",
      "                \n",
      "        return {\n",
      "            'name': fn,\n",
      "            'author': author,\n",
      "            'ingredients': ingredients,\n",
      "            'instructions': instructions,\n",
      "            }\n",
      "    else:\n",
      "        return {}\n",
      "\n",
      "\n",
      "recipe = parse_recipe(URL)\n",
      "print json.dumps(recipe, indent=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\n",
        "    \"instructions\": [\n",
        "        \"Preparation\", \n",
        "        \"Method\", \n",
        "        \"Place the flour, butter and salt into a large clean bowl.Rub the butter into the flour with your fingertips until the mixture resembles fine breadcrumbs, working as quickly as possible to prevent the dough becoming warm. Add the water to the mixture and using a cold knife stir until the dough binds together, add more cold water a teaspoon at a time if the mixture is too dry.Wrap the dough in Saran wrap/Clingfilm and chill for a minimum of 15 minutes, up to 30 minutes.\", \n",
        "        \"The dough can also be made in a food processor by mixing the flour, butter and salt in the bowl of the processor on a pulse setting. When the mixture resembles breadcrumbs, add the water, slowly, through the funnel until the dough comes together in a ball. Wrap in Saran wrap/ Clingfilm and chill as above.\", \n",
        "        \"Heat the oven to 220\\u00b0C/425\\u00b0F/gas 7.\", \n",
        "        \"Meanwhile simmer the apples with the lemon juice and water in a large pan until soft. Add the sugar and cinnamon to the cooked apples. Remove from the heat and add the butter and leave to cool.Roll out half the pastry and line a 13cm / 7\\\" pie dish. Put the cooled, cooked apple mixture into the pastry case.Roll out the remaining pastry to make a lid for the pie. Damp the edges of the pastry in the dish with a little cold water, cover with the lid,\\u00a0 press the edges firmly together and crimp to decorate.Brush the top of the pie with milk and bake at the top of a hot oven for 20 - 25 minutes.Serve hot or cold with cream, ice cream or custard sauce.\"\n",
        "    ], \n",
        "    \"ingredients\": [\n",
        "        \"Pastry\", \n",
        "        \"200g / 1 2/3 cups all purpose/plain flour\", \n",
        "        \"pinch of salt\", \n",
        "        \"110g / 4 oz, cubed butter or an equal mix of butter and lard\", \n",
        "        \"2-3 tbsp cold water\", \n",
        "        \"Filling\", \n",
        "        \"700g / 1\\u00a0\\u00bd lb cooking apples, peeled, cored and quartered\", \n",
        "        \"2 tbsp lemon juice\", \n",
        "        \"\\u00a0110g / 4 oz sugar\", \n",
        "        \"4 - 6 tbsp cold water\", \n",
        "        \"1 level tsp ground cinnamon (optional)\", \n",
        "        \"25g / 1 oz\\u00a0 butter\", \n",
        "        \"Milk to glaze\"\n",
        "    ], \n",
        "    \"name\": \"Home-Made British Apple Pie\", \n",
        "    \"author\": \"By Elaine Lemm\"\n",
        "}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 4. Parsing Review Rating-Aggregate Schema.org format data for a recipe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# Pass in a URL that contains Review-aggregate info...\n",
      "\n",
      "URL = 'http://britishfood.about.com/od/recipeindex/r/applepie.htm'\n",
      "\n",
      "# See http://Schema.org/Review\n",
      "    \n",
      "req = requests.get(URL)\n",
      "    \n",
      "soup = BeautifulSoup(req.text)\n",
      "    \n",
      "# Find the Recipe or whatever other kind of parent item encapsulates\n",
      "# the Review\n",
      "    \n",
      "item_element = soup.find('meta',{\"itemprop\":\"name\"})\n",
      "item = item_element['content']\n",
      "print item\n",
      "        \n",
      "# And now use aggregate rating to obtain all Rating info at once\n",
      "review = soup.find('div', {\"itemtype\" : \"http://schema.org/AggregateRating\"})\n",
      "reviewProperties = review.findAll('meta')\n",
      " \n",
      "    \n",
      "for i in range(0,len(reviewProperties)):\n",
      "    print \"%s = %s\" % (reviewProperties[i]['itemprop'],reviewProperties[i]['content'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Home-Made British Apple Pie\n",
        "ratingValue = 4.29\n",
        "reviewCount = 7\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**Note: You may also want to try Google's [structured data testing tool](http://www.google.com/webmasters/tools/richsnippets) to extract semantic markup from a webpage**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Execute the all three of the pip install commands listed under Installation here:  https://code.google.com/p/fuxi/wiki/Installation_Testing **"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**Note: You can use bash cell magic as shown below to invoke FuXi on the [sample data file](files/resources/ch08-semanticweb/chuck-norris.n3) introduced at the end of the chapter as follows:**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**On Unix:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "FuXi --rules=resources/ch08-semanticweb/chuck-norris.n3 --ruleFacts --naive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Couldn't find program: u'bash'\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**On Windows:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cmd\n",
      "\n",
      "FuXi --rules=resources/ch08-semanticweb/chuck-norris.n3 --ruleFacts --naive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Microsoft Windows [Version 6.3.9600]\r\n",
        "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
        "\r\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>FuXi --rules=resources/chuck-norris.n3 --ruleFacts --naive\n",
        "@prefix : <http://example.com/MiningTheSocialWeb#> .\r\n",
        "\r\n",
        ":ChuckNorris a :god .\r\n",
        "\r\n",
        ":Socrates a :Mortal;\r\n",
        "    :drinks :whisky .\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Parsing RDF facts from  resources/chuck-norris.n3\r\n",
        "Time to calculate closure on working memory:  1.00016593933 milli seconds\r\n",
        "<Network: 3 rules, 6 nodes, 3 tokens in working memory, 3 inferred tokens>\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:  %%bash can be ran from Windows by installing cygwin:  http://cygwin.com/faq.html#faq.setup.setup **"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**You can explore other options for FuXi by invoking its --help command**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Run this script on Unix**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "FuXi --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Or Run this script on Windows**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%cmd\n",
      "\n",
      "FuXi --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Microsoft Windows [Version 6.3.9600]\r\n",
        "(c) 2013 Microsoft Corporation. All rights reserved.\r\n",
        "\r\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>FuXi --help\n",
        "Usage: FuXi-script.py [options] factFile1 factFile2 ... factFileN\r\n",
        "\r\n",
        "Options:\r\n",
        "  -h, --help            show this help message and exit\r\n",
        "  --why=WHY             Specifies the goals to solve for\r\n",
        "  --closure             Whether or not to serialize the inferred triples along\r\n",
        "                        with the original triples.  Otherwise (the default\r\n",
        "                        behavior), serialize only the inferred triples\r\n",
        "  --naive               Naively perform forward chaining over rules and facts\r\n",
        "                        using the RETE network\r\n",
        "  --imports             Whether or not to follow owl:imports in the fact graph\r\n",
        "  --output=RDF_FORMAT   Serialize the inferred triples and/or original RDF\r\n",
        "                        triples to STDOUT using the specified RDF syntax ('xml\r\n",
        "                        ','pretty-xml','nt','turtle', or 'n3') or to print a\r\n",
        "                        summary of the conflict set (from the RETE network) if\r\n",
        "                        the value of this option is 'conflict'.  If the the\r\n",
        "                        value is 'rif' or 'rif-xml', Then the rules used for\r\n",
        "                        inference will be serialized as RIF.  If the value is\r\n",
        "                        'pml' and --why is used,  then the PML RDF statements\r\n",
        "                        are serialized.  If output is 'proof-graph then a\r\n",
        "                        graphviz .dot file of the proof graph is printed.\r\n",
        "                        Finally if the value is 'man-owl', then the RDF facts\r\n",
        "                        are assumed to be OWL/RDF and serialized via\r\n",
        "                        Manchester OWL syntax. The default is n3\r\n",
        "  --class=QNAME         Used with --output=man-owl to determine which classes\r\n",
        "                        within the entire OWL/RDF are targetted for\r\n",
        "                        serialization.  Can be used more than once\r\n",
        "  --hybrid              Used to determine whether or not to peek into the fact\r\n",
        "                        graph to identify predicates that are both derived and\r\n",
        "                        base.  This is expensive for large fact graphsand is\r\n",
        "                        explicitely not used against SPARQL endpoints\r\n",
        "  --property=QNAME      Used with --output=man-owl or --extract to determine\r\n",
        "                        which properties are serialized / extracted.  Can be\r\n",
        "                        used more than once\r\n",
        "  --normalize           Used with --output=man-owl to attempt to determine if\r\n",
        "                        the ontology is 'normalized' [Rector, A. 2003]The\r\n",
        "                        default is False\r\n",
        "  --ddlGraph=DDLGRAPH   The location of a N3 Data Description document\r\n",
        "                        describing the IDB predicates\r\n",
        "  --input-format=RDF_FORMAT\r\n",
        "                        The format of the RDF document(s) which serve as the\r\n",
        "                        initial facts  for the RETE network. One of\r\n",
        "                        'xml','n3','trix', 'nt', or 'rdfa'.  The default is\r\n",
        "                        xml\r\n",
        "  --safety=RULE_SAFETY  Determines how to handle RIF Core safety.  A value of\r\n",
        "                        'loose'  means that unsafe rules will be ignored.  A\r\n",
        "                        value of 'strict'  will cause a syntax exception upon\r\n",
        "                        any unsafe rule.  A value of 'none' (the default) does\r\n",
        "                        nothing\r\n",
        "  --pDSemantics         Used with --dlp to add pD semantics ruleset for\r\n",
        "                        semantics not covered by DLP but can be expressed in\r\n",
        "                        definite Datalog Logic Programming The default is\r\n",
        "                        False\r\n",
        "  --stdin               Parse STDIN as an RDF graph to contribute to the\r\n",
        "                        initial facts. The default is False\r\n",
        "  --ns=PREFIX=URI       Register a namespace binding (QName prefix to a base\r\n",
        "                        URI).  This can be used more than once\r\n",
        "  --rules=PATH_OR_URI   The Notation 3 documents to use as rulesets for the\r\n",
        "                        RETE network.  Can be specified more than once\r\n",
        "  -d, --debug           Include debugging output\r\n",
        "  --strictness=DDL_STRICTNESS\r\n",
        "                        Used with --why to specify whether to: *not* check if\r\n",
        "                        predicates are  both derived and base (loose), if they\r\n",
        "                        are, mark as derived (defaultDerived) or as base\r\n",
        "                        (defaultBase) predicates, else raise an exception\r\n",
        "                        (harsh)\r\n",
        "  --firstAnswer         Used with --why to determine whether to fetch all\r\n",
        "                        answers or just the first\r\n",
        "  --edb=EXTENSIONAL_DB_PREDICATE_QNAME\r\n",
        "                        Used with --why/--strictness=defaultDerived to specify\r\n",
        "                        which clashing predicate will be designated as a base\r\n",
        "                        predicate\r\n",
        "  --idb=INTENSIONAL_DB_PREDICATE_QNAME\r\n",
        "                        Used with --why/--strictness=defaultBase to specify\r\n",
        "                        which clashing predicate will be designated as a\r\n",
        "                        derived predicate\r\n",
        "  --hybridPredicate=PREDICATE_QNAME\r\n",
        "                        Used with --why to explicitely specify a hybrid\r\n",
        "                        predicate (in both  IDB and EDB)\r\n",
        "  --noMagic=DB_PREDICATE_QNAME\r\n",
        "                        Used with --why to specify that the predicate shouldnt\r\n",
        "                        have its magic sets calculated\r\n",
        "  --filter=PATH_OR_URI  The Notation 3 documents to use as a filter\r\n",
        "                        (entailments do not particpate in network)\r\n",
        "  --ruleFacts           Determines whether or not to attempt to parse initial\r\n",
        "                        facts from the rule graph.  The default is False\r\n",
        "  --builtins=PATH_TO_PYTHON_MODULE\r\n",
        "                        The path to a python module with function definitions\r\n",
        "                        (and a dicitonary called ADDITIONAL_FILTERS) to use\r\n",
        "                        for builtins implementations\r\n",
        "  --dlp                 Use Description Logic Programming (DLP) to extract\r\n",
        "                        rules from OWL/RDF.  The default is False\r\n",
        "  --sparqlEndpoint      Indicates that the sole argument is the URI of a\r\n",
        "                        SPARQL endpoint to query\r\n",
        "  --ontology=PATH_OR_URI\r\n",
        "                        The path to an OWL RDF/XML graph to use DLP to extract\r\n",
        "                        rules from (other wise, fact graph(s) are used)\r\n",
        "  --ruleFormat=RULE_FORMAT\r\n",
        "                        The format of the rules to parse ('n3', 'rif').  The\r\n",
        "                        default is n3\r\n",
        "  --ontologyFormat=RDF_FORMAT\r\n",
        "                        The format of the OWL RDF/XML graph specified via\r\n",
        "                        --ontology.  The default is xml\r\n",
        "  --builtinTemplates=N3_DOC_PATH_OR_URI\r\n",
        "                        The path to an N3 document associating SPARQL FILTER\r\n",
        "                        templates to rule builtins\r\n",
        "  --normalForm          Whether or not to reduce DL axioms & LP rules to a\r\n",
        "                        normal form\r\n",
        "\r\n",
        "C:\\Users\\MisaelNMelissa\\Dropbox\\Mel_School\\Web 2.0 Architectures & Algorithms\\Homework\\Homework4_Serrano>"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}